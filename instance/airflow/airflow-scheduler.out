[[34m2026-02-11T16:14:23.567+0800[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2026-02-11T16:14:23.571+0800[0m] {[34mexecutor_loader.py:[0m115} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2026-02-11T16:14:23.679+0800[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Starting the scheduler[0m
[[34m2026-02-11T16:14:23.680+0800[0m] {[34mscheduler_job_runner.py:[0m814} INFO[0m - Processing each file at most -1 times[0m
[[34m2026-02-11T16:14:23.688+0800[0m] {[34mmanager.py:[0m169} INFO[0m - Launched DagFileProcessorManager with pid: 14149[0m
[[34m2026-02-11T16:14:23.699+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:14:26.868+0800[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2026-02-11T16:14:26.960+0800] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2026-02-11T16:14:30.919+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello to 2026-02-11 00:00:00+00:00, run_after=2026-02-12 00:00:00+00:00[0m
[[34m2026-02-11T16:14:30.931+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-02 00:00:00+00:00, run_after=2026-01-03 00:00:00+00:00[0m
[[34m2026-02-11T16:14:30.939+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-02 00:00:00+00:00, run_after=2026-01-03 00:00:00+00:00[0m
[[34m2026-02-11T16:14:31.047+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 5 tasks up for execution:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.set_api_key scheduled__2026-02-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:11:04.643098+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.some_task scheduled__2026-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:14:31.048+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:14:31.049+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:14:31.050+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:14:31.051+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:14:31.052+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello has 1/16 running and queued tasks[0m
[[34m2026-02-11T16:14:31.054+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.set_api_key scheduled__2026-02-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:11:04.643098+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.some_task scheduled__2026-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:14:31.066+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task set_api_key because previous state change time has not been saved[0m
[[34m2026-02-11T16:14:31.067+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:14:31.068+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:14:31.069+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:14:31.070+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task some_task because previous state change time has not been saved[0m
[[34m2026-02-11T16:14:31.071+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='set_api_key', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2026-02-11T16:14:31.072+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'set_api_key', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:31.076+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:11:04.643098+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:14:31.077+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:11:04.643098+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:14:31.078+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:14:31.079+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:31.080+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:14:31.081+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:31.082+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='some_task', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:14:31.084+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'some_task', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:31.088+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'set_api_key', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:33.841+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:14:36.155+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.set_api_key scheduled__2026-02-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:14:37.459+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:11:04.643098+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:14:40.137+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:14:41.540+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:11:04.643098+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:14:42.573+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:45.136+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:14:47.100+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-01T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:14:48.358+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:50.911+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:14:52.247+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-01T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:14:53.436+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'some_task', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:14:55.991+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:14:57.332+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.some_task scheduled__2026-02-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:14:58.525+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='set_api_key', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:14:58.529+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:11:04.643098+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:14:58.530+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:14:58.530+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:14:58.531+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='some_task', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:14:58.551+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-01T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:14:47.187624+00:00, run_end_date=2026-02-11 08:14:47.649859+00:00, run_duration=0.462235, state=success, executor_state=success, try_number=1, max_tries=1, job_id=20, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:14:31.056934+00:00, queued_by_job_id=17, pid=14170[0m
[[34m2026-02-11T16:14:58.552+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello, task_id=set_api_key, run_id=scheduled__2026-02-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:14:36.231912+00:00, run_end_date=2026-02-11 08:14:36.771725+00:00, run_duration=0.539813, state=success, executor_state=success, try_number=1, max_tries=1, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-11 08:14:31.056934+00:00, queued_by_job_id=17, pid=14162[0m
[[34m2026-02-11T16:14:58.552+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello, task_id=some_task, run_id=scheduled__2026-02-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:14:57.408666+00:00, run_end_date=2026-02-11 08:14:57.848556+00:00, run_duration=0.43989, state=success, executor_state=success, try_number=1, max_tries=1, job_id=22, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:14:31.056934+00:00, queued_by_job_id=17, pid=14177[0m
[[34m2026-02-11T16:14:58.553+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-01T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:14:52.325941+00:00, run_end_date=2026-02-11 08:14:52.779444+00:00, run_duration=0.453503, state=success, executor_state=success, try_number=1, max_tries=1, job_id=21, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:14:31.056934+00:00, queued_by_job_id=17, pid=14174[0m
[[34m2026-02-11T16:14:58.554+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:11:04.643098+00:00, map_index=-1, run_start_date=2026-02-11 08:14:41.625100+00:00, run_end_date=2026-02-11 08:14:41.821071+00:00, run_duration=0.195971, state=success, executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:14:31.056934+00:00, queued_by_job_id=17, pid=14167[0m
[[34m2026-02-11T16:15:01.747+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:01.757+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:01.786+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-01 00:00:00+00:00: scheduled__2026-01-01T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:14:30.935001+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:01.787+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-01 00:00:00+00:00, run_id=scheduled__2026-01-01T00:00:00+00:00, run_start_date=2026-02-11 08:14:30.980823+00:00, run_end_date=2026-02-11 08:15:01.787684+00:00, run_duration=30.806861, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-01 00:00:00+00:00, data_interval_end=2026-01-02 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:15:01.793+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-02 00:00:00+00:00, run_after=2026-01-03 00:00:00+00:00[0m
[[34m2026-02-11T16:15:01.796+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-01 00:00:00+00:00: scheduled__2026-01-01T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:14:30.923628+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:01.797+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-01 00:00:00+00:00, run_id=scheduled__2026-01-01T00:00:00+00:00, run_start_date=2026-02-11 08:14:30.981112+00:00, run_end_date=2026-02-11 08:15:01.797395+00:00, run_duration=30.816283, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-01 00:00:00+00:00, data_interval_end=2026-01-02 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:15:01.801+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-02 00:00:00+00:00, run_after=2026-01-03 00:00:00+00:00[0m
[[34m2026-02-11T16:15:01.824+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 4 tasks up for execution:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.generate_test_cases scheduled__2026-02-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:11:04.643098+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:01.825+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:01.826+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:01.826+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:01.827+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:01.828+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.generate_test_cases scheduled__2026-02-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-02T00:00:00+00:00 [scheduled]>
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:11:04.643098+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:01.832+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task generate_test_cases because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:01.833+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:01.834+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:01.834+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:01.836+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='generate_test_cases', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:15:01.836+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'generate_test_cases', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:01.837+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:01.838+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:01.838+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-02T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:01.839+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:01.839+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:11:04.643098+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:01.840+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:11:04.643098+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:15:01.842+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'generate_test_cases', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:04.335+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:05.644+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.generate_test_cases scheduled__2026-02-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:15.704+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:18.158+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:19.568+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-02T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:20.778+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-02T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:23.363+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:24.684+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-02T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:25.951+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:11:04.643098+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:15:28.482+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:15:29.811+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:11:04.643098+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:30.768+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='generate_test_cases', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:30.771+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:30.771+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-02T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:30.772+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:11:04.643098+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:30.788+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello, task_id=generate_test_cases, run_id=scheduled__2026-02-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:05.722182+00:00, run_end_date=2026-02-11 08:15:15.055167+00:00, run_duration=9.332985, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-11 08:15:01.829859+00:00, queued_by_job_id=17, pid=14184[0m
[[34m2026-02-11T16:15:30.790+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-02T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:19.655829+00:00, run_end_date=2026-02-11 08:15:20.087418+00:00, run_duration=0.431589, state=success, executor_state=success, try_number=1, max_tries=1, job_id=24, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:01.829859+00:00, queued_by_job_id=17, pid=14196[0m
[[34m2026-02-11T16:15:30.791+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:11:04.643098+00:00, map_index=-1, run_start_date=2026-02-11 08:15:29.885634+00:00, run_end_date=2026-02-11 08:15:30.120666+00:00, run_duration=0.235032, state=success, executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:15:01.829859+00:00, queued_by_job_id=17, pid=14203[0m
[[34m2026-02-11T16:15:30.791+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-02T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:24.766845+00:00, run_end_date=2026-02-11 08:15:25.204708+00:00, run_duration=0.437863, state=success, executor_state=success, try_number=1, max_tries=1, job_id=25, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:01.829859+00:00, queued_by_job_id=17, pid=14199[0m
[2026-02-11T16:15:30.838+0800] {manager.py:523} INFO - DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_123 is missing and will be deactivated.
[2026-02-11T16:15:30.844+0800] {manager.py:535} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-11T16:15:30.846+0800] {manager.py:539} INFO - Deleted DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_123 in serialized_dag table
[[34m2026-02-11T16:15:33.699+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:33.702+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:33.720+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-02 00:00:00+00:00: scheduled__2026-01-02T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:01.743374+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:33.720+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-02 00:00:00+00:00, run_id=scheduled__2026-01-02T00:00:00+00:00, run_start_date=2026-02-11 08:15:01.767864+00:00, run_end_date=2026-02-11 08:15:33.720874+00:00, run_duration=31.95301, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-02 00:00:00+00:00, data_interval_end=2026-01-03 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:15:33.725+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:33.729+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-02 00:00:00+00:00: scheduled__2026-01-02T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:01.752450+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:33.730+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-02 00:00:00+00:00, run_id=scheduled__2026-01-02T00:00:00+00:00, run_start_date=2026-02-11 08:15:01.768189+00:00, run_end_date=2026-02-11 08:15:33.730030+00:00, run_duration=31.961841, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-02 00:00:00+00:00, data_interval_end=2026-01-03 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:15:33.734+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-03 00:00:00+00:00, run_after=2026-01-04 00:00:00+00:00[0m
[[34m2026-02-11T16:15:33.742+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:11:04.643098+00:00: manual__2026-02-11T08:11:04.643098+00:00, state:running, queued_at: 2026-02-11 08:11:04.658477+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:15:33.743+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:11:04.643098+00:00, run_id=manual__2026-02-11T08:11:04.643098+00:00, run_start_date=2026-02-11 08:14:30.984132+00:00, run_end_date=2026-02-11 08:15:33.743109+00:00, run_duration=62.758977, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:15:33.756+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.run_automation_test scheduled__2026-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:33.757+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:33.757+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.run_automation_test scheduled__2026-02-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:33.760+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:33.761+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='run_automation_test', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:33.762+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'run_automation_test', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:33.764+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', 'run_automation_test', 'scheduled__2026-02-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:36.087+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:37.302+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello.run_automation_test scheduled__2026-02-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
============================= test session starts ==============================
platform darwin -- Python 3.10.4, pytest-7.0.0, pluggy-1.4.0 -- /Users/linghuchong/miniconda3/envs/py310/bin/python
cachedir: .pytest_cache
metadata: {'Python': '3.10.4', 'Platform': 'macOS-10.16-x86_64-i386-64bit', 'Packages': {'pytest': '7.0.0', 'pluggy': '1.4.0'}, 'Plugins': {'anyio': '4.3.0', 'metadata': '3.1.1', 'mimesis': '18.0.0', 'html': '4.2.0', 'order': '1.3.0', 'allure-pytest': '2.14.0', 'time-machine': '2.14.0', 'ordering': '0.6', 'langsmith': '0.3.13', 'Faker': '19.13.0'}, 'JAVA_HOME': '/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home'}
rootdir: /Users/linghuchong/Downloads/51/Python/project/instance/pytest1, configfile: pytest.ini
plugins: anyio-4.3.0, metadata-3.1.1, mimesis-18.0.0, html-4.2.0, order-1.3.0, allure-pytest-2.14.0, time-machine-2.14.0, ordering-0.6, langsmith-0.3.13, Faker-19.13.0
collecting ... collected 4 items

Users/linghuchong/Downloads/51/Python/project/instance/pytest1/test_2.py::test_case1 PASSED [ 25%]
Users/linghuchong/Downloads/51/Python/project/instance/pytest1/test_2.py::test_case2 PASSED [ 50%]
Users/linghuchong/Downloads/51/Python/project/instance/pytest1/test_2.py::TestDemo::test_demo1 PASSED [ 75%]
Users/linghuchong/Downloads/51/Python/project/instance/pytest1/test_2.py::TestDemo::test_demo2 PASSED [100%]

- Generated html report: file:///Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_report.html -
============================== 4 passed in 0.10s ===============================
[[34m2026-02-11T16:15:40.227+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello', task_id='run_automation_test', run_id='scheduled__2026-02-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:40.245+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello, task_id=run_automation_test, run_id=scheduled__2026-02-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:37.375641+00:00, run_end_date=2026-02-11 08:15:39.545129+00:00, run_duration=2.169488, state=success, executor_state=success, try_number=1, max_tries=1, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:15:33.758919+00:00, queued_by_job_id=17, pid=14207[0m
[[34m2026-02-11T16:15:43.937+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-04 00:00:00+00:00, run_after=2026-01-05 00:00:00+00:00[0m
[[34m2026-02-11T16:15:43.942+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-04 00:00:00+00:00, run_after=2026-01-05 00:00:00+00:00[0m
[[34m2026-02-11T16:15:43.972+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello @ 2026-02-10 00:00:00+00:00: scheduled__2026-02-10T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:14:30.864314+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:43.973+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello, execution_date=2026-02-10 00:00:00+00:00, run_id=scheduled__2026-02-10T00:00:00+00:00, run_start_date=2026-02-11 08:14:30.981232+00:00, run_end_date=2026-02-11 08:15:43.973687+00:00, run_duration=72.992455, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=fb56d5a9a39933898950c268a7db9144[0m
[[34m2026-02-11T16:15:43.976+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for È°πÁõÆ_Â∑•‰ΩúÊµÅ_hello to 2026-02-11 00:00:00+00:00, run_after=2026-02-12 00:00:00+00:00[0m
[[34m2026-02-11T16:15:43.987+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:43.988+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:43.989+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:43.989+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-03T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-03T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:43.992+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:43.993+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:43.994+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:43.994+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:43.995+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:43.996+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:43.998+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:46.379+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:47.647+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-03T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:48.592+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:51.532+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:15:52.979+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-03T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:15:54.198+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:54.202+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-03T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:15:54.220+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-03T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:53.082374+00:00, run_end_date=2026-02-11 08:15:53.514205+00:00, run_duration=0.431831, state=success, executor_state=success, try_number=1, max_tries=1, job_id=29, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:43.990780+00:00, queued_by_job_id=17, pid=14218[0m
[[34m2026-02-11T16:15:54.221+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-03T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:15:47.740728+00:00, run_end_date=2026-02-11 08:15:47.929817+00:00, run_duration=0.189089, state=success, executor_state=success, try_number=1, max_tries=1, job_id=28, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:43.990780+00:00, queued_by_job_id=17, pid=14215[0m
[[34m2026-02-11T16:15:57.093+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:15:57.099+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:15:57.128+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-03 00:00:00+00:00: scheduled__2026-01-03T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:43.939230+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:57.129+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-03 00:00:00+00:00, run_id=scheduled__2026-01-03T00:00:00+00:00, run_start_date=2026-02-11 08:15:43.951967+00:00, run_end_date=2026-02-11 08:15:57.129236+00:00, run_duration=13.177269, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-03 00:00:00+00:00, data_interval_end=2026-01-04 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:15:57.133+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-04 00:00:00+00:00, run_after=2026-01-05 00:00:00+00:00[0m
[[34m2026-02-11T16:15:57.136+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-03 00:00:00+00:00: scheduled__2026-01-03T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:43.932239+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:15:57.137+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-03 00:00:00+00:00, run_id=scheduled__2026-01-03T00:00:00+00:00, run_start_date=2026-02-11 08:15:43.952870+00:00, run_end_date=2026-02-11 08:15:57.137682+00:00, run_duration=13.184812, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-03 00:00:00+00:00, data_interval_end=2026-01-04 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:15:57.141+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-04 00:00:00+00:00, run_after=2026-01-05 00:00:00+00:00[0m
[[34m2026-02-11T16:15:57.152+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:57.153+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:57.153+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:15:57.154+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-04T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-04T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:15:57.157+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:57.157+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:15:57.158+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:57.159+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:57.159+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-04T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:15:57.160+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:57.162+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:15:59.490+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:01.283+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-04T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:02.834+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-04T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:05.378+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:06.702+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-04T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:07.867+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:07.871+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-04T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:07.882+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-04T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:01.435998+00:00, run_end_date=2026-02-11 08:16:02.000556+00:00, run_duration=0.564558, state=success, executor_state=success, try_number=1, max_tries=1, job_id=30, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:57.155726+00:00, queued_by_job_id=17, pid=14231[0m
[[34m2026-02-11T16:16:07.883+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-04T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:06.784468+00:00, run_end_date=2026-02-11 08:16:07.203606+00:00, run_duration=0.419138, state=success, executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:15:57.155726+00:00, queued_by_job_id=17, pid=14238[0m
[[34m2026-02-11T16:16:10.916+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:16:10.920+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:16:10.933+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-04 00:00:00+00:00: scheduled__2026-01-04T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:57.088008+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:10.934+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-04 00:00:00+00:00, run_id=scheduled__2026-01-04T00:00:00+00:00, run_start_date=2026-02-11 08:15:57.111089+00:00, run_end_date=2026-02-11 08:16:10.934455+00:00, run_duration=13.823366, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-04 00:00:00+00:00, data_interval_end=2026-01-05 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:16:10.938+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:16:10.941+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-04 00:00:00+00:00: scheduled__2026-01-04T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:15:57.095041+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:10.942+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-04 00:00:00+00:00, run_id=scheduled__2026-01-04T00:00:00+00:00, run_start_date=2026-02-11 08:15:57.111853+00:00, run_end_date=2026-02-11 08:16:10.942716+00:00, run_duration=13.830863, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-04 00:00:00+00:00, data_interval_end=2026-01-05 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:16:10.946+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-05 00:00:00+00:00, run_after=2026-01-06 00:00:00+00:00[0m
[[34m2026-02-11T16:16:16.000+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-06 00:00:00+00:00, run_after=2026-01-07 00:00:00+00:00[0m
[[34m2026-02-11T16:16:16.024+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-06 00:00:00+00:00, run_after=2026-01-07 00:00:00+00:00[0m
[[34m2026-02-11T16:16:16.065+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:16.066+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:16.067+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:16.068+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-05T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-05T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:16.070+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:16.071+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:16.072+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:16.074+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:16.075+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-05T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:16.075+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:16.077+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:18.829+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:20.109+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-05T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:21.315+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-05T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:23.904+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:25.245+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-05T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:26.198+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:26.202+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-05T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:26.215+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-05T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:25.330468+00:00, run_end_date=2026-02-11 08:16:25.529277+00:00, run_duration=0.198809, state=success, executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:16.069324+00:00, queued_by_job_id=17, pid=14261[0m
[[34m2026-02-11T16:16:26.217+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-05T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:20.197558+00:00, run_end_date=2026-02-11 08:16:20.630483+00:00, run_duration=0.432925, state=success, executor_state=success, try_number=1, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:16.069324+00:00, queued_by_job_id=17, pid=14255[0m
[[34m2026-02-11T16:16:30.000+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:30.005+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:30.039+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-05 00:00:00+00:00: scheduled__2026-01-05T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:16.007899+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:30.040+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-05 00:00:00+00:00, run_id=scheduled__2026-01-05T00:00:00+00:00, run_start_date=2026-02-11 08:16:16.034711+00:00, run_end_date=2026-02-11 08:16:30.040661+00:00, run_duration=14.00595, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-05 00:00:00+00:00, data_interval_end=2026-01-06 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:16:30.044+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-06 00:00:00+00:00, run_after=2026-01-07 00:00:00+00:00[0m
[[34m2026-02-11T16:16:30.048+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-05 00:00:00+00:00: scheduled__2026-01-05T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:15.992751+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:30.049+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-05 00:00:00+00:00, run_id=scheduled__2026-01-05T00:00:00+00:00, run_start_date=2026-02-11 08:16:16.035580+00:00, run_end_date=2026-02-11 08:16:30.049284+00:00, run_duration=14.013704, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-05 00:00:00+00:00, data_interval_end=2026-01-06 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:16:30.053+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-06 00:00:00+00:00, run_after=2026-01-07 00:00:00+00:00[0m
[[34m2026-02-11T16:16:30.065+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:30.066+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:30.067+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:30.067+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-06T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-06T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:30.070+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:30.070+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:30.071+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:30.072+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:30.073+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-06T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:30.073+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:30.076+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:32.914+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:34.446+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-06T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:35.676+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-06T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:38.160+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:39.490+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-06T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:40.415+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:40.420+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-06T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:40.433+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-06T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:34.541104+00:00, run_end_date=2026-02-11 08:16:35.017412+00:00, run_duration=0.476308, state=success, executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:30.069067+00:00, queued_by_job_id=17, pid=14271[0m
[[34m2026-02-11T16:16:40.434+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-06T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:39.581139+00:00, run_end_date=2026-02-11 08:16:39.770552+00:00, run_duration=0.189413, state=success, executor_state=success, try_number=1, max_tries=1, job_id=35, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:30.069067+00:00, queued_by_job_id=17, pid=14276[0m
[[34m2026-02-11T16:16:44.039+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:44.042+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:44.057+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-06 00:00:00+00:00: scheduled__2026-01-06T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:29.994551+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:44.058+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-06 00:00:00+00:00, run_id=scheduled__2026-01-06T00:00:00+00:00, run_start_date=2026-02-11 08:16:30.018470+00:00, run_end_date=2026-02-11 08:16:44.058123+00:00, run_duration=14.039653, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-06 00:00:00+00:00, data_interval_end=2026-01-07 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:16:44.062+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:44.067+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-06 00:00:00+00:00: scheduled__2026-01-06T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:30.001824+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:16:44.068+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-06 00:00:00+00:00, run_id=scheduled__2026-01-06T00:00:00+00:00, run_start_date=2026-02-11 08:16:30.019308+00:00, run_end_date=2026-02-11 08:16:44.068162+00:00, run_duration=14.048854, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-06 00:00:00+00:00, data_interval_end=2026-01-07 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:16:44.073+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-07 00:00:00+00:00, run_after=2026-01-08 00:00:00+00:00[0m
[[34m2026-02-11T16:16:48.176+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-08 00:00:00+00:00, run_after=2026-01-09 00:00:00+00:00[0m
[[34m2026-02-11T16:16:48.183+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-08 00:00:00+00:00, run_after=2026-01-09 00:00:00+00:00[0m
[[34m2026-02-11T16:16:48.223+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:48.224+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:48.225+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:16:48.225+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-07T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-07T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:16:48.228+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:48.229+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:16:48.231+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:48.231+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:48.232+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-07T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:16:48.233+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:48.235+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:50.694+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:52.500+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-07T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:53.748+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-07T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:16:56.639+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:16:58.024+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-07T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:16:59.369+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:59.373+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-07T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:16:59.387+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-07T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:52.611182+00:00, run_end_date=2026-02-11 08:16:53.073337+00:00, run_duration=0.462155, state=success, executor_state=success, try_number=1, max_tries=1, job_id=36, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:48.227288+00:00, queued_by_job_id=17, pid=14287[0m
[[34m2026-02-11T16:16:59.388+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-07T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:16:58.110724+00:00, run_end_date=2026-02-11 08:16:58.539014+00:00, run_duration=0.42829, state=success, executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:16:48.227288+00:00, queued_by_job_id=17, pid=14297[0m
[[34m2026-02-11T16:17:02.892+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:02.923+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:02.968+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-07 00:00:00+00:00: scheduled__2026-01-07T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:48.171616+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:02.969+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-07 00:00:00+00:00, run_id=scheduled__2026-01-07T00:00:00+00:00, run_start_date=2026-02-11 08:16:48.195121+00:00, run_end_date=2026-02-11 08:17:02.969009+00:00, run_duration=14.773888, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-07 00:00:00+00:00, data_interval_end=2026-01-08 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:17:02.973+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-08 00:00:00+00:00, run_after=2026-01-09 00:00:00+00:00[0m
[[34m2026-02-11T16:17:02.977+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-07 00:00:00+00:00: scheduled__2026-01-07T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:16:48.179638+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:02.978+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-07 00:00:00+00:00, run_id=scheduled__2026-01-07T00:00:00+00:00, run_start_date=2026-02-11 08:16:48.195756+00:00, run_end_date=2026-02-11 08:17:02.978239+00:00, run_duration=14.782483, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-07 00:00:00+00:00, data_interval_end=2026-01-08 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:17:02.983+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-08 00:00:00+00:00, run_after=2026-01-09 00:00:00+00:00[0m
[[34m2026-02-11T16:17:02.993+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:02.993+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:02.994+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:02.995+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-08T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-08T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:03.000+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:03.005+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:03.007+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:03.008+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:03.009+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-08T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:03.010+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:03.015+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:05.914+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:07.235+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-08T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:08.413+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-08T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:10.853+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:12.197+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-08T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:13.142+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:13.146+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-08T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:13.157+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-08T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:12.285880+00:00, run_end_date=2026-02-11 08:17:12.475109+00:00, run_duration=0.189229, state=success, executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:02.997314+00:00, queued_by_job_id=17, pid=14324[0m
[[34m2026-02-11T16:17:13.158+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-08T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:07.315425+00:00, run_end_date=2026-02-11 08:17:07.734028+00:00, run_duration=0.418603, state=success, executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:02.997314+00:00, queued_by_job_id=17, pid=14319[0m
[[34m2026-02-11T16:17:16.146+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:16.149+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:16.168+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-08 00:00:00+00:00: scheduled__2026-01-08T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:02.914716+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:16.169+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-08 00:00:00+00:00, run_id=scheduled__2026-01-08T00:00:00+00:00, run_start_date=2026-02-11 08:17:02.939047+00:00, run_end_date=2026-02-11 08:17:16.169043+00:00, run_duration=13.229996, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-08 00:00:00+00:00, data_interval_end=2026-01-09 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:17:16.173+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:16.177+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-08 00:00:00+00:00: scheduled__2026-01-08T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:02.884806+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:16.178+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-08 00:00:00+00:00, run_id=scheduled__2026-01-08T00:00:00+00:00, run_start_date=2026-02-11 08:17:02.939556+00:00, run_end_date=2026-02-11 08:17:16.178227+00:00, run_duration=13.238671, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-08 00:00:00+00:00, data_interval_end=2026-01-09 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:17:16.183+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-09 00:00:00+00:00, run_after=2026-01-10 00:00:00+00:00[0m
[[34m2026-02-11T16:17:20.326+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-10 00:00:00+00:00, run_after=2026-01-11 00:00:00+00:00[0m
[[34m2026-02-11T16:17:20.332+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-10 00:00:00+00:00, run_after=2026-01-11 00:00:00+00:00[0m
[[34m2026-02-11T16:17:20.373+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:20.375+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:20.379+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:20.380+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-09T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-09T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:20.383+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:20.384+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:20.386+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:20.387+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:20.388+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-09T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:20.391+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:20.394+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:23.403+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:24.859+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-09T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:25.979+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-09T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:29.038+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:30.665+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-09T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:32.205+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:32.211+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-09T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:32.225+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-09T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:30.756139+00:00, run_end_date=2026-02-11 08:17:31.247861+00:00, run_duration=0.491722, state=success, executor_state=success, try_number=1, max_tries=1, job_id=41, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:20.381883+00:00, queued_by_job_id=17, pid=14349[0m
[[34m2026-02-11T16:17:35.796+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-11 00:00:00+00:00, run_after=2026-01-12 00:00:00+00:00[0m
[[34m2026-02-11T16:17:35.803+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-11 00:00:00+00:00, run_after=2026-01-12 00:00:00+00:00[0m
[[34m2026-02-11T16:17:35.840+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-09 00:00:00+00:00: scheduled__2026-01-09T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:20.329180+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:35.841+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-09 00:00:00+00:00, run_id=scheduled__2026-01-09T00:00:00+00:00, run_start_date=2026-02-11 08:17:20.341849+00:00, run_end_date=2026-02-11 08:17:35.841114+00:00, run_duration=15.499265, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-09 00:00:00+00:00, data_interval_end=2026-01-10 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:17:35.847+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-10 00:00:00+00:00, run_after=2026-01-11 00:00:00+00:00[0m
[[34m2026-02-11T16:17:35.859+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:35.860+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:35.861+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:35.862+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-10T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-10T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:35.864+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:35.865+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:35.866+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:35.866+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:35.867+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-10T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:35.868+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:35.870+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:38.739+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:40.409+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:41.605+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-10T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:44.101+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:45.680+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-10T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:46.603+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:46.606+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-10T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:46.618+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:40.505970+00:00, run_end_date=2026-02-11 08:17:40.978610+00:00, run_duration=0.47264, state=success, executor_state=success, try_number=1, max_tries=1, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:35.863489+00:00, queued_by_job_id=17, pid=14353[0m
[[34m2026-02-11T16:17:46.619+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-10T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:45.765564+00:00, run_end_date=2026-02-11 08:17:45.950321+00:00, run_duration=0.184757, state=success, executor_state=success, try_number=1, max_tries=1, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:35.863489+00:00, queued_by_job_id=17, pid=14356[0m
[[34m2026-02-11T16:17:49.724+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-12 00:00:00+00:00, run_after=2026-01-13 00:00:00+00:00[0m
[[34m2026-02-11T16:17:49.726+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-11 00:00:00+00:00, run_after=2026-01-12 00:00:00+00:00[0m
[[34m2026-02-11T16:17:49.753+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-10 00:00:00+00:00: scheduled__2026-01-10T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:35.790002+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:49.755+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-10 00:00:00+00:00, run_id=scheduled__2026-01-10T00:00:00+00:00, run_start_date=2026-02-11 08:17:35.818833+00:00, run_end_date=2026-02-11 08:17:49.755163+00:00, run_duration=13.93633, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-10 00:00:00+00:00, data_interval_end=2026-01-11 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:17:49.761+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-11 00:00:00+00:00, run_after=2026-01-12 00:00:00+00:00[0m
[[34m2026-02-11T16:17:49.766+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-10 00:00:00+00:00: scheduled__2026-01-10T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:35.798764+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:49.767+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-10 00:00:00+00:00, run_id=scheduled__2026-01-10T00:00:00+00:00, run_start_date=2026-02-11 08:17:35.820102+00:00, run_end_date=2026-02-11 08:17:49.767160+00:00, run_duration=13.947058, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-10 00:00:00+00:00, data_interval_end=2026-01-11 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:17:49.772+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-11 00:00:00+00:00, run_after=2026-01-12 00:00:00+00:00[0m
[[34m2026-02-11T16:17:49.786+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:49.786+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:49.788+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:49.790+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:49.791+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:49.792+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:49.795+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:53.185+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:17:54.778+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-11T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:17:55.795+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:17:55.810+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:17:54.900462+00:00, run_end_date=2026-02-11 08:17:55.122210+00:00, run_duration=0.221748, state=success, executor_state=success, try_number=1, max_tries=1, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:49.789089+00:00, queued_by_job_id=17, pid=14370[0m
[[34m2026-02-11T16:17:59.223+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-12 00:00:00+00:00, run_after=2026-01-13 00:00:00+00:00[0m
[[34m2026-02-11T16:17:59.230+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-12 00:00:00+00:00, run_after=2026-01-13 00:00:00+00:00[0m
[[34m2026-02-11T16:17:59.254+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-11 00:00:00+00:00: scheduled__2026-01-11T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:49.718396+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:17:59.255+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-11 00:00:00+00:00, run_id=scheduled__2026-01-11T00:00:00+00:00, run_start_date=2026-02-11 08:17:49.736608+00:00, run_end_date=2026-02-11 08:17:59.255654+00:00, run_duration=9.519046, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-11 00:00:00+00:00, data_interval_end=2026-01-12 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:17:59.260+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-12 00:00:00+00:00, run_after=2026-01-13 00:00:00+00:00[0m
[[34m2026-02-11T16:17:59.275+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:59.276+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:17:59.276+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-11T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:17:59.279+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:17:59.280+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:17:59.281+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:17:59.283+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:01.882+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:03.254+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-11T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:04.246+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-11T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:04.260+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:03.338898+00:00, run_end_date=2026-02-11 08:18:03.552517+00:00, run_duration=0.213619, state=success, executor_state=success, try_number=1, max_tries=1, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:17:59.277981+00:00, queued_by_job_id=17, pid=14377[0m
[[34m2026-02-11T16:18:08.088+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-13 00:00:00+00:00, run_after=2026-01-14 00:00:00+00:00[0m
[[34m2026-02-11T16:18:08.093+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-13 00:00:00+00:00, run_after=2026-01-14 00:00:00+00:00[0m
[[34m2026-02-11T16:18:08.135+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-11 00:00:00+00:00: scheduled__2026-01-11T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:17:59.225144+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:08.136+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-11 00:00:00+00:00, run_id=scheduled__2026-01-11T00:00:00+00:00, run_start_date=2026-02-11 08:17:59.239009+00:00, run_end_date=2026-02-11 08:18:08.136346+00:00, run_duration=8.897337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-11 00:00:00+00:00, data_interval_end=2026-01-12 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:18:08.141+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-12 00:00:00+00:00, run_after=2026-01-13 00:00:00+00:00[0m
[[34m2026-02-11T16:18:08.154+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:08.155+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:08.156+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:08.156+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-12T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-12T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:08.159+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:08.159+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:08.161+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:08.162+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:08.162+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-12T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:08.163+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:08.165+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:10.761+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:12.707+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-12T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:13.942+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:16.433+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:17.765+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-12T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:18.853+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:18.858+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-12T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:18.873+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-12T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:17.849500+00:00, run_end_date=2026-02-11 08:18:18.048978+00:00, run_duration=0.199478, state=success, executor_state=success, try_number=1, max_tries=1, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:08.157828+00:00, queued_by_job_id=17, pid=14395[0m
[[34m2026-02-11T16:18:18.875+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-12T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:12.845487+00:00, run_end_date=2026-02-11 08:18:13.319171+00:00, run_duration=0.473684, state=success, executor_state=success, try_number=1, max_tries=1, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:08.157828+00:00, queued_by_job_id=17, pid=14392[0m
[[34m2026-02-11T16:18:23.015+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-14 00:00:00+00:00, run_after=2026-01-15 00:00:00+00:00[0m
[[34m2026-02-11T16:18:23.018+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-13 00:00:00+00:00, run_after=2026-01-14 00:00:00+00:00[0m
[[34m2026-02-11T16:18:23.041+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-12 00:00:00+00:00: scheduled__2026-01-12T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:08.090638+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:23.042+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-12 00:00:00+00:00, run_id=scheduled__2026-01-12T00:00:00+00:00, run_start_date=2026-02-11 08:18:08.105073+00:00, run_end_date=2026-02-11 08:18:23.042154+00:00, run_duration=14.937081, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-12 00:00:00+00:00, data_interval_end=2026-01-13 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:18:23.046+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-13 00:00:00+00:00, run_after=2026-01-14 00:00:00+00:00[0m
[[34m2026-02-11T16:18:23.050+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-12 00:00:00+00:00: scheduled__2026-01-12T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:08.083325+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:23.051+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-12 00:00:00+00:00, run_id=scheduled__2026-01-12T00:00:00+00:00, run_start_date=2026-02-11 08:18:08.106211+00:00, run_end_date=2026-02-11 08:18:23.051221+00:00, run_duration=14.94501, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-12 00:00:00+00:00, data_interval_end=2026-01-13 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:18:23.056+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-13 00:00:00+00:00, run_after=2026-01-14 00:00:00+00:00[0m
[[34m2026-02-11T16:18:23.067+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:23.068+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:23.068+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:23.070+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:23.071+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:23.072+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:23.074+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:26.052+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:27.534+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-13T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:28.434+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:28.447+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-13T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:27.619312+00:00, run_end_date=2026-02-11 08:18:27.801743+00:00, run_duration=0.182431, state=success, executor_state=success, try_number=1, max_tries=1, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:23.069712+00:00, queued_by_job_id=17, pid=14400[0m
[[34m2026-02-11T16:18:32.031+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-14 00:00:00+00:00, run_after=2026-01-15 00:00:00+00:00[0m
[[34m2026-02-11T16:18:32.034+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-14 00:00:00+00:00, run_after=2026-01-15 00:00:00+00:00[0m
[[34m2026-02-11T16:18:32.058+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-13 00:00:00+00:00: scheduled__2026-01-13T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:23.009874+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:32.059+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-13 00:00:00+00:00, run_id=scheduled__2026-01-13T00:00:00+00:00, run_start_date=2026-02-11 08:18:23.026938+00:00, run_end_date=2026-02-11 08:18:32.059677+00:00, run_duration=9.032739, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-13 00:00:00+00:00, data_interval_end=2026-01-14 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:18:32.064+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-14 00:00:00+00:00, run_after=2026-01-15 00:00:00+00:00[0m
[[34m2026-02-11T16:18:32.074+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:32.075+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:32.076+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-13T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:32.078+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:32.079+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-13T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:32.080+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:32.082+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-13T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:34.678+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:36.179+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-13T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:37.268+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-13T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:37.282+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-13T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:36.276644+00:00, run_end_date=2026-02-11 08:18:36.458118+00:00, run_duration=0.181474, state=success, executor_state=success, try_number=1, max_tries=1, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:32.077447+00:00, queued_by_job_id=17, pid=14404[0m
[[34m2026-02-11T16:18:40.727+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-15 00:00:00+00:00, run_after=2026-01-16 00:00:00+00:00[0m
[[34m2026-02-11T16:18:40.734+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-15 00:00:00+00:00, run_after=2026-01-16 00:00:00+00:00[0m
[[34m2026-02-11T16:18:40.766+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-13 00:00:00+00:00: scheduled__2026-01-13T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:32.025016+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:40.768+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-13 00:00:00+00:00, run_id=scheduled__2026-01-13T00:00:00+00:00, run_start_date=2026-02-11 08:18:32.045017+00:00, run_end_date=2026-02-11 08:18:40.768092+00:00, run_duration=8.723075, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-13 00:00:00+00:00, data_interval_end=2026-01-14 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:18:40.774+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-14 00:00:00+00:00, run_after=2026-01-15 00:00:00+00:00[0m
[[34m2026-02-11T16:18:40.786+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:40.787+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:40.787+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:40.788+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-14T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-14T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:40.791+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:40.791+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:40.792+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:40.793+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:40.794+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-14T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:40.794+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:40.796+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:43.367+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:44.789+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-14T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:45.878+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-14T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:48.524+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:50.126+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-14T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:18:51.222+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:51.225+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-14T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:18:51.238+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-14T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:44.878939+00:00, run_end_date=2026-02-11 08:18:45.100187+00:00, run_duration=0.221248, state=success, executor_state=success, try_number=1, max_tries=1, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:40.790087+00:00, queued_by_job_id=17, pid=14408[0m
[[34m2026-02-11T16:18:51.239+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-14T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:50.216433+00:00, run_end_date=2026-02-11 08:18:50.423340+00:00, run_duration=0.206907, state=success, executor_state=success, try_number=1, max_tries=1, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:40.790087+00:00, queued_by_job_id=17, pid=14411[0m
[[34m2026-02-11T16:18:54.944+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-15 00:00:00+00:00, run_after=2026-01-16 00:00:00+00:00[0m
[[34m2026-02-11T16:18:54.953+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-16 00:00:00+00:00, run_after=2026-01-17 00:00:00+00:00[0m
[[34m2026-02-11T16:18:54.981+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-14 00:00:00+00:00: scheduled__2026-01-14T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:40.723046+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:54.982+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-14 00:00:00+00:00, run_id=scheduled__2026-01-14T00:00:00+00:00, run_start_date=2026-02-11 08:18:40.745641+00:00, run_end_date=2026-02-11 08:18:54.982611+00:00, run_duration=14.23697, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-14 00:00:00+00:00, data_interval_end=2026-01-15 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:18:54.989+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-15 00:00:00+00:00, run_after=2026-01-16 00:00:00+00:00[0m
[[34m2026-02-11T16:18:54.994+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-14 00:00:00+00:00: scheduled__2026-01-14T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:40.730171+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:18:54.995+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-14 00:00:00+00:00, run_id=scheduled__2026-01-14T00:00:00+00:00, run_start_date=2026-02-11 08:18:40.746431+00:00, run_end_date=2026-02-11 08:18:54.995605+00:00, run_duration=14.249174, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-14 00:00:00+00:00, data_interval_end=2026-01-15 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:18:55.000+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-15 00:00:00+00:00, run_after=2026-01-16 00:00:00+00:00[0m
[[34m2026-02-11T16:18:55.018+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:55.019+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:18:55.020+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:18:55.022+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:18:55.024+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:18:55.024+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:55.027+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:18:57.734+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:18:59.068+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-15T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:00.671+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:00.723+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-15T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:18:59.152750+00:00, run_end_date=2026-02-11 08:18:59.873348+00:00, run_duration=0.720598, state=success, executor_state=success, try_number=1, max_tries=1, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:18:55.021613+00:00, queued_by_job_id=17, pid=14415[0m
[[34m2026-02-11T16:19:04.010+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-16 00:00:00+00:00, run_after=2026-01-17 00:00:00+00:00[0m
[[34m2026-02-11T16:19:04.013+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-16 00:00:00+00:00, run_after=2026-01-17 00:00:00+00:00[0m
[[34m2026-02-11T16:19:04.037+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-15 00:00:00+00:00: scheduled__2026-01-15T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:18:54.946841+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:04.038+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-15 00:00:00+00:00, run_id=scheduled__2026-01-15T00:00:00+00:00, run_start_date=2026-02-11 08:18:54.963583+00:00, run_end_date=2026-02-11 08:19:04.038816+00:00, run_duration=9.075233, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-15 00:00:00+00:00, data_interval_end=2026-01-16 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:19:04.043+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-16 00:00:00+00:00, run_after=2026-01-17 00:00:00+00:00[0m
[[34m2026-02-11T16:19:04.054+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:04.054+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:04.055+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-15T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:04.057+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:04.058+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-15T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:04.059+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:04.061+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:06.395+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:07.741+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-15T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:09.175+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-15T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:09.190+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-15T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:07.824451+00:00, run_end_date=2026-02-11 08:19:08.351640+00:00, run_duration=0.527189, state=success, executor_state=success, try_number=1, max_tries=1, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:04.056468+00:00, queued_by_job_id=17, pid=14422[0m
[[34m2026-02-11T16:19:12.203+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-17 00:00:00+00:00, run_after=2026-01-18 00:00:00+00:00[0m
[[34m2026-02-11T16:19:12.212+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-17 00:00:00+00:00, run_after=2026-01-18 00:00:00+00:00[0m
[[34m2026-02-11T16:19:12.239+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-15 00:00:00+00:00: scheduled__2026-01-15T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:04.004438+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:12.240+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-15 00:00:00+00:00, run_id=scheduled__2026-01-15T00:00:00+00:00, run_start_date=2026-02-11 08:19:04.023663+00:00, run_end_date=2026-02-11 08:19:12.240797+00:00, run_duration=8.217134, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-15 00:00:00+00:00, data_interval_end=2026-01-16 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:19:12.244+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-16 00:00:00+00:00, run_after=2026-01-17 00:00:00+00:00[0m
[[34m2026-02-11T16:19:12.256+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:12.257+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:12.258+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:12.258+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-16T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-16T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:12.261+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:12.261+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:12.263+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:12.263+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:12.264+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-16T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:12.265+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:12.266+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:14.685+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:15.930+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-16T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:16.846+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-16T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:19.182+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:20.475+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-16T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:21.693+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:21.696+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-16T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:21.707+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-16T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:20.561785+00:00, run_end_date=2026-02-11 08:19:20.983555+00:00, run_duration=0.42177, state=success, executor_state=success, try_number=1, max_tries=1, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:12.259792+00:00, queued_by_job_id=17, pid=14429[0m
[[34m2026-02-11T16:19:21.708+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-16T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:16.013693+00:00, run_end_date=2026-02-11 08:19:16.204040+00:00, run_duration=0.190347, state=success, executor_state=success, try_number=1, max_tries=1, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:12.259792+00:00, queued_by_job_id=17, pid=14426[0m
[[34m2026-02-11T16:19:24.725+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-17 00:00:00+00:00, run_after=2026-01-18 00:00:00+00:00[0m
[[34m2026-02-11T16:19:24.734+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-18 00:00:00+00:00, run_after=2026-01-19 00:00:00+00:00[0m
[[34m2026-02-11T16:19:24.757+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-16 00:00:00+00:00: scheduled__2026-01-16T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:12.205642+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:24.758+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-16 00:00:00+00:00, run_id=scheduled__2026-01-16T00:00:00+00:00, run_start_date=2026-02-11 08:19:12.222241+00:00, run_end_date=2026-02-11 08:19:24.757983+00:00, run_duration=12.535742, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-16 00:00:00+00:00, data_interval_end=2026-01-17 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:19:24.762+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-17 00:00:00+00:00, run_after=2026-01-18 00:00:00+00:00[0m
[[34m2026-02-11T16:19:24.766+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-16 00:00:00+00:00: scheduled__2026-01-16T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:12.198175+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:24.767+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-16 00:00:00+00:00, run_id=scheduled__2026-01-16T00:00:00+00:00, run_start_date=2026-02-11 08:19:12.222964+00:00, run_end_date=2026-02-11 08:19:24.767026+00:00, run_duration=12.544062, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-16 00:00:00+00:00, data_interval_end=2026-01-17 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:19:24.770+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-17 00:00:00+00:00, run_after=2026-01-18 00:00:00+00:00[0m
[[34m2026-02-11T16:19:24.781+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:24.782+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:24.782+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:24.785+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:24.786+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:24.787+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:24.789+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:27.212+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:28.569+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-17T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:29.731+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:29.744+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-17T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:28.651192+00:00, run_end_date=2026-02-11 08:19:29.072135+00:00, run_duration=0.420943, state=success, executor_state=success, try_number=1, max_tries=1, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:24.784065+00:00, queued_by_job_id=17, pid=14434[0m
[[34m2026-02-11T16:19:29.766+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:19:32.788+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-18 00:00:00+00:00, run_after=2026-01-19 00:00:00+00:00[0m
[[34m2026-02-11T16:19:32.790+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-18 00:00:00+00:00, run_after=2026-01-19 00:00:00+00:00[0m
[[34m2026-02-11T16:19:32.815+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-17 00:00:00+00:00: scheduled__2026-01-17T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:24.728859+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:32.816+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-17 00:00:00+00:00, run_id=scheduled__2026-01-17T00:00:00+00:00, run_start_date=2026-02-11 08:19:24.742697+00:00, run_end_date=2026-02-11 08:19:32.816342+00:00, run_duration=8.073645, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-17 00:00:00+00:00, data_interval_end=2026-01-18 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:19:32.820+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-18 00:00:00+00:00, run_after=2026-01-19 00:00:00+00:00[0m
[[34m2026-02-11T16:19:32.830+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:32.831+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:32.832+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-17T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:32.834+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:32.835+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-17T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:32.835+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:32.837+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-17T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:36.088+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:37.664+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-17T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:38.620+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-17T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:38.634+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-17T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:37.751081+00:00, run_end_date=2026-02-11 08:19:37.964057+00:00, run_duration=0.212976, state=success, executor_state=success, try_number=1, max_tries=1, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:32.833125+00:00, queued_by_job_id=17, pid=14439[0m
[[34m2026-02-11T16:19:41.953+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-19 00:00:00+00:00, run_after=2026-01-20 00:00:00+00:00[0m
[[34m2026-02-11T16:19:41.960+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-19 00:00:00+00:00, run_after=2026-01-20 00:00:00+00:00[0m
[[34m2026-02-11T16:19:41.989+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-17 00:00:00+00:00: scheduled__2026-01-17T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:32.782990+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:41.990+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-17 00:00:00+00:00, run_id=scheduled__2026-01-17T00:00:00+00:00, run_start_date=2026-02-11 08:19:32.801031+00:00, run_end_date=2026-02-11 08:19:41.990251+00:00, run_duration=9.18922, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-17 00:00:00+00:00, data_interval_end=2026-01-18 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:19:41.994+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-18 00:00:00+00:00, run_after=2026-01-19 00:00:00+00:00[0m
[[34m2026-02-11T16:19:42.006+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:42.007+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:42.008+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:42.008+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-18T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-18T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:42.012+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:42.013+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:42.014+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:42.015+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:42.015+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-18T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:42.016+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:42.018+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:44.556+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:46.238+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-18T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:47.218+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-18T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:49.836+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:19:51.220+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-18T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:19:52.760+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:52.765+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-18T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:19:52.790+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-18T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:51.306641+00:00, run_end_date=2026-02-11 08:19:51.823840+00:00, run_duration=0.517199, state=success, executor_state=success, try_number=1, max_tries=1, job_id=59, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:42.010024+00:00, queued_by_job_id=17, pid=14455[0m
[[34m2026-02-11T16:19:52.792+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-18T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:19:46.327874+00:00, run_end_date=2026-02-11 08:19:46.521321+00:00, run_duration=0.193447, state=success, executor_state=success, try_number=1, max_tries=1, job_id=58, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:42.010024+00:00, queued_by_job_id=17, pid=14450[0m
[[34m2026-02-11T16:19:56.070+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-19 00:00:00+00:00, run_after=2026-01-20 00:00:00+00:00[0m
[[34m2026-02-11T16:19:56.078+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-20 00:00:00+00:00, run_after=2026-01-21 00:00:00+00:00[0m
[[34m2026-02-11T16:19:56.104+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-18 00:00:00+00:00: scheduled__2026-01-18T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:41.955773+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:56.105+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-18 00:00:00+00:00, run_id=scheduled__2026-01-18T00:00:00+00:00, run_start_date=2026-02-11 08:19:41.970304+00:00, run_end_date=2026-02-11 08:19:56.105494+00:00, run_duration=14.13519, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-18 00:00:00+00:00, data_interval_end=2026-01-19 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:19:56.110+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-19 00:00:00+00:00, run_after=2026-01-20 00:00:00+00:00[0m
[[34m2026-02-11T16:19:56.113+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-18 00:00:00+00:00: scheduled__2026-01-18T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:41.948024+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:19:56.114+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-18 00:00:00+00:00, run_id=scheduled__2026-01-18T00:00:00+00:00, run_start_date=2026-02-11 08:19:41.971264+00:00, run_end_date=2026-02-11 08:19:56.114369+00:00, run_duration=14.143105, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-18 00:00:00+00:00, data_interval_end=2026-01-19 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:19:56.118+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-19 00:00:00+00:00, run_after=2026-01-20 00:00:00+00:00[0m
[[34m2026-02-11T16:19:56.130+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:56.131+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:19:56.131+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:19:56.134+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:19:56.135+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:19:56.136+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:56.138+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:19:58.614+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:00.015+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-19T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:01.248+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:01.262+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-19T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:00.105255+00:00, run_end_date=2026-02-11 08:20:00.556792+00:00, run_duration=0.451537, state=success, executor_state=success, try_number=1, max_tries=1, job_id=60, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:19:56.132850+00:00, queued_by_job_id=17, pid=14462[0m
[[34m2026-02-11T16:20:04.749+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-20 00:00:00+00:00, run_after=2026-01-21 00:00:00+00:00[0m
[[34m2026-02-11T16:20:04.752+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-20 00:00:00+00:00, run_after=2026-01-21 00:00:00+00:00[0m
[[34m2026-02-11T16:20:04.781+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-19 00:00:00+00:00: scheduled__2026-01-19T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:19:56.072291+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:04.782+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-19 00:00:00+00:00, run_id=scheduled__2026-01-19T00:00:00+00:00, run_start_date=2026-02-11 08:19:56.088835+00:00, run_end_date=2026-02-11 08:20:04.782479+00:00, run_duration=8.693644, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-19 00:00:00+00:00, data_interval_end=2026-01-20 00:00:00+00:00, dag_hash=786b1512e65069b3a18aabcd41909504[0m
[[34m2026-02-11T16:20:04.788+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-20 00:00:00+00:00, run_after=2026-01-21 00:00:00+00:00[0m
[[34m2026-02-11T16:20:04.803+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:04.804+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:04.805+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-19T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:04.807+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:04.809+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-19T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:04.810+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:04.812+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-19T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:07.767+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:09.165+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-19T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:10.457+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-19T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:10.473+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-19T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:09.250730+00:00, run_end_date=2026-02-11 08:20:09.706675+00:00, run_duration=0.455945, state=success, executor_state=success, try_number=1, max_tries=1, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:04.806032+00:00, queued_by_job_id=17, pid=14470[0m
[[34m2026-02-11T16:20:14.684+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-21 00:00:00+00:00, run_after=2026-01-22 00:00:00+00:00[0m
[[34m2026-02-11T16:20:14.689+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-21 00:00:00+00:00, run_after=2026-01-22 00:00:00+00:00[0m
[[34m2026-02-11T16:20:14.718+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-19 00:00:00+00:00: scheduled__2026-01-19T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:04.743999+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:14.719+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-19 00:00:00+00:00, run_id=scheduled__2026-01-19T00:00:00+00:00, run_start_date=2026-02-11 08:20:04.763068+00:00, run_end_date=2026-02-11 08:20:14.719414+00:00, run_duration=9.956346, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-19 00:00:00+00:00, data_interval_end=2026-01-20 00:00:00+00:00, dag_hash=173eef9cfaeb86d6b1c942f5c29626cb[0m
[[34m2026-02-11T16:20:14.723+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-20 00:00:00+00:00, run_after=2026-01-21 00:00:00+00:00[0m
[[34m2026-02-11T16:20:14.735+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:14.736+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:14.737+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:14.737+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-20T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-20T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:14.740+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:14.741+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:14.742+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:14.744+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:14.744+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-20T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:14.745+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:14.748+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:17.533+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:18.898+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-20T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:19.779+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-20T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:22.515+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:23.834+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-20T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:24.922+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:24.926+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-20T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:24.940+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-20T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:18.982846+00:00, run_end_date=2026-02-11 08:20:19.178813+00:00, run_duration=0.195967, state=success, executor_state=success, try_number=1, max_tries=1, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:14.739051+00:00, queued_by_job_id=17, pid=14485[0m
[[34m2026-02-11T16:20:24.941+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-20T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:23.915884+00:00, run_end_date=2026-02-11 08:20:24.113260+00:00, run_duration=0.197376, state=success, executor_state=success, try_number=1, max_tries=1, job_id=63, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:14.739051+00:00, queued_by_job_id=17, pid=14491[0m
[[34m2026-02-11T16:20:29.475+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-25 00:00:00+00:00, run_after=2026-02-01 00:00:00+00:00[0m
[[34m2026-02-11T16:20:29.480+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-25 00:00:00+00:00, run_after=2026-02-01 00:00:00+00:00[0m
[[34m2026-02-11T16:20:29.499+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-20 00:00:00+00:00: scheduled__2026-01-20T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:14.678751+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:29.500+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-20 00:00:00+00:00, run_id=scheduled__2026-01-20T00:00:00+00:00, run_start_date=2026-02-11 08:20:14.699618+00:00, run_end_date=2026-02-11 08:20:29.500344+00:00, run_duration=14.800726, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-20 00:00:00+00:00, data_interval_end=2026-01-21 00:00:00+00:00, dag_hash=e6456c1fada1806526fd4d2f401be87d[0m
[[34m2026-02-11T16:20:29.504+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-18 00:00:00+00:00, run_after=2026-01-25 00:00:00+00:00[0m
[[34m2026-02-11T16:20:29.510+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-20 00:00:00+00:00: scheduled__2026-01-20T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:14.685721+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:29.511+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-20 00:00:00+00:00, run_id=scheduled__2026-01-20T00:00:00+00:00, run_start_date=2026-02-11 08:20:14.700417+00:00, run_end_date=2026-02-11 08:20:29.511375+00:00, run_duration=14.810958, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-20 00:00:00+00:00, data_interval_end=2026-01-21 00:00:00+00:00, dag_hash=f42beef0c58e9d6479b9e689a95ccd00[0m
[[34m2026-02-11T16:20:29.515+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-18 00:00:00+00:00, run_after=2026-01-25 00:00:00+00:00[0m
[[34m2026-02-11T16:20:32.845+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-01-25 00:00:00+00:00, run_after=2026-02-01 00:00:00+00:00[0m
[[34m2026-02-11T16:20:32.847+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-01-25 00:00:00+00:00, run_after=2026-02-01 00:00:00+00:00[0m
[[34m2026-02-11T16:20:36.816+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-02-01 00:00:00+00:00, run_after=2026-02-08 00:00:00+00:00[0m
[[34m2026-02-11T16:20:36.820+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-01 00:00:00+00:00, run_after=2026-02-08 00:00:00+00:00[0m
[[34m2026-02-11T16:20:36.854+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:36.855+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:36.855+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:36.856+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_3.data_check scheduled__2026-01-25T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_2.data_check scheduled__2026-01-25T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:36.859+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:36.859+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:36.860+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:36.861+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:36.862+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-25T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:36.863+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:36.865+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:39.279+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:40.725+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-01-25T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:41.892+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-01-25T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:44.406+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:46.681+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-01-25T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:47.797+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-01-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:47.805+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-01-25T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:20:47.818+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-01-25T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:46.777290+00:00, run_end_date=2026-02-11 08:20:46.978641+00:00, run_duration=0.201351, state=success, executor_state=success, try_number=1, max_tries=1, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:36.858122+00:00, queued_by_job_id=17, pid=14514[0m
[[34m2026-02-11T16:20:47.819+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-01-25T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:40.811040+00:00, run_end_date=2026-02-11 08:20:41.256889+00:00, run_duration=0.445849, state=success, executor_state=success, try_number=1, max_tries=1, job_id=64, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:36.858122+00:00, queued_by_job_id=17, pid=14511[0m
[[34m2026-02-11T16:20:51.053+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:20:51.064+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:20:51.106+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-01-25 00:00:00+00:00: scheduled__2026-01-25T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:36.817443+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:51.107+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-01-25 00:00:00+00:00, run_id=scheduled__2026-01-25T00:00:00+00:00, run_start_date=2026-02-11 08:20:36.831358+00:00, run_end_date=2026-02-11 08:20:51.107528+00:00, run_duration=14.27617, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-25 00:00:00+00:00, data_interval_end=2026-02-01 00:00:00+00:00, dag_hash=e6456c1fada1806526fd4d2f401be87d[0m
[[34m2026-02-11T16:20:51.112+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-01 00:00:00+00:00, run_after=2026-02-08 00:00:00+00:00[0m
[[34m2026-02-11T16:20:51.117+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-01-25 00:00:00+00:00: scheduled__2026-01-25T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:36.811531+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:20:51.117+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-01-25 00:00:00+00:00, run_id=scheduled__2026-01-25T00:00:00+00:00, run_start_date=2026-02-11 08:20:36.831555+00:00, run_end_date=2026-02-11 08:20:51.117881+00:00, run_duration=14.286326, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-01-25 00:00:00+00:00, data_interval_end=2026-02-01 00:00:00+00:00, dag_hash=f42beef0c58e9d6479b9e689a95ccd00[0m
[[34m2026-02-11T16:20:51.122+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-02-01 00:00:00+00:00, run_after=2026-02-08 00:00:00+00:00[0m
[[34m2026-02-11T16:20:51.137+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 2 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:51.138+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:51.140+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_3 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:20:51.141+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: test_workflow_3.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:20:51.144+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:51.144+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:20:51.146+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:51.147+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:51.148+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:20:51.149+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:51.151+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:53.914+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:20:55.476+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:20:56.427+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_3', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:20:58.921+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:21:00.437+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_3.data_check scheduled__2026-02-01T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:21:01.466+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:21:01.470+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_3', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:21:01.483+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-02-01T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:20:55.568240+00:00, run_end_date=2026-02-11 08:20:55.809995+00:00, run_duration=0.241755, state=success, executor_state=success, try_number=1, max_tries=1, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:51.142678+00:00, queued_by_job_id=17, pid=14518[0m
[[34m2026-02-11T16:21:01.484+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_3, task_id=data_check, run_id=scheduled__2026-02-01T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:21:00.523172+00:00, run_end_date=2026-02-11 08:21:00.749146+00:00, run_duration=0.225974, state=success, executor_state=success, try_number=1, max_tries=1, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:20:51.142678+00:00, queued_by_job_id=17, pid=14521[0m
[[34m2026-02-11T16:21:05.023+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:21:05.028+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:21:05.045+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-02-01 00:00:00+00:00: scheduled__2026-02-01T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:51.036057+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:21:05.046+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-02-01 00:00:00+00:00, run_id=scheduled__2026-02-01T00:00:00+00:00, run_start_date=2026-02-11 08:20:51.079205+00:00, run_end_date=2026-02-11 08:21:05.046658+00:00, run_duration=13.967453, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-01 00:00:00+00:00, data_interval_end=2026-02-08 00:00:00+00:00, dag_hash=e6456c1fada1806526fd4d2f401be87d[0m
[[34m2026-02-11T16:21:05.051+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:21:05.056+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_3 @ 2026-02-01 00:00:00+00:00: scheduled__2026-02-01T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:20:51.057996+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:21:05.057+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_3, execution_date=2026-02-01 00:00:00+00:00, run_id=scheduled__2026-02-01T00:00:00+00:00, run_start_date=2026-02-11 08:20:51.080765+00:00, run_end_date=2026-02-11 08:21:05.057590+00:00, run_duration=13.976825, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-01 00:00:00+00:00, data_interval_end=2026-02-08 00:00:00+00:00, dag_hash=f42beef0c58e9d6479b9e689a95ccd00[0m
[[34m2026-02-11T16:21:05.061+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_3 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:21:18.740+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:21:17.875843+00:00 [scheduled]>[0m
[[34m2026-02-11T16:21:18.741+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:21:18.741+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:21:17.875843+00:00 [scheduled]>[0m
[[34m2026-02-11T16:21:18.744+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:21:18.745+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:21:17.875843+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:21:18.745+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:21:17.875843+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:21:18.747+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:21:17.875843+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:21:21.705+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:21:23.602+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:21:17.875843+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:21:24.947+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:21:17.875843+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:21:24.970+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:21:17.875843+00:00, map_index=-1, run_start_date=2026-02-11 08:21:23.758095+00:00, run_end_date=2026-02-11 08:21:24.146324+00:00, run_duration=0.388229, state=success, executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:21:18.742730+00:00, queued_by_job_id=17, pid=14533[0m
[[34m2026-02-11T16:21:28.225+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:21:17.875843+00:00 [scheduled]>[0m
[[34m2026-02-11T16:21:28.225+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:21:28.226+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:21:17.875843+00:00 [scheduled]>[0m
[[34m2026-02-11T16:21:28.228+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:21:28.229+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:21:17.875843+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:21:28.230+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:21:17.875843+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:21:28.232+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:21:17.875843+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:21:30.669+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:21:31.976+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:21:17.875843+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:21:33.021+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:21:17.875843+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:21:33.035+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:21:17.875843+00:00, map_index=-1, run_start_date=2026-02-11 08:21:32.060912+00:00, run_end_date=2026-02-11 08:21:32.300165+00:00, run_duration=0.239253, state=success, executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:21:28.227377+00:00, queued_by_job_id=17, pid=14537[0m
[[34m2026-02-11T16:21:36.191+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:21:17.875843+00:00: manual__2026-02-11T08:21:17.875843+00:00, state:running, queued_at: 2026-02-11 08:21:17.902644+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:21:36.192+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:21:17.875843+00:00, run_id=manual__2026-02-11T08:21:17.875843+00:00, run_start_date=2026-02-11 08:21:18.710684+00:00, run_end_date=2026-02-11 08:21:36.192140+00:00, run_duration=17.481456, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:22:27.737+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:22:25.610413+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:27.738+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:22:27.739+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:22:25.610413+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:27.741+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:22:27.742+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:22:25.610413+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:22:27.743+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:22:25.610413+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:22:27.745+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:22:25.610413+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:22:30.265+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:22:32.597+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:22:25.610413+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:22:33.706+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:22:25.610413+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:22:33.720+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:22:25.610413+00:00, map_index=-1, run_start_date=2026-02-11 08:22:32.708161+00:00, run_end_date=2026-02-11 08:22:32.933960+00:00, run_duration=0.225799, state=success, executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:22:27.740574+00:00, queued_by_job_id=17, pid=14599[0m
[[34m2026-02-11T16:22:37.036+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:22:25.610413+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:37.037+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:22:37.038+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:22:25.610413+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:37.041+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:22:37.042+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:22:25.610413+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:22:37.043+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:22:25.610413+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:22:37.045+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:22:25.610413+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:22:39.482+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:22:40.784+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:22:25.610413+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:22:41.778+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:22:25.610413+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:22:41.792+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:22:25.610413+00:00, map_index=-1, run_start_date=2026-02-11 08:22:40.865311+00:00, run_end_date=2026-02-11 08:22:41.114837+00:00, run_duration=0.249526, state=success, executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:22:37.040105+00:00, queued_by_job_id=17, pid=14604[0m
[[34m2026-02-11T16:22:44.986+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:22:25.610413+00:00: manual__2026-02-11T08:22:25.610413+00:00, state:running, queued_at: 2026-02-11 08:22:25.625851+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:22:44.988+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:22:25.610413+00:00, run_id=manual__2026-02-11T08:22:25.610413+00:00, run_start_date=2026-02-11 08:22:27.714455+00:00, run_end_date=2026-02-11 08:22:44.988068+00:00, run_duration=17.273613, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:22:49.222+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:22:49.250+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:49.251+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG test_workflow_2 has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:22:49.251+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2026-02-11T16:22:49.254+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task data_check because previous state change time has not been saved[0m
[[34m2026-02-11T16:22:49.255+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:22:49.256+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:22:49.258+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'test_workflow_2', 'data_check', 'scheduled__2026-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_workflow_dag.py'][0m
[[34m2026-02-11T16:22:51.691+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/test_workflow_dag.py[0m
[[34m2026-02-11T16:22:52.955+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: test_workflow_2.data_check scheduled__2026-02-01T00:00:00+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:22:53.955+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_workflow_2', task_id='data_check', run_id='scheduled__2026-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:22:53.969+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=test_workflow_2, task_id=data_check, run_id=scheduled__2026-02-01T00:00:00+00:00, map_index=-1, run_start_date=2026-02-11 08:22:53.040114+00:00, run_end_date=2026-02-11 08:22:53.226433+00:00, run_duration=0.186319, state=success, executor_state=success, try_number=1, max_tries=1, job_id=72, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2026-02-11 08:22:49.253181+00:00, queued_by_job_id=17, pid=14609[0m
[[34m2026-02-11T16:22:57.520+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun test_workflow_2 @ 2026-02-01 00:00:00+00:00: scheduled__2026-02-01T00:00:00+00:00, state:running, queued_at: 2026-02-11 08:22:49.217246+00:00. externally triggered: False> successful[0m
[[34m2026-02-11T16:22:57.522+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=test_workflow_2, execution_date=2026-02-01 00:00:00+00:00, run_id=scheduled__2026-02-01T00:00:00+00:00, run_start_date=2026-02-11 08:22:49.231033+00:00, run_end_date=2026-02-11 08:22:57.522212+00:00, run_duration=8.291179, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-01 00:00:00+00:00, data_interval_end=2026-02-08 00:00:00+00:00, dag_hash=e6456c1fada1806526fd4d2f401be87d[0m
[[34m2026-02-11T16:22:57.537+0800[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for test_workflow_2 to 2026-02-08 00:00:00+00:00, run_after=2026-02-15 00:00:00+00:00[0m
[[34m2026-02-11T16:24:32.448+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:25:11.146+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:25:08.734726+00:00 [scheduled]>[0m
[[34m2026-02-11T16:25:11.147+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:25:11.148+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:25:08.734726+00:00 [scheduled]>[0m
[[34m2026-02-11T16:25:11.150+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:25:11.152+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:25:08.734726+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:25:11.153+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:25:08.734726+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:25:11.155+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:25:08.734726+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:25:13.583+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:25:14.900+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:25:08.734726+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:25:15.817+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:25:08.734726+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:25:15.826+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:25:08.734726+00:00, map_index=-1, run_start_date=2026-02-11 08:25:14.987021+00:00, run_end_date=2026-02-11 08:25:15.182197+00:00, run_duration=0.195176, state=success, executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:25:11.149247+00:00, queued_by_job_id=17, pid=14780[0m
[[34m2026-02-11T16:25:19.513+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:25:08.734726+00:00 [scheduled]>[0m
[[34m2026-02-11T16:25:19.514+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:25:19.515+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:25:08.734726+00:00 [scheduled]>[0m
[[34m2026-02-11T16:25:19.518+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:25:19.519+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:25:08.734726+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:25:19.520+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:25:08.734726+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:25:19.522+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:25:08.734726+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:25:22.480+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:25:23.995+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:25:08.734726+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:25:25.016+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:25:08.734726+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:25:25.028+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:25:08.734726+00:00, map_index=-1, run_start_date=2026-02-11 08:25:24.082162+00:00, run_end_date=2026-02-11 08:25:24.335264+00:00, run_duration=0.253102, state=success, executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:25:19.516134+00:00, queued_by_job_id=17, pid=14784[0m
[[34m2026-02-11T16:25:28.045+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:25:08.734726+00:00: manual__2026-02-11T08:25:08.734726+00:00, state:running, queued_at: 2026-02-11 08:25:08.755026+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:25:28.046+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:25:08.734726+00:00, run_id=manual__2026-02-11T08:25:08.734726+00:00, run_start_date=2026-02-11 08:25:11.117941+00:00, run_end_date=2026-02-11 08:25:28.046170+00:00, run_duration=16.928229, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:27:42.550+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:27:41.927230+00:00 [scheduled]>[0m
[[34m2026-02-11T16:27:42.551+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:27:42.552+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:27:41.927230+00:00 [scheduled]>[0m
[[34m2026-02-11T16:27:42.555+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:27:42.556+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:27:41.927230+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:27:42.558+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:27:41.927230+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:27:42.563+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:27:41.927230+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:27:45.461+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:27:47.216+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:27:41.927230+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:27:48.382+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:27:41.927230+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:27:48.392+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:27:41.927230+00:00, map_index=-1, run_start_date=2026-02-11 08:27:47.342287+00:00, run_end_date=2026-02-11 08:27:47.621045+00:00, run_duration=0.278758, state=success, executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:27:42.553086+00:00, queued_by_job_id=17, pid=14863[0m
[[34m2026-02-11T16:27:51.361+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:27:41.927230+00:00 [scheduled]>[0m
[[34m2026-02-11T16:27:51.362+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:27:51.363+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:27:41.927230+00:00 [scheduled]>[0m
[[34m2026-02-11T16:27:51.365+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:27:51.366+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:27:41.927230+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:27:51.367+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:27:41.927230+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:27:51.369+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:27:41.927230+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:27:53.902+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:27:55.243+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:27:41.927230+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:27:56.208+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:27:41.927230+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:27:56.218+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:27:41.927230+00:00, map_index=-1, run_start_date=2026-02-11 08:27:55.324436+00:00, run_end_date=2026-02-11 08:27:55.565804+00:00, run_duration=0.241368, state=success, executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:27:51.364296+00:00, queued_by_job_id=17, pid=14867[0m
[[34m2026-02-11T16:27:59.193+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:27:41.927230+00:00: manual__2026-02-11T08:27:41.927230+00:00, state:running, queued_at: 2026-02-11 08:27:41.941588+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:27:59.194+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:27:41.927230+00:00, run_id=manual__2026-02-11T08:27:41.927230+00:00, run_start_date=2026-02-11 08:27:42.522243+00:00, run_end_date=2026-02-11 08:27:59.194164+00:00, run_duration=16.671921, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:29:35.782+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:32:26.935+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:32:24.586692+00:00 [scheduled]>[0m
[[34m2026-02-11T16:32:26.936+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:32:26.937+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:32:24.586692+00:00 [scheduled]>[0m
[[34m2026-02-11T16:32:26.939+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:32:26.940+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:32:24.586692+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:32:26.941+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:32:24.586692+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:32:26.943+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:32:24.586692+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:32:29.300+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:32:30.696+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:32:24.586692+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:32:31.626+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:32:24.586692+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:32:31.635+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:32:24.586692+00:00, map_index=-1, run_start_date=2026-02-11 08:32:30.779946+00:00, run_end_date=2026-02-11 08:32:30.970102+00:00, run_duration=0.190156, state=success, executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:32:26.937988+00:00, queued_by_job_id=17, pid=15245[0m
[[34m2026-02-11T16:32:35.354+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:32:24.586692+00:00 [scheduled]>[0m
[[34m2026-02-11T16:32:35.356+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:32:35.357+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:32:24.586692+00:00 [scheduled]>[0m
[[34m2026-02-11T16:32:35.359+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:32:35.360+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:32:24.586692+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:32:35.361+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:32:24.586692+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:32:35.363+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:32:24.586692+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:32:37.904+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:32:39.232+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:32:24.586692+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:32:40.205+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:32:24.586692+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:32:40.215+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:32:24.586692+00:00, map_index=-1, run_start_date=2026-02-11 08:32:39.313427+00:00, run_end_date=2026-02-11 08:32:39.565887+00:00, run_duration=0.25246, state=success, executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:32:35.358605+00:00, queued_by_job_id=17, pid=15250[0m
[[34m2026-02-11T16:32:43.179+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:32:24.586692+00:00: manual__2026-02-11T08:32:24.586692+00:00, state:running, queued_at: 2026-02-11 08:32:24.640884+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:32:43.180+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:32:24.586692+00:00, run_id=manual__2026-02-11T08:32:24.586692+00:00, run_start_date=2026-02-11 08:32:26.882805+00:00, run_end_date=2026-02-11 08:32:43.180512+00:00, run_duration=16.297707, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:34:36.113+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:39:36.330+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:44:37.582+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:47:04.870+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:47:01.063385+00:00 [scheduled]>[0m
[[34m2026-02-11T16:47:04.871+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:47:04.872+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:47:01.063385+00:00 [scheduled]>[0m
[[34m2026-02-11T16:47:04.874+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:47:04.875+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:47:01.063385+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:47:04.876+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:47:01.063385+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:47:04.878+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:47:01.063385+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:47:07.339+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:47:08.740+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:47:01.063385+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:47:09.876+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:47:01.063385+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:47:09.888+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:47:01.063385+00:00, map_index=-1, run_start_date=2026-02-11 08:47:08.829141+00:00, run_end_date=2026-02-11 08:47:09.137864+00:00, run_duration=0.308723, state=success, executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:47:04.873114+00:00, queued_by_job_id=17, pid=16123[0m
[[34m2026-02-11T16:47:13.611+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:47:01.063385+00:00 [scheduled]>[0m
[[34m2026-02-11T16:47:13.612+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:47:13.613+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:47:01.063385+00:00 [scheduled]>[0m
[[34m2026-02-11T16:47:13.616+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:47:13.617+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:47:01.063385+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:47:13.617+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:47:01.063385+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:47:13.620+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:47:01.063385+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:47:16.175+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:47:17.571+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:47:01.063385+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:47:18.616+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:47:01.063385+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:47:18.626+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:47:01.063385+00:00, map_index=-1, run_start_date=2026-02-11 08:47:17.654334+00:00, run_end_date=2026-02-11 08:47:17.914963+00:00, run_duration=0.260629, state=success, executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:47:13.614770+00:00, queued_by_job_id=17, pid=16128[0m
[[34m2026-02-11T16:47:21.837+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:47:01.063385+00:00: manual__2026-02-11T08:47:01.063385+00:00, state:running, queued_at: 2026-02-11 08:47:01.078804+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:47:21.838+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:47:01.063385+00:00, run_id=manual__2026-02-11T08:47:01.063385+00:00, run_start_date=2026-02-11 08:47:04.847214+00:00, run_end_date=2026-02-11 08:47:21.838833+00:00, run_duration=16.991619, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:49:38.235+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:49:55.970+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:49:53.899884+00:00 [scheduled]>[0m
[[34m2026-02-11T16:49:55.971+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:49:55.972+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:49:53.899884+00:00 [scheduled]>[0m
[[34m2026-02-11T16:49:55.975+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:49:55.976+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:49:53.899884+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:49:55.977+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:49:53.899884+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:49:55.979+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:49:53.899884+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:49:58.906+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:50:00.800+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:49:53.899884+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:50:01.734+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:49:53.899884+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:50:01.743+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:49:53.899884+00:00, map_index=-1, run_start_date=2026-02-11 08:50:00.883884+00:00, run_end_date=2026-02-11 08:50:01.074978+00:00, run_duration=0.191094, state=success, executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:49:55.973443+00:00, queued_by_job_id=17, pid=16324[0m
[[34m2026-02-11T16:50:04.821+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:49:53.899884+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:04.823+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:50:04.824+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:49:53.899884+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:04.826+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:50:04.827+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:49:53.899884+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:50:04.828+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:49:53.899884+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:04.831+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:49:53.899884+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:07.466+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:50:08.791+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:49:53.899884+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:50:09.799+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:49:53.899884+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:50:09.808+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:49:53.899884+00:00, map_index=-1, run_start_date=2026-02-11 08:50:08.876502+00:00, run_end_date=2026-02-11 08:50:09.125551+00:00, run_duration=0.249049, state=success, executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:50:04.825036+00:00, queued_by_job_id=17, pid=16328[0m
[[34m2026-02-11T16:50:12.748+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:49:53.899884+00:00: manual__2026-02-11T08:49:53.899884+00:00, state:running, queued_at: 2026-02-11 08:49:53.914480+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:50:12.749+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:49:53.899884+00:00, run_id=manual__2026-02-11T08:49:53.899884+00:00, run_start_date=2026-02-11 08:49:55.940941+00:00, run_end_date=2026-02-11 08:50:12.749656+00:00, run_duration=16.808715, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:50:46.881+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:50:45.001490+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:46.882+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:50:46.882+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:50:45.001490+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:46.885+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:50:46.886+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:50:45.001490+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:50:46.886+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:50:45.001490+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:46.888+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:50:45.001490+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:49.285+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:50:51.053+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:50:45.001490+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:50:51.986+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:50:45.001490+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:50:51.995+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:50:45.001490+00:00, map_index=-1, run_start_date=2026-02-11 08:50:51.131500+00:00, run_end_date=2026-02-11 08:50:51.313371+00:00, run_duration=0.181871, state=success, executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:50:46.883783+00:00, queued_by_job_id=17, pid=16362[0m
[[34m2026-02-11T16:50:54.987+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:50:45.001490+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:54.988+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:50:54.989+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:50:45.001490+00:00 [scheduled]>[0m
[[34m2026-02-11T16:50:54.991+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:50:54.992+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:50:45.001490+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:50:54.992+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:50:45.001490+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:54.995+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:50:45.001490+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:50:57.859+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:50:59.557+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:50:45.001490+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:51:03.078+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:50:45.001490+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:51:03.091+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:50:45.001490+00:00, map_index=-1, run_start_date=2026-02-11 08:50:59.671902+00:00, run_end_date=2026-02-11 08:51:02.375633+00:00, run_duration=2.703731, state=success, executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:50:54.989871+00:00, queued_by_job_id=17, pid=16368[0m
[[34m2026-02-11T16:51:06.322+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:50:45.001490+00:00: manual__2026-02-11T08:50:45.001490+00:00, state:running, queued_at: 2026-02-11 08:50:45.023371+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:51:06.323+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:50:45.001490+00:00, run_id=manual__2026-02-11T08:50:45.001490+00:00, run_start_date=2026-02-11 08:50:46.856358+00:00, run_end_date=2026-02-11 08:51:06.323272+00:00, run_duration=19.466914, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:53:43.211+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:53:41.610702+00:00 [scheduled]>[0m
[[34m2026-02-11T16:53:43.212+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:53:43.213+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:53:41.610702+00:00 [scheduled]>[0m
[[34m2026-02-11T16:53:43.215+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T16:53:43.216+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:53:41.610702+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T16:53:43.217+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:53:41.610702+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:53:43.219+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T08:53:41.610702+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:53:45.688+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:53:47.567+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T08:53:41.610702+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:53:48.513+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T08:53:41.610702+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:53:48.524+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T08:53:41.610702+00:00, map_index=-1, run_start_date=2026-02-11 08:53:47.654036+00:00, run_end_date=2026-02-11 08:53:47.851721+00:00, run_duration=0.197685, state=success, executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 08:53:43.214149+00:00, queued_by_job_id=17, pid=16480[0m
[[34m2026-02-11T16:53:52.748+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:53:41.610702+00:00 [scheduled]>[0m
[[34m2026-02-11T16:53:52.749+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T16:53:52.750+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:53:41.610702+00:00 [scheduled]>[0m
[[34m2026-02-11T16:53:52.753+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T16:53:52.754+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:53:41.610702+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T16:53:52.754+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:53:41.610702+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:53:52.757+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T08:53:41.610702+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T16:53:55.402+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T16:53:56.853+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T08:53:41.610702+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T16:54:00.077+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T08:53:41.610702+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T16:54:00.090+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T08:53:41.610702+00:00, map_index=-1, run_start_date=2026-02-11 08:53:56.938201+00:00, run_end_date=2026-02-11 08:53:59.436258+00:00, run_duration=2.498057, state=success, executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 08:53:52.751450+00:00, queued_by_job_id=17, pid=16491[0m
[[34m2026-02-11T16:54:03.393+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 08:53:41.610702+00:00: manual__2026-02-11T08:53:41.610702+00:00, state:running, queued_at: 2026-02-11 08:53:41.624481+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T16:54:03.394+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 08:53:41.610702+00:00, run_id=manual__2026-02-11T08:53:41.610702+00:00, run_start_date=2026-02-11 08:53:43.182253+00:00, run_end_date=2026-02-11 08:54:03.394283+00:00, run_duration=20.21203, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T16:54:39.069+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T16:59:41.374+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:00:25.070+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:00:23.032126+00:00 [scheduled]>[0m
[[34m2026-02-11T17:00:25.071+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:00:25.072+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:00:23.032126+00:00 [scheduled]>[0m
[[34m2026-02-11T17:00:25.074+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T17:00:25.075+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:00:23.032126+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T17:00:25.077+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:00:23.032126+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:00:25.079+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:00:23.032126+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:00:27.447+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:00:29.477+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:00:23.032126+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:00:30.434+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:00:23.032126+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:00:30.445+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T09:00:23.032126+00:00, map_index=-1, run_start_date=2026-02-11 09:00:29.566355+00:00, run_end_date=2026-02-11 09:00:29.782841+00:00, run_duration=0.216486, state=success, executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 09:00:25.073432+00:00, queued_by_job_id=17, pid=16717[0m
[[34m2026-02-11T17:00:33.805+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:00:23.032126+00:00 [scheduled]>[0m
[[34m2026-02-11T17:00:33.806+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:00:33.807+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:00:23.032126+00:00 [scheduled]>[0m
[[34m2026-02-11T17:00:33.809+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T17:00:33.810+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:00:23.032126+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T17:00:33.811+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:00:23.032126+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:00:33.813+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:00:23.032126+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:00:36.739+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:00:38.274+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:00:23.032126+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:00:41.512+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:00:23.032126+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:00:41.522+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T09:00:23.032126+00:00, map_index=-1, run_start_date=2026-02-11 09:00:38.360739+00:00, run_end_date=2026-02-11 09:00:40.857586+00:00, run_duration=2.496847, state=success, executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 09:00:33.808122+00:00, queued_by_job_id=17, pid=16721[0m
[[34m2026-02-11T17:00:44.888+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 09:00:23.032126+00:00: manual__2026-02-11T09:00:23.032126+00:00, state:running, queued_at: 2026-02-11 09:00:23.046249+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T17:00:44.890+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 09:00:23.032126+00:00, run_id=manual__2026-02-11T09:00:23.032126+00:00, run_start_date=2026-02-11 09:00:25.046869+00:00, run_end_date=2026-02-11 09:00:44.890070+00:00, run_duration=19.843201, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T17:01:56.956+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:01:55.682358+00:00 [scheduled]>[0m
[[34m2026-02-11T17:01:56.959+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:01:56.961+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:01:55.682358+00:00 [scheduled]>[0m
[[34m2026-02-11T17:01:56.971+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T17:01:56.972+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:01:55.682358+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T17:01:56.975+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:01:55.682358+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:01:56.991+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:01:55.682358+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:01:59.651+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:02:01.497+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:01:55.682358+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:02:02.471+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:01:55.682358+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:02:02.482+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T09:01:55.682358+00:00, map_index=-1, run_start_date=2026-02-11 09:02:01.578233+00:00, run_end_date=2026-02-11 09:02:01.763419+00:00, run_duration=0.185186, state=success, executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 09:01:56.967261+00:00, queued_by_job_id=17, pid=16817[0m
[[34m2026-02-11T17:02:06.509+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:01:55.682358+00:00 [scheduled]>[0m
[[34m2026-02-11T17:02:06.509+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:02:06.510+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:01:55.682358+00:00 [scheduled]>[0m
[[34m2026-02-11T17:02:06.513+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T17:02:06.514+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:01:55.682358+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T17:02:06.514+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:01:55.682358+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:02:06.517+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:01:55.682358+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:02:09.015+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:02:10.543+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:01:55.682358+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:02:13.807+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:01:55.682358+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:02:13.816+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T09:01:55.682358+00:00, map_index=-1, run_start_date=2026-02-11 09:02:10.627316+00:00, run_end_date=2026-02-11 09:02:13.120316+00:00, run_duration=2.493, state=success, executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 09:02:06.511791+00:00, queued_by_job_id=17, pid=16821[0m
[[34m2026-02-11T17:02:17.260+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 09:01:55.682358+00:00: manual__2026-02-11T09:01:55.682358+00:00, state:running, queued_at: 2026-02-11 09:01:55.700669+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T17:02:17.261+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 09:01:55.682358+00:00, run_id=manual__2026-02-11T09:01:55.682358+00:00, run_start_date=2026-02-11 09:01:56.931656+00:00, run_end_date=2026-02-11 09:02:17.261699+00:00, run_duration=20.330043, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T17:04:41.776+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:06:37.120+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:06:35.236247+00:00 [scheduled]>[0m
[[34m2026-02-11T17:06:37.121+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:06:37.121+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:06:35.236247+00:00 [scheduled]>[0m
[[34m2026-02-11T17:06:37.124+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved[0m
[[34m2026-02-11T17:06:37.125+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:06:35.236247+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2026-02-11T17:06:37.125+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:06:35.236247+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:06:37.128+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-11T09:06:35.236247+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:06:39.979+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:06:41.854+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.sync_order_data manual__2026-02-11T09:06:35.236247+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:06:42.727+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-11T09:06:35.236247+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:06:42.737+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-11T09:06:35.236247+00:00, map_index=-1, run_start_date=2026-02-11 09:06:41.938664+00:00, run_end_date=2026-02-11 09:06:42.134075+00:00, run_duration=0.195411, state=success, executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2026-02-11 09:06:37.122916+00:00, queued_by_job_id=17, pid=17005[0m
[[34m2026-02-11T17:06:45.796+0800[0m] {[34mscheduler_job_runner.py:[0m423} INFO[0m - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:06:35.236247+00:00 [scheduled]>[0m
[[34m2026-02-11T17:06:45.797+0800[0m] {[34mscheduler_job_runner.py:[0m486} INFO[0m - DAG dw_order_sync has 0/16 running and queued tasks[0m
[[34m2026-02-11T17:06:45.798+0800[0m] {[34mscheduler_job_runner.py:[0m602} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:06:35.236247+00:00 [scheduled]>[0m
[[34m2026-02-11T17:06:45.800+0800[0m] {[34mtaskinstance.py:[0m2286} WARNING[0m - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved[0m
[[34m2026-02-11T17:06:45.801+0800[0m] {[34mscheduler_job_runner.py:[0m645} INFO[0m - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:06:35.236247+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2026-02-11T17:06:45.802+0800[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:06:35.236247+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:06:45.804+0800[0m] {[34msequential_executor.py:[0m73} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-11T09:06:35.236247+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py'][0m
[[34m2026-02-11T17:06:48.254+0800[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow/dags/tag_data_warehouse/dw_order_sync.py[0m
[[34m2026-02-11T17:06:49.560+0800[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: dw_order_sync.run_automation_test manual__2026-02-11T09:06:35.236247+00:00 [queued]> on host localhost-2.local[0m
[[34m2026-02-11T17:14:13.896+0800[0m] {[34mscheduler_job_runner.py:[0m695} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-11T09:06:35.236247+00:00', try_number=1, map_index=-1)[0m
[[34m2026-02-11T17:14:13.909+0800[0m] {[34mscheduler_job_runner.py:[0m732} INFO[0m - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-11T09:06:35.236247+00:00, map_index=-1, run_start_date=2026-02-11 09:06:49.644482+00:00, run_end_date=2026-02-11 09:14:13.133820+00:00, run_duration=443.489338, state=success, executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-11 09:06:45.799522+00:00, queued_by_job_id=17, pid=17009[0m
[[34m2026-02-11T17:14:13.921+0800[0m] {[34mmanager.py:[0m284} ERROR[0m - DagFileProcessorManager (PID=14149) last sent a heartbeat 448.17 seconds ago! Restarting it[0m
[[34m2026-02-11T17:14:13.933+0800[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 14149. PIDs of all processes in the group: [14149][0m
[[34m2026-02-11T17:14:13.934+0800[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 14149[0m
[[34m2026-02-11T17:14:14.794+0800[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=14149, status='terminated', exitcode=0, started='16:14:23') (14149) terminated with exit code 0[0m
[[34m2026-02-11T17:14:14.804+0800[0m] {[34mmanager.py:[0m169} INFO[0m - Launched DagFileProcessorManager with pid: 17672[0m
[[34m2026-02-11T17:14:14.842+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:14:17.756+0800[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2026-02-11T17:14:17.806+0800] {manager.py:392} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2026-02-11T17:14:21.169+0800[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun dw_order_sync @ 2026-02-11 09:06:35.236247+00:00: manual__2026-02-11T09:06:35.236247+00:00, state:running, queued_at: 2026-02-11 09:06:35.263951+00:00. externally triggered: True> successful[0m
[[34m2026-02-11T17:14:21.170+0800[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 09:06:35.236247+00:00, run_id=manual__2026-02-11T09:06:35.236247+00:00, run_start_date=2026-02-11 09:06:37.097530+00:00, run_end_date=2026-02-11 09:14:21.170266+00:00, run_duration=464.072736, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-10 00:00:00+00:00, data_interval_end=2026-02-11 00:00:00+00:00, dag_hash=18b6485188fd364812e8cde73c6c3d20[0m
[[34m2026-02-11T17:19:17.521+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:24:18.412+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:29:18.998+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:34:22.144+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:39:25.872+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:44:26.544+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:49:27.565+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:54:30.379+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2026-02-11T17:59:33.575+0800[0m] {[34mscheduler_job_runner.py:[0m1607} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
