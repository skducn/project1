# Variables

位置：Airflow 中 Admin → Variables
核心结论：Variables 是 Airflow 提供的键值对配置管理功能，用于存储动态配置（如数据库地址、文件路径、环境参数等），避免硬编码在 DAG 脚本中。

作用：替代 DAG 中的硬编码配置，支持动态修改（无需改代码、重启服务）；
存储：默认存在 Airflow 元数据库（如 MySQL/PostgreSQL）中，也可配置用 Redis 存储；
特点：支持字符串、JSON 格式，可全局共享，权限可控（Admin 角色可管理）

# todo 在 UI 中创建 Variable（Admin → Variables）
1. 基础操作（单键值对）
进入 Airflow Web UI → 点击顶部「Admin」→ 选择「Variables」；
点击「+ Add a new record」：
Key：配置名称（如 mysql_conn_info）；
Val：配置值（支持纯文本 / JSON，如 {"host":"127.0.0.1","port":3306,"user":"airflow"}）；
Description（可选）：备注（如「订单同步数据库连接信息」）；
点击「Save」完成创建。

# todo 批量导入（适合多配置）
如果有大量配置，可通过 JSON 文件批量导入：
新建 variables.json 文件：
{
  "etl_file_path": "/data/etl/order",
  "max_retry_times": "3",
  "email_notify": "admin@example.com"
}
在 Variables 页面点击「Import Variables」→ 上传该文件 → 确认导入。


# todo 在 DAG 中调用 Variable（核心实操）
;todo 场景 1：基础调用（单个变量）
适用于简单的键值对配置
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable  # 导入 Variable 模块
from datetime import datetime

# 定义读取变量的函数
def read_basic_variables():
    # 方式 1：基础读取（推荐，指定默认值避免变量不存在报错）
    file_path = Variable.get("etl_file_path", default_var="/data/default")
    max_retry = Variable.get("max_retry_times", default_var=1, deserialize_json=False)
    email = Variable.get("email_notify", default_var="default@example.com")

    # 打印变量（实际场景可用于业务逻辑）
    print(f"ETL 文件路径：{file_path}")
    print(f"最大重试次数：{max_retry}")
    print(f"通知邮箱：{email}")

# 定义 DAG
with DAG(
    dag_id="test_variable_basic",
    start_date=datetime(2026, 2, 12),
    schedule_interval="@once",
    catchup=False
) as dag:
    task1 = PythonOperator(
        task_id="read_variables",
        python_callable=read_basic_variables
    )

task1


;todo 场景 2：JSON 格式变量（复杂配置）
适用于存储结构化配置（如数据库连接信息、多参数组合），无需拆分多个变量。
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime
import mysql.connector

# 定义读取 JSON 变量并连接数据库的函数
def connect_mysql_with_variable():
    # 方式 2：读取 JSON 格式变量（deserialize_json=True）
    mysql_config = Variable.get(
        key="mysql_conn_info",
        default_var={"host":"localhost","port":3306,"user":"root","password":"123456"},
        deserialize_json=True  # 关键：自动解析 JSON 为字典
    )

    # 使用变量连接数据库（实际场景可执行查询/写入）
    try:
        conn = mysql.connector.connect(
            host=mysql_config["host"],
            port=mysql_config["port"],
            user=mysql_config["user"],
            password=mysql_config["password"],
            database=mysql_config.get("db", "airflow")  # 可选参数加容错
        )
        print(f"成功连接 MySQL：{mysql_config['host']}:{mysql_config['port']}")
        conn.close()
    except Exception as e:
        print(f"连接失败：{e}")

# 定义 DAG
with DAG(
    dag_id="test_variable_json",
    start_date=datetime(2026, 2, 12),
    schedule_interval="@once",
    catchup=False
) as dag:
    task2 = PythonOperator(
        task_id="connect_mysql",
        python_callable=connect_mysql_with_variable
    )

task2

# todo 场景 3：在模板中使用（BashOperator/Jinja2）
适用于 Bash 任务中直接引用变量，无需通过 Python 函数中转。
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

# 定义 DAG
with DAG(
    dag_id="test_variable_template",
    start_date=datetime(2026, 2, 12),
    schedule_interval="@once",
    catchup=False
) as dag:
    # 方式 3：Jinja2 模板引用 Variable（格式：{{ var.value.变量名 }}）
    task3 = BashOperator(
        task_id="bash_use_variable",
        # 引用 etl_file_path 变量，拼接成脚本执行命令
        bash_command="""
            echo "ETL 文件路径：{{ var.value.etl_file_path }}"
            # 实际场景：执行脚本并传入变量
            # python /scripts/order_sync.py --path {{ var.value.etl_file_path }} --retry {{ var.value.max_retry_times }}
        """
    )

task3

;todo 场景 4：更新 / 删除变量（代码层面）
适用于动态修改配置（如根据环境切换配置）。
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable
from datetime import datetime

def update_variable():
    # 更新变量（不存在则创建）
    Variable.set("max_retry_times", 5)  # 将重试次数改为 5
    print("变量已更新为 5")

    # 删除变量（谨慎使用）
    # Variable.delete("email_notify")

with DAG(
    dag_id="test_variable_update",
    start_date=datetime(2026, 2, 12),
    schedule_interval="@once",
    catchup=False
) as dag:
    task4 = PythonOperator(
        task_id="update_var",
        python_callable=update_variable
    )

task4

;todo 变量分组（避免键名冲突）
如果变量较多，可通过「命名空间」分组（Airflow 2.0+ 支持)
创建分组变量：
    Key 填写 etl_config.order.file_path（用 . 分隔分组）；
    Val 填写 /data/etl/order。
读取分组变量：
    # 读取分组变量
    order_file_path = Variable.get("etl_config.order.file_path")
    # 批量读取分组下的所有变量（返回字典）
    etl_config = Variable.get("etl_config", deserialize_json=True)
    print(etl_config["order"]["file_path"])  # 输出 /data/etl/order

;todo 关键注意事项
1,默认值必须加：Variable.get() 一定要加 default_var，否则变量不存在时会抛错，导致 DAG 加载失败；
2,敏感信息不建议用：Variables 存储的是明文（UI 中可直接查看），敏感信息（如密码、密钥）建议用「Connections」或「Secrets」（如 AWS Secrets Manager、HashiCorp Vault）；
3,性能优化：频繁读取变量会访问数据库，可缓存变量：
    # 缓存变量（避免多次查库）
    Variable.get("etl_file_path", default_var="/data/default", cache_timeout=3600)  # 缓存 1 小时
4,DAG 加载时读取：Variable 会在 DAG 解析时读取，若变量修改后需刷新 DAG，可点击 UI 中的「Refresh DAGs」。