[2026-02-15T15:57:53.910+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-15T15:57:54.960+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-15T15:57:54.962+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-15T15:57:54.973+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 1011
[2026-02-15T15:57:54.983+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T15:57:59.391+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-15T15:57:59.494+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-15T15:58:00.127+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-15T16:02:56.734+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:11:25.198+0800] {job.py:229} INFO - Heartbeat recovered after 390.08 seconds
[2026-02-15T16:14:20.523+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:19:23.164+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:20:34.310+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>
[2026-02-15T16:20:34.310+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:20:34.311+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>
[2026-02-15T16:20:34.314+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:20:34.315+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:20:31.221949+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:20:34.316+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:20:31.221949+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:20:34.318+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:20:31.221949+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:20:36.210+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:20:36.887+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:20:37.990+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:20:31.221949+00:00', try_number=1, map_index=-1)
[2026-02-15T16:20:38.005+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:20:31.221949+00:00, map_index=-1, run_start_date=2026-02-15 08:20:36.957461+00:00, run_end_date=2026-02-15 08:20:37.318201+00:00, run_duration=0.36074, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=126, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:20:34.313061+00:00, queued_by_job_id=125, pid=2348
[2026-02-15T16:20:40.656+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:20:31.221949+00:00: manual__2026-02-15T08:20:31.221949+00:00, state:running, queued_at: 2026-02-15 08:20:31.249112+00:00. externally triggered: True> successful
[2026-02-15T16:20:40.658+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:20:31.221949+00:00, run_id=manual__2026-02-15T08:20:31.221949+00:00, run_start_date=2026-02-15 08:20:34.270820+00:00, run_end_date=2026-02-15 08:20:40.658445+00:00, run_duration=6.387625, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:20:31.221949+00:00, data_interval_end=2026-02-15 08:20:31.221949+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:20:40.667+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>
[2026-02-15T16:20:40.668+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:20:40.669+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>
[2026-02-15T16:20:40.671+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:20:40.672+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:20:37.334108+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:20:40.673+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:20:37.334108+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:20:40.675+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:20:37.334108+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:20:42.562+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:20:43.127+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:20:44.081+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:20:37.334108+00:00', try_number=1, map_index=-1)
[2026-02-15T16:20:44.091+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:20:37.334108+00:00, map_index=-1, run_start_date=2026-02-15 08:20:43.189450+00:00, run_end_date=2026-02-15 08:20:43.529989+00:00, run_duration=0.340539, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=127, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:20:40.670651+00:00, queued_by_job_id=125, pid=2351
[2026-02-15T16:20:46.710+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:20:37.334108+00:00: dataset_triggered__2026-02-15T08:20:37.334108+00:00, state:running, queued_at: 2026-02-15 08:20:40.623731+00:00. externally triggered: False> failed
[2026-02-15T16:20:46.711+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:20:37.334108+00:00, run_id=dataset_triggered__2026-02-15T08:20:37.334108+00:00, run_start_date=2026-02-15 08:20:40.644735+00:00, run_end_date=2026-02-15 08:20:46.711057+00:00, run_duration=6.066322, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:20:31.221949+00:00, data_interval_end=2026-02-15 08:20:31.221949+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:23:06.551+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>
[2026-02-15T16:23:06.552+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:23:06.553+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>
[2026-02-15T16:23:06.555+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:23:06.556+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:23:03.595532+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:23:06.557+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:23:03.595532+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:23:06.559+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:23:03.595532+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:23:08.479+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:23:09.207+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:23:10.251+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:23:03.595532+00:00', try_number=1, map_index=-1)
[2026-02-15T16:23:10.263+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:23:03.595532+00:00, map_index=-1, run_start_date=2026-02-15 08:23:09.275814+00:00, run_end_date=2026-02-15 08:23:09.660724+00:00, run_duration=0.38491, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=128, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:23:06.554078+00:00, queued_by_job_id=125, pid=2490
[2026-02-15T16:23:12.887+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:23:03.595532+00:00: manual__2026-02-15T08:23:03.595532+00:00, state:running, queued_at: 2026-02-15 08:23:03.615355+00:00. externally triggered: True> successful
[2026-02-15T16:23:12.888+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:23:03.595532+00:00, run_id=manual__2026-02-15T08:23:03.595532+00:00, run_start_date=2026-02-15 08:23:06.531922+00:00, run_end_date=2026-02-15 08:23:12.888437+00:00, run_duration=6.356515, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:23:03.595532+00:00, data_interval_end=2026-02-15 08:23:03.595532+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:23:12.897+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>
[2026-02-15T16:23:12.898+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:23:12.899+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>
[2026-02-15T16:23:12.901+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:23:12.902+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:23:09.677763+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:23:12.903+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:23:09.677763+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:23:12.905+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:23:09.677763+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:23:14.799+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:23:15.387+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:23:16.562+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:23:09.677763+00:00', try_number=1, map_index=-1)
[2026-02-15T16:23:16.572+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:23:09.677763+00:00, map_index=-1, run_start_date=2026-02-15 08:23:15.449617+00:00, run_end_date=2026-02-15 08:23:15.887352+00:00, run_duration=0.437735, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=129, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:23:12.900177+00:00, queued_by_job_id=125, pid=2493
[2026-02-15T16:23:19.131+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:23:09.677763+00:00: dataset_triggered__2026-02-15T08:23:09.677763+00:00, state:running, queued_at: 2026-02-15 08:23:12.861334+00:00. externally triggered: False> successful
[2026-02-15T16:23:19.132+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:23:09.677763+00:00, run_id=dataset_triggered__2026-02-15T08:23:09.677763+00:00, run_start_date=2026-02-15 08:23:12.875117+00:00, run_end_date=2026-02-15 08:23:19.131958+00:00, run_duration=6.256841, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:23:03.595532+00:00, data_interval_end=2026-02-15 08:23:03.595532+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:24:24.206+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:29:25.763+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:34:26.931+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:39:28.907+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:43:39.969+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>
[2026-02-15T16:43:39.970+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:43:39.971+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>
[2026-02-15T16:43:39.973+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:43:39.974+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:43:37.783738+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:43:39.975+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:43:37.783738+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:43:39.977+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:43:37.783738+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:43:41.925+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:43:42.675+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:43:43.700+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:43:37.783738+00:00', try_number=1, map_index=-1)
[2026-02-15T16:43:43.709+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:43:37.783738+00:00, map_index=-1, run_start_date=2026-02-15 08:43:42.748972+00:00, run_end_date=2026-02-15 08:43:43.123360+00:00, run_duration=0.374388, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=130, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:43:39.972153+00:00, queued_by_job_id=125, pid=3390
[2026-02-15T16:43:46.296+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:43:37.783738+00:00: manual__2026-02-15T08:43:37.783738+00:00, state:running, queued_at: 2026-02-15 08:43:37.802419+00:00. externally triggered: True> successful
[2026-02-15T16:43:46.297+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:43:37.783738+00:00, run_id=manual__2026-02-15T08:43:37.783738+00:00, run_start_date=2026-02-15 08:43:39.948365+00:00, run_end_date=2026-02-15 08:43:46.297152+00:00, run_duration=6.348787, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:43:37.783738+00:00, data_interval_end=2026-02-15 08:43:37.783738+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:43:46.307+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>
[2026-02-15T16:43:46.308+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:43:46.308+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>
[2026-02-15T16:43:46.311+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:43:46.312+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:43:43.140152+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:43:46.312+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:43:43.140152+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:43:46.315+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:43:43.140152+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:43:48.233+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:43:48.808+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:43:49.814+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:43:43.140152+00:00', try_number=1, map_index=-1)
[2026-02-15T16:43:49.822+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:43:43.140152+00:00, map_index=-1, run_start_date=2026-02-15 08:43:48.872686+00:00, run_end_date=2026-02-15 08:43:49.311331+00:00, run_duration=0.438645, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:43:46.309880+00:00, queued_by_job_id=125, pid=3393
[2026-02-15T16:43:52.492+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:43:43.140152+00:00: dataset_triggered__2026-02-15T08:43:43.140152+00:00, state:running, queued_at: 2026-02-15 08:43:46.269290+00:00. externally triggered: False> successful
[2026-02-15T16:43:52.493+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:43:43.140152+00:00, run_id=dataset_triggered__2026-02-15T08:43:43.140152+00:00, run_start_date=2026-02-15 08:43:46.282966+00:00, run_end_date=2026-02-15 08:43:52.493241+00:00, run_duration=6.210275, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:43:37.783738+00:00, data_interval_end=2026-02-15 08:43:37.783738+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:44:31.221+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:47:10.700+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>
[2026-02-15T16:47:10.702+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:47:10.703+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>
[2026-02-15T16:47:10.705+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:47:10.706+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:47:09.878272+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:47:10.707+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:47:09.878272+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:47:10.709+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:47:09.878272+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:47:12.801+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:47:13.815+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:47:14.866+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:47:09.878272+00:00', try_number=1, map_index=-1)
[2026-02-15T16:47:14.875+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:47:09.878272+00:00, map_index=-1, run_start_date=2026-02-15 08:47:13.887974+00:00, run_end_date=2026-02-15 08:47:14.261265+00:00, run_duration=0.373291, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=132, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:47:10.704327+00:00, queued_by_job_id=125, pid=3535
[2026-02-15T16:47:17.361+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:47:09.878272+00:00: manual__2026-02-15T08:47:09.878272+00:00, state:running, queued_at: 2026-02-15 08:47:09.890049+00:00. externally triggered: True> successful
[2026-02-15T16:47:17.363+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:47:09.878272+00:00, run_id=manual__2026-02-15T08:47:09.878272+00:00, run_start_date=2026-02-15 08:47:10.671755+00:00, run_end_date=2026-02-15 08:47:17.363024+00:00, run_duration=6.691269, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:47:09.878272+00:00, data_interval_end=2026-02-15 08:47:09.878272+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:47:17.374+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>
[2026-02-15T16:47:17.375+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:47:17.375+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>
[2026-02-15T16:47:17.378+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:47:17.379+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:47:14.278092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:47:17.380+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:47:14.278092+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:47:17.383+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:47:14.278092+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:47:19.437+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:47:20.056+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:47:21.203+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:47:14.278092+00:00', try_number=1, map_index=-1)
[2026-02-15T16:47:21.212+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:47:14.278092+00:00, map_index=-1, run_start_date=2026-02-15 08:47:20.124929+00:00, run_end_date=2026-02-15 08:47:20.608615+00:00, run_duration=0.483686, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=133, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:47:17.377017+00:00, queued_by_job_id=125, pid=3538
[2026-02-15T16:47:23.839+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:47:14.278092+00:00: dataset_triggered__2026-02-15T08:47:14.278092+00:00, state:running, queued_at: 2026-02-15 08:47:17.332903+00:00. externally triggered: False> successful
[2026-02-15T16:47:23.840+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:47:14.278092+00:00, run_id=dataset_triggered__2026-02-15T08:47:14.278092+00:00, run_start_date=2026-02-15 08:47:17.348345+00:00, run_end_date=2026-02-15 08:47:23.840515+00:00, run_duration=6.49217, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:47:09.878272+00:00, data_interval_end=2026-02-15 08:47:09.878272+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:49:33.736+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:52:01.430+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>
[2026-02-15T16:52:01.431+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:52:01.432+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>
[2026-02-15T16:52:01.434+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:52:01.436+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:51:59.598541+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:52:01.436+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:51:59.598541+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:52:01.439+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:51:59.598541+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:52:03.413+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:52:04.314+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:52:05.410+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:51:59.598541+00:00', try_number=1, map_index=-1)
[2026-02-15T16:52:05.419+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:51:59.598541+00:00, map_index=-1, run_start_date=2026-02-15 08:52:04.391133+00:00, run_end_date=2026-02-15 08:52:04.822273+00:00, run_duration=0.43114, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=134, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:52:01.432882+00:00, queued_by_job_id=125, pid=3749
[2026-02-15T16:52:08.020+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:51:59.598541+00:00: manual__2026-02-15T08:51:59.598541+00:00, state:running, queued_at: 2026-02-15 08:51:59.609747+00:00. externally triggered: True> successful
[2026-02-15T16:52:08.021+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:51:59.598541+00:00, run_id=manual__2026-02-15T08:51:59.598541+00:00, run_start_date=2026-02-15 08:52:01.410145+00:00, run_end_date=2026-02-15 08:52:08.021039+00:00, run_duration=6.610894, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:51:59.598541+00:00, data_interval_end=2026-02-15 08:51:59.598541+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:52:08.030+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>
[2026-02-15T16:52:08.031+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:52:08.031+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>
[2026-02-15T16:52:08.033+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:52:08.035+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:52:04.840837+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:52:08.035+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:52:04.840837+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:52:08.038+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:52:04.840837+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:52:09.999+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:52:10.592+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:52:11.726+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:52:04.840837+00:00', try_number=1, map_index=-1)
[2026-02-15T16:52:11.735+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:52:04.840837+00:00, map_index=-1, run_start_date=2026-02-15 08:52:10.658116+00:00, run_end_date=2026-02-15 08:52:11.145601+00:00, run_duration=0.487485, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=135, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:52:08.032712+00:00, queued_by_job_id=125, pid=3752
[2026-02-15T16:52:14.362+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:52:04.840837+00:00: dataset_triggered__2026-02-15T08:52:04.840837+00:00, state:running, queued_at: 2026-02-15 08:52:07.989842+00:00. externally triggered: False> successful
[2026-02-15T16:52:14.363+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:52:04.840837+00:00, run_id=dataset_triggered__2026-02-15T08:52:04.840837+00:00, run_start_date=2026-02-15 08:52:08.006323+00:00, run_end_date=2026-02-15 08:52:14.363724+00:00, run_duration=6.357401, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:51:59.598541+00:00, data_interval_end=2026-02-15 08:51:59.598541+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:53:05.118+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>
[2026-02-15T16:53:05.119+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:53:05.120+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>
[2026-02-15T16:53:05.122+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:53:05.124+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:53:01.949106+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:53:05.124+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:53:01.949106+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:53:05.127+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:53:01.949106+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:53:07.122+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:53:07.729+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:53:08.819+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:53:01.949106+00:00', try_number=1, map_index=-1)
[2026-02-15T16:53:08.829+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:53:01.949106+00:00, map_index=-1, run_start_date=2026-02-15 08:53:07.797065+00:00, run_end_date=2026-02-15 08:53:08.206592+00:00, run_duration=0.409527, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=136, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:53:05.121622+00:00, queued_by_job_id=125, pid=3781
[2026-02-15T16:53:11.710+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:53:01.949106+00:00: manual__2026-02-15T08:53:01.949106+00:00, state:running, queued_at: 2026-02-15 08:53:01.968186+00:00. externally triggered: True> successful
[2026-02-15T16:53:11.711+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:53:01.949106+00:00, run_id=manual__2026-02-15T08:53:01.949106+00:00, run_start_date=2026-02-15 08:53:05.094052+00:00, run_end_date=2026-02-15 08:53:11.711074+00:00, run_duration=6.617022, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:53:01.949106+00:00, data_interval_end=2026-02-15 08:53:01.949106+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
[2026-02-15T16:53:11.720+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>
[2026-02-15T16:53:11.721+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
[2026-02-15T16:53:11.722+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>
[2026-02-15T16:53:11.724+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T16:53:11.725+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:53:08.224546+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T16:53:11.726+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:53:08.224546+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:53:11.728+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:53:08.224546+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
[2026-02-15T16:53:13.693+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db2.py
[2026-02-15T16:53:14.308+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [queued]> on host localhost-2.local
[2026-02-15T16:53:15.456+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:53:08.224546+00:00', try_number=1, map_index=-1)
[2026-02-15T16:53:15.465+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:53:08.224546+00:00, map_index=-1, run_start_date=2026-02-15 08:53:14.379673+00:00, run_end_date=2026-02-15 08:53:14.885960+00:00, run_duration=0.506287, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=137, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:53:11.723081+00:00, queued_by_job_id=125, pid=3785
[2026-02-15T16:53:18.304+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:53:08.224546+00:00: dataset_triggered__2026-02-15T08:53:08.224546+00:00, state:running, queued_at: 2026-02-15 08:53:11.684187+00:00. externally triggered: False> successful
[2026-02-15T16:53:18.305+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:53:08.224546+00:00, run_id=dataset_triggered__2026-02-15T08:53:08.224546+00:00, run_start_date=2026-02-15 08:53:11.697224+00:00, run_end_date=2026-02-15 08:53:18.305273+00:00, run_duration=6.608049, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:53:01.949106+00:00, data_interval_end=2026-02-15 08:53:01.949106+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
[2026-02-15T16:54:36.744+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T16:59:39.240+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T17:04:40.588+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T17:08:08.599+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>
[2026-02-15T17:08:08.600+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:08:08.601+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>
[2026-02-15T17:08:08.604+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:08:08.605+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:08:07.117190+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:08:08.606+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:08:07.117190+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:08:08.608+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:08:07.117190+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:08:10.612+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:08:12.415+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:08:13.562+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:08:07.117190+00:00', try_number=1, map_index=-1)
[2026-02-15T17:08:13.571+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:08:07.117190+00:00, map_index=-1, run_start_date=2026-02-15 09:08:12.481857+00:00, run_end_date=2026-02-15 09:08:12.852376+00:00, run_duration=0.370519, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=138, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:08:08.602841+00:00, queued_by_job_id=125, pid=4423
[2026-02-15T17:08:16.219+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:08:07.117190+00:00: manual__2026-02-15T09:08:07.117190+00:00, state:running, queued_at: 2026-02-15 09:08:07.129819+00:00. externally triggered: True> successful
[2026-02-15T17:08:16.220+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:08:07.117190+00:00, run_id=manual__2026-02-15T09:08:07.117190+00:00, run_start_date=2026-02-15 09:08:08.576755+00:00, run_end_date=2026-02-15 09:08:16.220235+00:00, run_duration=7.64348, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:08:07.117190+00:00, data_interval_end=2026-02-15 09:08:07.117190+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
[2026-02-15T17:08:16.229+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>
[2026-02-15T17:08:16.230+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:08:16.231+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>
[2026-02-15T17:08:16.233+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:08:16.234+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:08:12.868772+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:08:16.235+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:08:12.868772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:08:16.238+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:08:12.868772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:08:18.194+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:08:18.784+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:08:19.838+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:08:12.868772+00:00', try_number=1, map_index=-1)
[2026-02-15T17:08:19.846+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_sqlserver, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T09:08:12.868772+00:00, map_index=-1, run_start_date=2026-02-15 09:08:18.847774+00:00, run_end_date=2026-02-15 09:08:19.210150+00:00, run_duration=0.362376, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=139, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:08:16.232545+00:00, queued_by_job_id=125, pid=4426
[2026-02-15T17:08:22.549+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_order_sqlserver @ 2026-02-15 09:08:12.868772+00:00: dataset_triggered__2026-02-15T09:08:12.868772+00:00, state:running, queued_at: 2026-02-15 09:08:16.192012+00:00. externally triggered: False> failed
[2026-02-15T17:08:22.550+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_sqlserver, execution_date=2026-02-15 09:08:12.868772+00:00, run_id=dataset_triggered__2026-02-15T09:08:12.868772+00:00, run_start_date=2026-02-15 09:08:16.206206+00:00, run_end_date=2026-02-15 09:08:22.550239+00:00, run_duration=6.344033, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 09:08:07.117190+00:00, data_interval_end=2026-02-15 09:08:07.117190+00:00, dag_hash=4bfcc7af7a5b312e79e6f8e335db1b32
[2026-02-15T17:09:43.425+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T17:11:08.964+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>
[2026-02-15T17:11:08.965+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:11:08.966+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>
[2026-02-15T17:11:08.968+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:11:08.970+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:11:05.621117+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:11:08.970+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:11:05.621117+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:11:08.973+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:11:05.621117+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:11:10.968+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:11:11.938+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:11:13.029+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:11:05.621117+00:00', try_number=1, map_index=-1)
[2026-02-15T17:11:13.038+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:11:05.621117+00:00, map_index=-1, run_start_date=2026-02-15 09:11:12.018323+00:00, run_end_date=2026-02-15 09:11:12.396732+00:00, run_duration=0.378409, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=140, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:11:08.967435+00:00, queued_by_job_id=125, pid=4535
[2026-02-15T17:11:15.669+0800] {dagrun.py:823} ERROR - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:11:05.621117+00:00: manual__2026-02-15T09:11:05.621117+00:00, state:running, queued_at: 2026-02-15 09:11:05.633027+00:00. externally triggered: True> failed
[2026-02-15T17:11:15.670+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:11:05.621117+00:00, run_id=manual__2026-02-15T09:11:05.621117+00:00, run_start_date=2026-02-15 09:11:08.942850+00:00, run_end_date=2026-02-15 09:11:15.670612+00:00, run_duration=6.727762, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:11:05.621117+00:00, data_interval_end=2026-02-15 09:11:05.621117+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
[2026-02-15T17:14:45.184+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-15T17:15:46.526+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>
[2026-02-15T17:15:46.527+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:15:46.528+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>
[2026-02-15T17:15:46.530+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:15:46.531+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:15:43.478244+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:15:46.532+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:15:43.478244+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:15:46.534+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:15:43.478244+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:15:48.556+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:15:49.537+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:15:50.677+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:15:43.478244+00:00', try_number=1, map_index=-1)
[2026-02-15T17:15:50.686+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:15:43.478244+00:00, map_index=-1, run_start_date=2026-02-15 09:15:49.614062+00:00, run_end_date=2026-02-15 09:15:49.993795+00:00, run_duration=0.379733, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:15:46.529585+00:00, queued_by_job_id=125, pid=4704
[2026-02-15T17:15:53.248+0800] {dagrun.py:823} ERROR - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:15:43.478244+00:00: manual__2026-02-15T09:15:43.478244+00:00, state:running, queued_at: 2026-02-15 09:15:43.489512+00:00. externally triggered: True> failed
[2026-02-15T17:15:53.250+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:15:43.478244+00:00, run_id=manual__2026-02-15T09:15:43.478244+00:00, run_start_date=2026-02-15 09:15:46.507541+00:00, run_end_date=2026-02-15 09:15:53.249903+00:00, run_duration=6.742362, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:15:43.478244+00:00, data_interval_end=2026-02-15 09:15:43.478244+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
[2026-02-15T17:18:04.013+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>
[2026-02-15T17:18:04.027+0800] {scheduler_job_runner.py:507} INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:18:04.028+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>
[2026-02-15T17:18:04.034+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:18:04.035+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:18:02.962311+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:18:04.036+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:18:02.962311+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:18:04.052+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:18:02.962311+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:18:06.077+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:18:07.113+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:18:08.384+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:18:02.962311+00:00', try_number=1, map_index=-1)
[2026-02-15T17:18:08.392+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:18:02.962311+00:00, map_index=-1, run_start_date=2026-02-15 09:18:07.187874+00:00, run_end_date=2026-02-15 09:18:07.829922+00:00, run_duration=0.642048, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=142, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:18:04.031523+00:00, queued_by_job_id=125, pid=4788
[2026-02-15T17:18:10.980+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:18:02.962311+00:00: manual__2026-02-15T09:18:02.962311+00:00, state:running, queued_at: 2026-02-15 09:18:02.970703+00:00. externally triggered: True> successful
[2026-02-15T17:18:10.981+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:18:02.962311+00:00, run_id=manual__2026-02-15T09:18:02.962311+00:00, run_start_date=2026-02-15 09:18:03.961549+00:00, run_end_date=2026-02-15 09:18:10.981245+00:00, run_duration=7.019696, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:18:02.962311+00:00, data_interval_end=2026-02-15 09:18:02.962311+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
[2026-02-15T17:18:10.991+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>
[2026-02-15T17:18:10.992+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_order_sqlserver has 0/16 running and queued tasks
[2026-02-15T17:18:10.992+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>
[2026-02-15T17:18:10.995+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-15T17:18:10.996+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:18:07.845834+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-15T17:18:10.997+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:18:07.845834+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:18:10.999+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:18:07.845834+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
[2026-02-15T17:18:12.988+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_db_sqlserver.py
[2026-02-15T17:18:13.580+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [queued]> on host localhost-2.local
[2026-02-15T17:18:14.934+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:18:07.845834+00:00', try_number=1, map_index=-1)
[2026-02-15T17:18:14.942+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_order_sqlserver, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T09:18:07.845834+00:00, map_index=-1, run_start_date=2026-02-15 09:18:13.644053+00:00, run_end_date=2026-02-15 09:18:14.445610+00:00, run_duration=0.801557, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:18:10.993879+00:00, queued_by_job_id=125, pid=4791
[2026-02-15T17:18:17.733+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_order_sqlserver @ 2026-02-15 09:18:07.845834+00:00: dataset_triggered__2026-02-15T09:18:07.845834+00:00, state:running, queued_at: 2026-02-15 09:18:10.951674+00:00. externally triggered: False> successful
[2026-02-15T17:18:17.734+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_order_sqlserver, execution_date=2026-02-15 09:18:07.845834+00:00, run_id=dataset_triggered__2026-02-15T09:18:07.845834+00:00, run_start_date=2026-02-15 09:18:10.966958+00:00, run_end_date=2026-02-15 09:18:17.734565+00:00, run_duration=6.767607, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 09:18:02.962311+00:00, data_interval_end=2026-02-15 09:18:02.962311+00:00, dag_hash=4bfcc7af7a5b312e79e6f8e335db1b32
[2026-02-15T17:19:46.387+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
