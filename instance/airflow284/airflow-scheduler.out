[2026-02-23T20:20:52.804+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-23T20:20:53.323+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-23T20:20:53.324+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-23T20:20:53.331+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 6919
[2026-02-23T20:20:53.336+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:20:55.393+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-23T20:20:55.445+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-23T20:20:55.736+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-23T20:21:13.578+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:21:11.058403+00:00 [scheduled]>
[2026-02-23T20:21:13.579+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p_1n_1n_DAG has 0/16 running and queued tasks
[2026-02-23T20:21:13.580+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:21:11.058403+00:00 [scheduled]>
[2026-02-23T20:21:13.583+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:21:11.058403+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:21:13.584+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p_1n_1n_DAG', task_id='producer_1p_1n_1n_TASK', run_id='manual__2026-02-23T12:21:11.058403+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:21:13.584+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p_1n_1n_DAG', 'producer_1p_1n_1n_TASK', 'manual__2026-02-23T12:21:11.058403+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p_1n_1n.py']
[2026-02-23T20:21:13.587+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p_1n_1n_DAG', 'producer_1p_1n_1n_TASK', 'manual__2026-02-23T12:21:11.058403+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p_1n_1n.py']
[2026-02-23T20:21:15.676+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p_1n_1n.py
[2026-02-23T20:21:16.275+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:21:11.058403+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:21:17.431+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p_1n_1n_DAG', task_id='producer_1p_1n_1n_TASK', run_id='manual__2026-02-23T12:21:11.058403+00:00', try_number=1, map_index=-1)
[2026-02-23T20:21:17.448+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p_1n_1n_DAG, task_id=producer_1p_1n_1n_TASK, run_id=manual__2026-02-23T12:21:11.058403+00:00, map_index=-1, run_start_date=2026-02-23 12:21:16.347044+00:00, run_end_date=2026-02-23 12:21:16.720956+00:00, run_duration=0.373912, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=165, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:21:13.581235+00:00, queued_by_job_id=164, pid=6947
[2026-02-23T20:21:20.105+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p_1n_1n_DAG @ 2026-02-23 12:21:11.058403+00:00: manual__2026-02-23T12:21:11.058403+00:00, state:running, queued_at: 2026-02-23 12:21:11.083633+00:00. externally triggered: True> successful
[2026-02-23T20:21:20.107+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p_1n_1n_DAG, execution_date=2026-02-23 12:21:11.058403+00:00, run_id=manual__2026-02-23T12:21:11.058403+00:00, run_start_date=2026-02-23 12:21:13.400915+00:00, run_end_date=2026-02-23 12:21:20.107015+00:00, run_duration=6.7061, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:21:11.058403+00:00, data_interval_end=2026-02-23 12:21:11.058403+00:00, dag_hash=0668088bbb31448c0150ccfb187b0b3b
[2026-02-23T20:25:55.926+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:27:19.257+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1_N_DAG.producer_1_N_save_TASK manual__2026-02-23T12:27:15.701229+00:00 [scheduled]>
[2026-02-23T20:27:19.258+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1_N_DAG has 0/16 running and queued tasks
[2026-02-23T20:27:19.259+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1_N_DAG.producer_1_N_save_TASK manual__2026-02-23T12:27:15.701229+00:00 [scheduled]>
[2026-02-23T20:27:19.261+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1_N_DAG.producer_1_N_save_TASK manual__2026-02-23T12:27:15.701229+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:27:19.262+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1_N_DAG', task_id='producer_1_N_save_TASK', run_id='manual__2026-02-23T12:27:15.701229+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:27:19.263+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1_N_DAG', 'producer_1_N_save_TASK', 'manual__2026-02-23T12:27:15.701229+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:27:19.265+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1_N_DAG', 'producer_1_N_save_TASK', 'manual__2026-02-23T12:27:15.701229+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:27:21.322+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:27:22.049+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1_N_DAG.producer_1_N_save_TASK manual__2026-02-23T12:27:15.701229+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:27:23.055+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1_N_DAG', task_id='producer_1_N_save_TASK', run_id='manual__2026-02-23T12:27:15.701229+00:00', try_number=1, map_index=-1)
[2026-02-23T20:27:23.067+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1_N_DAG, task_id=producer_1_N_save_TASK, run_id=manual__2026-02-23T12:27:15.701229+00:00, map_index=-1, run_start_date=2026-02-23 12:27:22.115623+00:00, run_end_date=2026-02-23 12:27:22.500813+00:00, run_duration=0.38519, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=166, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:27:19.260378+00:00, queued_by_job_id=164, pid=7141
[2026-02-23T20:27:25.866+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1_N_DAG @ 2026-02-23 12:27:15.701229+00:00: manual__2026-02-23T12:27:15.701229+00:00, state:running, queued_at: 2026-02-23 12:27:15.723523+00:00. externally triggered: True> successful
[2026-02-23T20:27:25.867+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1_N_DAG, execution_date=2026-02-23 12:27:15.701229+00:00, run_id=manual__2026-02-23T12:27:15.701229+00:00, run_start_date=2026-02-23 12:27:19.237909+00:00, run_end_date=2026-02-23 12:27:25.866971+00:00, run_duration=6.629062, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:27:15.701229+00:00, data_interval_end=2026-02-23 12:27:15.701229+00:00, dag_hash=b211071d00cf88176e2d79e172154d28
[2026-02-23T20:27:25.877+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_1_N_clean_order_DAG.consumer_1_N_read_clean_order_TASK dataset_triggered__2026-02-23T12:27:22.517861+00:00 [scheduled]>
[2026-02-23T20:27:25.878+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1_N_clean_order_DAG has 0/16 running and queued tasks
[2026-02-23T20:27:25.879+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1_N_clean_order_DAG.consumer_1_N_read_clean_order_TASK dataset_triggered__2026-02-23T12:27:22.517861+00:00 [scheduled]>
[2026-02-23T20:27:25.882+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1_N_clean_order_DAG.consumer_1_N_read_clean_order_TASK dataset_triggered__2026-02-23T12:27:22.517861+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:27:25.883+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1_N_clean_order_DAG', task_id='consumer_1_N_read_clean_order_TASK', run_id='dataset_triggered__2026-02-23T12:27:22.517861+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:27:25.884+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_DAG', 'consumer_1_N_read_clean_order_TASK', 'dataset_triggered__2026-02-23T12:27:22.517861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:27:25.899+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_DAG', 'consumer_1_N_read_clean_order_TASK', 'dataset_triggered__2026-02-23T12:27:22.517861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:27:27.885+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:27:28.520+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1_N_clean_order_DAG.consumer_1_N_read_clean_order_TASK dataset_triggered__2026-02-23T12:27:22.517861+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:27:29.536+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1_N_clean_order_DAG', task_id='consumer_1_N_read_clean_order_TASK', run_id='dataset_triggered__2026-02-23T12:27:22.517861+00:00', try_number=1, map_index=-1)
[2026-02-23T20:27:29.547+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1_N_clean_order_DAG, task_id=consumer_1_N_read_clean_order_TASK, run_id=dataset_triggered__2026-02-23T12:27:22.517861+00:00, map_index=-1, run_start_date=2026-02-23 12:27:28.585951+00:00, run_end_date=2026-02-23 12:27:28.972626+00:00, run_duration=0.386675, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=167, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:27:25.881333+00:00, queued_by_job_id=164, pid=7144
[2026-02-23T20:27:32.552+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1_N_clean_order_DAG @ 2026-02-23 12:27:22.517861+00:00: dataset_triggered__2026-02-23T12:27:22.517861+00:00, state:running, queued_at: 2026-02-23 12:27:25.832429+00:00. externally triggered: False> successful
[2026-02-23T20:27:32.554+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1_N_clean_order_DAG, execution_date=2026-02-23 12:27:22.517861+00:00, run_id=dataset_triggered__2026-02-23T12:27:22.517861+00:00, run_start_date=2026-02-23 12:27:25.852335+00:00, run_end_date=2026-02-23 12:27:32.553956+00:00, run_duration=6.701621, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:27:15.701229+00:00, data_interval_end=2026-02-23 12:27:15.701229+00:00, dag_hash=f5f40795c8ffc8aaddffb97149f9b5dc
[2026-02-23T20:30:34.270+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:30:32.913130+00:00 [scheduled]>
[2026-02-23T20:30:34.271+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p_1n_1n_DAG has 0/16 running and queued tasks
[2026-02-23T20:30:34.272+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:30:32.913130+00:00 [scheduled]>
[2026-02-23T20:30:34.274+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:30:32.913130+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:30:34.277+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p_1n_1n_DAG', task_id='producer_1p_1n_1n_TASK', run_id='manual__2026-02-23T12:30:32.913130+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:30:34.277+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p_1n_1n_DAG', 'producer_1p_1n_1n_TASK', 'manual__2026-02-23T12:30:32.913130+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p_1n_1n.py']
[2026-02-23T20:30:34.280+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p_1n_1n_DAG', 'producer_1p_1n_1n_TASK', 'manual__2026-02-23T12:30:32.913130+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p_1n_1n.py']
[2026-02-23T20:30:36.357+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p_1n_1n.py
[2026-02-23T20:30:37.412+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p_1n_1n_DAG.producer_1p_1n_1n_TASK manual__2026-02-23T12:30:32.913130+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:30:38.565+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p_1n_1n_DAG', task_id='producer_1p_1n_1n_TASK', run_id='manual__2026-02-23T12:30:32.913130+00:00', try_number=1, map_index=-1)
[2026-02-23T20:30:38.576+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p_1n_1n_DAG, task_id=producer_1p_1n_1n_TASK, run_id=manual__2026-02-23T12:30:32.913130+00:00, map_index=-1, run_start_date=2026-02-23 12:30:37.484847+00:00, run_end_date=2026-02-23 12:30:37.855549+00:00, run_duration=0.370702, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=168, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:30:34.273791+00:00, queued_by_job_id=164, pid=7226
[2026-02-23T20:30:41.240+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p_1n_1n_DAG @ 2026-02-23 12:30:32.913130+00:00: manual__2026-02-23T12:30:32.913130+00:00, state:running, queued_at: 2026-02-23 12:30:32.933578+00:00. externally triggered: True> successful
[2026-02-23T20:30:41.241+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p_1n_1n_DAG, execution_date=2026-02-23 12:30:32.913130+00:00, run_id=manual__2026-02-23T12:30:32.913130+00:00, run_start_date=2026-02-23 12:30:34.250160+00:00, run_end_date=2026-02-23 12:30:41.241086+00:00, run_duration=6.990926, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:30:32.913130+00:00, data_interval_end=2026-02-23 12:30:32.913130+00:00, dag_hash=0668088bbb31448c0150ccfb187b0b3b
[2026-02-23T20:30:58.830+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:36:01.634+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:36:24.151+0800] {manager.py:537} INFO - DAG consumer_1_N_clean_order_DAG is missing and will be deactivated.
[2026-02-23T20:36:24.152+0800] {manager.py:537} INFO - DAG producer_1_N_DAG is missing and will be deactivated.
[2026-02-23T20:36:24.156+0800] {manager.py:549} INFO - Deactivated 2 DAGs which are no longer present in file.
[2026-02-23T20:36:24.158+0800] {manager.py:553} INFO - Deleted DAG producer_1_N_DAG in serialized_dag table
[2026-02-23T20:36:24.160+0800] {manager.py:553} INFO - Deleted DAG consumer_1_N_clean_order_DAG in serialized_dag table
[2026-02-23T20:36:45.764+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG.producer_1p1n1n_TASK manual__2026-02-23T12:36:42.067898+00:00 [scheduled]>
[2026-02-23T20:36:45.765+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG has 0/16 running and queued tasks
[2026-02-23T20:36:45.766+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG.producer_1p1n1n_TASK manual__2026-02-23T12:36:42.067898+00:00 [scheduled]>
[2026-02-23T20:36:45.768+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG.producer_1p1n1n_TASK manual__2026-02-23T12:36:42.067898+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:36:45.769+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG', task_id='producer_1p1n1n_TASK', run_id='manual__2026-02-23T12:36:42.067898+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:36:45.770+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG', 'producer_1p1n1n_TASK', 'manual__2026-02-23T12:36:42.067898+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:36:45.772+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG', 'producer_1p1n1n_TASK', 'manual__2026-02-23T12:36:42.067898+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:36:47.820+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:36:48.649+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG.producer_1p1n1n_TASK manual__2026-02-23T12:36:42.067898+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:36:49.699+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG', task_id='producer_1p1n1n_TASK', run_id='manual__2026-02-23T12:36:42.067898+00:00', try_number=1, map_index=-1)
[2026-02-23T20:36:49.710+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG, task_id=producer_1p1n1n_TASK, run_id=manual__2026-02-23T12:36:42.067898+00:00, map_index=-1, run_start_date=2026-02-23 12:36:48.718285+00:00, run_end_date=2026-02-23 12:36:49.095870+00:00, run_duration=0.377585, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=169, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:36:45.767107+00:00, queued_by_job_id=164, pid=7418
[2026-02-23T20:36:52.568+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG @ 2026-02-23 12:36:42.067898+00:00: manual__2026-02-23T12:36:42.067898+00:00, state:running, queued_at: 2026-02-23 12:36:42.100555+00:00. externally triggered: True> successful
[2026-02-23T20:36:52.569+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG, execution_date=2026-02-23 12:36:42.067898+00:00, run_id=manual__2026-02-23T12:36:42.067898+00:00, run_start_date=2026-02-23 12:36:45.745761+00:00, run_end_date=2026-02-23 12:36:52.569726+00:00, run_duration=6.823965, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 12:36:42.067898+00:00, dag_hash=73eff315c4e370fe2fee9d0c70db841f
[2026-02-23T20:41:04.025+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:42:36.535+0800] {manager.py:537} INFO - DAG producer_1p1n1n_DAG is missing and will be deactivated.
[2026-02-23T20:42:36.538+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-23T20:42:36.542+0800] {manager.py:553} INFO - Deleted DAG producer_1p1n1n_DAG in serialized_dag table
[2026-02-23T20:42:46.838+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:42:43.289053+00:00 [scheduled]>
[2026-02-23T20:42:46.839+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:42:46.840+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:42:43.289053+00:00 [scheduled]>
[2026-02-23T20:42:46.842+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:42:43.289053+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:42:46.843+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:42:43.289053+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:42:46.844+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:42:43.289053+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:42:46.847+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:42:43.289053+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:42:48.874+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:42:49.960+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:42:43.289053+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:42:51.030+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:42:43.289053+00:00', try_number=1, map_index=-1)
[2026-02-23T20:42:51.042+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T12:42:43.289053+00:00, map_index=-1, run_start_date=2026-02-23 12:42:50.030507+00:00, run_end_date=2026-02-23 12:42:50.409909+00:00, run_duration=0.379402, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=170, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:42:46.841417+00:00, queued_by_job_id=164, pid=7611
[2026-02-23T20:42:53.854+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 12:42:43.289053+00:00: manual__2026-02-23T12:42:43.289053+00:00, state:running, queued_at: 2026-02-23 12:42:43.299780+00:00. externally triggered: True> successful
[2026-02-23T20:42:53.855+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 12:42:43.289053+00:00, run_id=manual__2026-02-23T12:42:43.289053+00:00, run_start_date=2026-02-23 12:42:46.817996+00:00, run_end_date=2026-02-23 12:42:53.855545+00:00, run_duration=7.037549, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:42:43.289053+00:00, data_interval_end=2026-02-23 12:42:43.289053+00:00, dag_hash=54a8a8f7dac833eb3120bed1fc344456
[2026-02-23T20:42:53.865+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:42:50.427346+00:00 [scheduled]>
[2026-02-23T20:42:53.865+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:42:53.866+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:42:50.427346+00:00 [scheduled]>
[2026-02-23T20:42:53.868+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:42:50.427346+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:42:53.870+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:42:50.427346+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:42:53.870+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:42:50.427346+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:42:53.872+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:42:50.427346+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:42:55.903+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:42:56.544+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:42:50.427346+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:42:57.575+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:42:50.427346+00:00', try_number=1, map_index=-1)
[2026-02-23T20:42:57.586+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T12:42:50.427346+00:00, map_index=-1, run_start_date=2026-02-23 12:42:56.613066+00:00, run_end_date=2026-02-23 12:42:57.010439+00:00, run_duration=0.397373, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=171, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:42:53.867508+00:00, queued_by_job_id=164, pid=7615
[2026-02-23T20:43:00.439+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 12:42:50.427346+00:00: dataset_triggered__2026-02-23T12:42:50.427346+00:00, state:running, queued_at: 2026-02-23 12:42:53.818389+00:00. externally triggered: False> successful
[2026-02-23T20:43:00.440+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 12:42:50.427346+00:00, run_id=dataset_triggered__2026-02-23T12:42:50.427346+00:00, run_start_date=2026-02-23 12:42:53.841849+00:00, run_end_date=2026-02-23 12:43:00.440342+00:00, run_duration=6.598493, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 12:42:43.289053+00:00, dag_hash=0e218173cd11ca6528128f8aadab625f
[2026-02-23T20:43:38.429+0800] {manager.py:537} INFO - DAG consumer_1p1n1n_DAG is missing and will be deactivated.
[2026-02-23T20:43:38.432+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-23T20:43:38.436+0800] {manager.py:553} INFO - Deleted DAG consumer_1p1n1n_DAG in serialized_dag table
[2026-02-23T20:44:47.681+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:44:44.367568+00:00 [scheduled]>
[2026-02-23T20:44:47.682+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:44:47.682+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:44:44.367568+00:00 [scheduled]>
[2026-02-23T20:44:47.684+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:44:44.367568+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:44:47.685+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:44:44.367568+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:44:47.686+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:44:44.367568+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:44:47.688+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:44:44.367568+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:44:49.727+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:44:50.691+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:44:44.367568+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:44:51.791+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:44:44.367568+00:00', try_number=1, map_index=-1)
[2026-02-23T20:44:51.801+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T12:44:44.367568+00:00, map_index=-1, run_start_date=2026-02-23 12:44:50.769522+00:00, run_end_date=2026-02-23 12:44:51.155398+00:00, run_duration=0.385876, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=172, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:44:47.683745+00:00, queued_by_job_id=164, pid=7687
[2026-02-23T20:44:54.322+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 12:44:44.367568+00:00: manual__2026-02-23T12:44:44.367568+00:00, state:running, queued_at: 2026-02-23 12:44:44.379953+00:00. externally triggered: True> successful
[2026-02-23T20:44:54.324+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 12:44:44.367568+00:00, run_id=manual__2026-02-23T12:44:44.367568+00:00, run_start_date=2026-02-23 12:44:47.658753+00:00, run_end_date=2026-02-23 12:44:54.324019+00:00, run_duration=6.665266, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:44:44.367568+00:00, data_interval_end=2026-02-23 12:44:44.367568+00:00, dag_hash=54a8a8f7dac833eb3120bed1fc344456
[2026-02-23T20:44:54.334+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:44:51.172956+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:44:51.175494+00:00 [scheduled]>
[2026-02-23T20:44:54.335+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:44:54.335+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_2_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:44:54.336+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:44:51.172956+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:44:51.175494+00:00 [scheduled]>
[2026-02-23T20:44:54.339+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:44:51.172956+00:00 [scheduled]>, <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:44:51.175494+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:44:54.340+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:44:51.172956+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:44:54.341+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:44:51.172956+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:44:54.342+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:44:51.175494+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:44:54.342+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:44:51.175494+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:44:54.345+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:44:51.172956+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:44:56.362+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:44:57.009+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:44:51.172956+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:44:58.172+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:44:51.175494+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:45:00.208+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:45:00.801+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:44:51.175494+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:45:01.888+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:44:51.172956+00:00', try_number=1, map_index=-1)
[2026-02-23T20:45:01.892+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:44:51.175494+00:00', try_number=1, map_index=-1)
[2026-02-23T20:45:01.904+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T12:44:51.172956+00:00, map_index=-1, run_start_date=2026-02-23 12:44:57.084589+00:00, run_end_date=2026-02-23 12:44:57.494667+00:00, run_duration=0.410078, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=173, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:44:54.337606+00:00, queued_by_job_id=164, pid=7692
[2026-02-23T20:45:01.905+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_2_DAG2, task_id=consumer_1p1n1n_2_TASK, run_id=dataset_triggered__2026-02-23T12:44:51.175494+00:00, map_index=-1, run_start_date=2026-02-23 12:45:00.866084+00:00, run_end_date=2026-02-23 12:45:01.289128+00:00, run_duration=0.423044, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=174, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:44:54.337606+00:00, queued_by_job_id=164, pid=7697
[2026-02-23T20:45:05.940+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 12:44:51.172956+00:00: dataset_triggered__2026-02-23T12:44:51.172956+00:00, state:running, queued_at: 2026-02-23 12:44:54.285542+00:00. externally triggered: False> successful
[2026-02-23T20:45:05.941+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 12:44:51.172956+00:00, run_id=dataset_triggered__2026-02-23T12:44:51.172956+00:00, run_start_date=2026-02-23 12:44:54.306404+00:00, run_end_date=2026-02-23 12:45:05.941225+00:00, run_duration=11.634821, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:44:44.367568+00:00, data_interval_end=2026-02-23 12:44:44.367568+00:00, dag_hash=0e218173cd11ca6528128f8aadab625f
[2026-02-23T20:45:05.956+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_2_DAG2 @ 2026-02-23 12:44:51.175494+00:00: dataset_triggered__2026-02-23T12:44:51.175494+00:00, state:running, queued_at: 2026-02-23 12:44:54.295007+00:00. externally triggered: False> successful
[2026-02-23T20:45:05.959+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_2_DAG2, execution_date=2026-02-23 12:44:51.175494+00:00, run_id=dataset_triggered__2026-02-23T12:44:51.175494+00:00, run_start_date=2026-02-23 12:44:54.306517+00:00, run_end_date=2026-02-23 12:45:05.959922+00:00, run_duration=11.653405, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 12:44:44.367568+00:00, dag_hash=1dec3ea4c62eae8079bc8fe8ab2f2619
[2026-02-23T20:46:05.126+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:47:38.954+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:47:37.536170+00:00 [scheduled]>
[2026-02-23T20:47:38.954+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:47:38.955+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:47:37.536170+00:00 [scheduled]>
[2026-02-23T20:47:38.957+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:47:37.536170+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:47:38.958+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:47:37.536170+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:47:38.959+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:47:37.536170+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:38.961+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:47:37.536170+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:40.999+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:47:41.764+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:47:37.536170+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:47:42.907+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:47:37.536170+00:00', try_number=1, map_index=-1)
[2026-02-23T20:47:42.918+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T12:47:37.536170+00:00, map_index=-1, run_start_date=2026-02-23 12:47:41.838649+00:00, run_end_date=2026-02-23 12:47:42.212661+00:00, run_duration=0.374012, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=175, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:47:38.956640+00:00, queued_by_job_id=164, pid=7789
[2026-02-23T20:47:45.655+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 12:47:37.536170+00:00: manual__2026-02-23T12:47:37.536170+00:00, state:running, queued_at: 2026-02-23 12:47:37.547703+00:00. externally triggered: True> successful
[2026-02-23T20:47:45.656+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 12:47:37.536170+00:00, run_id=manual__2026-02-23T12:47:37.536170+00:00, run_start_date=2026-02-23 12:47:38.933495+00:00, run_end_date=2026-02-23 12:47:45.656878+00:00, run_duration=6.723383, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:47:37.536170+00:00, data_interval_end=2026-02-23 12:47:37.536170+00:00, dag_hash=54a8a8f7dac833eb3120bed1fc344456
[2026-02-23T20:47:45.666+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:47:42.229098+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:47:42.230542+00:00 [scheduled]>
[2026-02-23T20:47:45.667+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_2_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:47:45.668+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:47:45.669+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:47:42.229098+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:47:42.230542+00:00 [scheduled]>
[2026-02-23T20:47:45.671+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:47:42.229098+00:00 [scheduled]>, <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:47:42.230542+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:47:45.672+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:47:42.229098+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:47:45.673+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:47:42.229098+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:45.674+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:47:42.230542+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:47:45.675+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:47:42.230542+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:45.677+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:47:42.229098+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:47.785+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:47:48.385+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:47:42.229098+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:47:49.508+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:47:42.230542+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:47:51.642+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:47:52.263+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:47:42.230542+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:47:53.462+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:47:42.229098+00:00', try_number=1, map_index=-1)
[2026-02-23T20:47:53.466+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:47:42.230542+00:00', try_number=1, map_index=-1)
[2026-02-23T20:47:53.475+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T12:47:42.230542+00:00, map_index=-1, run_start_date=2026-02-23 12:47:52.336654+00:00, run_end_date=2026-02-23 12:47:52.758501+00:00, run_duration=0.421847, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=177, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:47:45.670492+00:00, queued_by_job_id=164, pid=7794
[2026-02-23T20:47:53.476+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_2_DAG2, task_id=consumer_1p1n1n_2_TASK, run_id=dataset_triggered__2026-02-23T12:47:42.229098+00:00, map_index=-1, run_start_date=2026-02-23 12:47:48.454975+00:00, run_end_date=2026-02-23 12:47:48.835320+00:00, run_duration=0.380345, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=176, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:47:45.670492+00:00, queued_by_job_id=164, pid=7792
[2026-02-23T20:47:55.904+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_2_DAG2 @ 2026-02-23 12:47:42.229098+00:00: dataset_triggered__2026-02-23T12:47:42.229098+00:00, state:running, queued_at: 2026-02-23 12:47:45.625390+00:00. externally triggered: False> successful
[2026-02-23T20:47:55.905+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_2_DAG2, execution_date=2026-02-23 12:47:42.229098+00:00, run_id=dataset_triggered__2026-02-23T12:47:42.229098+00:00, run_start_date=2026-02-23 12:47:45.638055+00:00, run_end_date=2026-02-23 12:47:55.905428+00:00, run_duration=10.267373, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:47:37.536170+00:00, data_interval_end=2026-02-23 12:47:37.536170+00:00, dag_hash=1dec3ea4c62eae8079bc8fe8ab2f2619
[2026-02-23T20:47:55.912+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 12:47:42.230542+00:00: dataset_triggered__2026-02-23T12:47:42.230542+00:00, state:running, queued_at: 2026-02-23 12:47:45.617308+00:00. externally triggered: False> failed
[2026-02-23T20:47:55.913+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 12:47:42.230542+00:00, run_id=dataset_triggered__2026-02-23T12:47:42.230542+00:00, run_start_date=2026-02-23 12:47:45.638161+00:00, run_end_date=2026-02-23 12:47:55.913509+00:00, run_duration=10.275348, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:47:37.536170+00:00, data_interval_end=2026-02-23 12:47:37.536170+00:00, dag_hash=0e218173cd11ca6528128f8aadab625f
[2026-02-23T20:48:38.761+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:48:35.015388+00:00 [scheduled]>
[2026-02-23T20:48:38.762+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:48:38.763+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:48:35.015388+00:00 [scheduled]>
[2026-02-23T20:48:38.765+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:48:35.015388+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:48:38.766+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:48:35.015388+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:48:38.767+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:48:35.015388+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:38.769+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:48:35.015388+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:40.800+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:48:41.568+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:48:35.015388+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:48:42.813+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:48:35.015388+00:00', try_number=1, map_index=-1)
[2026-02-23T20:48:42.824+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T12:48:35.015388+00:00, map_index=-1, run_start_date=2026-02-23 12:48:41.643936+00:00, run_end_date=2026-02-23 12:48:42.041275+00:00, run_duration=0.397339, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=178, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:48:38.764393+00:00, queued_by_job_id=164, pid=7829
[2026-02-23T20:48:45.461+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 12:48:35.015388+00:00: manual__2026-02-23T12:48:35.015388+00:00, state:running, queued_at: 2026-02-23 12:48:35.029627+00:00. externally triggered: True> successful
[2026-02-23T20:48:45.462+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 12:48:35.015388+00:00, run_id=manual__2026-02-23T12:48:35.015388+00:00, run_start_date=2026-02-23 12:48:38.743692+00:00, run_end_date=2026-02-23 12:48:45.462597+00:00, run_duration=6.718905, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:48:35.015388+00:00, data_interval_end=2026-02-23 12:48:35.015388+00:00, dag_hash=54a8a8f7dac833eb3120bed1fc344456
[2026-02-23T20:48:45.473+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:48:42.060509+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:48:42.062035+00:00 [scheduled]>
[2026-02-23T20:48:45.474+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_2_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:48:45.474+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:48:45.475+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:48:42.060509+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:48:42.062035+00:00 [scheduled]>
[2026-02-23T20:48:45.478+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:48:42.060509+00:00 [scheduled]>, <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:48:42.062035+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:48:45.479+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:48:42.060509+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:48:45.479+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:48:42.060509+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:45.480+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:48:42.062035+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:48:45.481+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:48:42.062035+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:45.483+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:48:42.060509+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:47.526+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:48:48.170+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:48:42.060509+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:48:49.364+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:48:42.062035+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:48:51.362+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:48:51.959+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:48:42.062035+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:48:53.063+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:48:42.060509+00:00', try_number=1, map_index=-1)
[2026-02-23T20:48:53.066+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:48:42.062035+00:00', try_number=1, map_index=-1)
[2026-02-23T20:48:53.075+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T12:48:42.062035+00:00, map_index=-1, run_start_date=2026-02-23 12:48:52.028771+00:00, run_end_date=2026-02-23 12:48:52.409983+00:00, run_duration=0.381212, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=180, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:48:45.476693+00:00, queued_by_job_id=164, pid=7834
[2026-02-23T20:48:53.076+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_2_DAG2, task_id=consumer_1p1n1n_2_TASK, run_id=dataset_triggered__2026-02-23T12:48:42.060509+00:00, map_index=-1, run_start_date=2026-02-23 12:48:48.245563+00:00, run_end_date=2026-02-23 12:48:48.650219+00:00, run_duration=0.404656, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=179, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:48:45.476693+00:00, queued_by_job_id=164, pid=7832
[2026-02-23T20:48:55.877+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_2_DAG2 @ 2026-02-23 12:48:42.060509+00:00: dataset_triggered__2026-02-23T12:48:42.060509+00:00, state:running, queued_at: 2026-02-23 12:48:45.431483+00:00. externally triggered: False> successful
[2026-02-23T20:48:55.878+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_2_DAG2, execution_date=2026-02-23 12:48:42.060509+00:00, run_id=dataset_triggered__2026-02-23T12:48:42.060509+00:00, run_start_date=2026-02-23 12:48:45.444466+00:00, run_end_date=2026-02-23 12:48:55.878238+00:00, run_duration=10.433772, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:48:35.015388+00:00, data_interval_end=2026-02-23 12:48:35.015388+00:00, dag_hash=1dec3ea4c62eae8079bc8fe8ab2f2619
[2026-02-23T20:48:55.883+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 12:48:42.062035+00:00: dataset_triggered__2026-02-23T12:48:42.062035+00:00, state:running, queued_at: 2026-02-23 12:48:45.422572+00:00. externally triggered: False> successful
[2026-02-23T20:48:55.884+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 12:48:42.062035+00:00, run_id=dataset_triggered__2026-02-23T12:48:42.062035+00:00, run_start_date=2026-02-23 12:48:45.444583+00:00, run_end_date=2026-02-23 12:48:55.884490+00:00, run_duration=10.439907, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:48:35.015388+00:00, data_interval_end=2026-02-23 12:48:35.015388+00:00, dag_hash=0e218173cd11ca6528128f8aadab625f
[2026-02-23T20:51:05.744+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:56:05.906+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T20:57:44.167+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:57:40.059074+00:00 [scheduled]>
[2026-02-23T20:57:44.168+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:57:44.169+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:57:40.059074+00:00 [scheduled]>
[2026-02-23T20:57:44.171+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:57:40.059074+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:57:44.172+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:57:40.059074+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:57:44.173+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:57:40.059074+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:44.176+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T12:57:40.059074+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:46.294+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:57:47.178+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T12:57:40.059074+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:57:48.337+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T12:57:40.059074+00:00', try_number=1, map_index=-1)
[2026-02-23T20:57:48.348+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T12:57:40.059074+00:00, map_index=-1, run_start_date=2026-02-23 12:57:47.256474+00:00, run_end_date=2026-02-23 12:57:47.640973+00:00, run_duration=0.384499, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=181, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:57:44.170339+00:00, queued_by_job_id=164, pid=8119
[2026-02-23T20:57:50.896+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 12:57:40.059074+00:00: manual__2026-02-23T12:57:40.059074+00:00, state:running, queued_at: 2026-02-23 12:57:40.076516+00:00. externally triggered: True> successful
[2026-02-23T20:57:50.897+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 12:57:40.059074+00:00, run_id=manual__2026-02-23T12:57:40.059074+00:00, run_start_date=2026-02-23 12:57:44.146568+00:00, run_end_date=2026-02-23 12:57:50.897618+00:00, run_duration=6.75105, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 12:57:40.059074+00:00, data_interval_end=2026-02-23 12:57:40.059074+00:00, dag_hash=54a8a8f7dac833eb3120bed1fc344456
[2026-02-23T20:57:50.907+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:57:47.658868+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:57:47.660222+00:00 [scheduled]>
[2026-02-23T20:57:50.908+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_2_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:57:50.909+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T20:57:50.910+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:57:47.658868+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:57:47.660222+00:00 [scheduled]>
[2026-02-23T20:57:50.913+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:57:47.658868+00:00 [scheduled]>, <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:57:47.660222+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T20:57:50.914+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:57:47.658868+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:57:50.914+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:57:47.658868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:50.915+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:57:47.660222+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T20:57:50.916+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:57:47.660222+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:50.918+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T12:57:47.658868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:52.973+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:57:53.638+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T12:57:47.658868+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:57:54.804+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T12:57:47.660222+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer2.py']
[2026-02-23T20:57:56.909+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer2.py
[2026-02-23T20:57:57.521+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T12:57:47.660222+00:00 [queued]> on host localhost-2.local
[2026-02-23T20:57:58.631+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T12:57:47.658868+00:00', try_number=1, map_index=-1)
[2026-02-23T20:57:58.634+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T12:57:47.660222+00:00', try_number=1, map_index=-1)
[2026-02-23T20:57:58.645+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T12:57:47.660222+00:00, map_index=-1, run_start_date=2026-02-23 12:57:57.592466+00:00, run_end_date=2026-02-23 12:57:57.980252+00:00, run_duration=0.387786, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=183, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:57:50.911672+00:00, queued_by_job_id=164, pid=8124
[2026-02-23T20:57:58.646+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_2_DAG2, task_id=consumer_1p1n1n_2_TASK, run_id=dataset_triggered__2026-02-23T12:57:47.658868+00:00, map_index=-1, run_start_date=2026-02-23 12:57:53.713484+00:00, run_end_date=2026-02-23 12:57:54.122157+00:00, run_duration=0.408673, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=182, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 12:57:50.911672+00:00, queued_by_job_id=164, pid=8122
[2026-02-23T20:58:01.321+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_2_DAG2 @ 2026-02-23 12:57:47.658868+00:00: dataset_triggered__2026-02-23T12:57:47.658868+00:00, state:running, queued_at: 2026-02-23 12:57:50.865017+00:00. externally triggered: False> successful
[2026-02-23T20:58:01.322+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_2_DAG2, execution_date=2026-02-23 12:57:47.658868+00:00, run_id=dataset_triggered__2026-02-23T12:57:47.658868+00:00, run_start_date=2026-02-23 12:57:50.878467+00:00, run_end_date=2026-02-23 12:58:01.322407+00:00, run_duration=10.44394, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:57:40.059074+00:00, data_interval_end=2026-02-23 12:57:40.059074+00:00, dag_hash=1dec3ea4c62eae8079bc8fe8ab2f2619
[2026-02-23T20:58:01.328+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 12:57:47.660222+00:00: dataset_triggered__2026-02-23T12:57:47.660222+00:00, state:running, queued_at: 2026-02-23 12:57:50.855560+00:00. externally triggered: False> successful
[2026-02-23T20:58:01.329+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 12:57:47.660222+00:00, run_id=dataset_triggered__2026-02-23T12:57:47.660222+00:00, run_start_date=2026-02-23 12:57:50.878575+00:00, run_end_date=2026-02-23 12:58:01.329360+00:00, run_duration=10.450785, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:57:40.059074+00:00, data_interval_end=2026-02-23 12:57:40.059074+00:00, dag_hash=0e218173cd11ca6528128f8aadab625f
[2026-02-23T21:01:07.925+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:05:56.889+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T13:05:54.745009+00:00 [scheduled]>
[2026-02-23T21:05:56.890+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:05:56.891+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T13:05:54.745009+00:00 [scheduled]>
[2026-02-23T21:05:56.893+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T13:05:54.745009+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:05:56.894+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T13:05:54.745009+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:05:56.895+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T13:05:54.745009+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:05:56.897+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1p1n1n_DAG2', 'producer_1p1n1n_TASK2', 'manual__2026-02-23T13:05:54.745009+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:05:58.928+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:05:59.996+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1p1n1n_DAG2.producer_1p1n1n_TASK2 manual__2026-02-23T13:05:54.745009+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:06:01.164+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1p1n1n_DAG2', task_id='producer_1p1n1n_TASK2', run_id='manual__2026-02-23T13:05:54.745009+00:00', try_number=1, map_index=-1)
[2026-02-23T21:06:01.177+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1p1n1n_DAG2, task_id=producer_1p1n1n_TASK2, run_id=manual__2026-02-23T13:05:54.745009+00:00, map_index=-1, run_start_date=2026-02-23 13:06:00.077113+00:00, run_end_date=2026-02-23 13:06:00.433427+00:00, run_duration=0.356314, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=184, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:05:56.892275+00:00, queued_by_job_id=164, pid=8388
[2026-02-23T21:06:03.985+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1p1n1n_DAG2 @ 2026-02-23 13:05:54.745009+00:00: manual__2026-02-23T13:05:54.745009+00:00, state:running, queued_at: 2026-02-23 13:05:54.757991+00:00. externally triggered: True> successful
[2026-02-23T21:06:03.986+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1p1n1n_DAG2, execution_date=2026-02-23 13:05:54.745009+00:00, run_id=manual__2026-02-23T13:05:54.745009+00:00, run_start_date=2026-02-23 13:05:56.866943+00:00, run_end_date=2026-02-23 13:06:03.986864+00:00, run_duration=7.119921, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:05:54.745009+00:00, data_interval_end=2026-02-23 13:05:54.745009+00:00, dag_hash=afab7f3ad8e780ea6c3bfb9334342fad
[2026-02-23T21:06:03.996+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T13:06:00.449957+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T13:06:00.451215+00:00 [scheduled]>
[2026-02-23T21:06:03.997+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:06:03.998+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1p1n1n_2_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:06:03.999+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T13:06:00.449957+00:00 [scheduled]>
	<TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T13:06:00.451215+00:00 [scheduled]>
[2026-02-23T21:06:04.001+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T13:06:00.449957+00:00 [scheduled]>, <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T13:06:00.451215+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:06:04.002+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T13:06:00.449957+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:06:04.003+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T13:06:00.449957+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:06:04.004+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T13:06:00.451215+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:06:04.005+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T13:06:00.451215+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:06:04.008+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_DAG2', 'consumer_1p1n1n_TASK', 'dataset_triggered__2026-02-23T13:06:00.449957+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:06:06.005+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:06:06.615+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_DAG2.consumer_1p1n1n_TASK dataset_triggered__2026-02-23T13:06:00.449957+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:06:07.727+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1p1n1n_2_DAG2', 'consumer_1p1n1n_2_TASK', 'dataset_triggered__2026-02-23T13:06:00.451215+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:06:09.844+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:06:10.442+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1p1n1n_2_DAG2.consumer_1p1n1n_2_TASK dataset_triggered__2026-02-23T13:06:00.451215+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:06:11.587+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_DAG2', task_id='consumer_1p1n1n_TASK', run_id='dataset_triggered__2026-02-23T13:06:00.449957+00:00', try_number=1, map_index=-1)
[2026-02-23T21:06:11.590+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1p1n1n_2_DAG2', task_id='consumer_1p1n1n_2_TASK', run_id='dataset_triggered__2026-02-23T13:06:00.451215+00:00', try_number=1, map_index=-1)
[2026-02-23T21:06:11.599+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_DAG2, task_id=consumer_1p1n1n_TASK, run_id=dataset_triggered__2026-02-23T13:06:00.449957+00:00, map_index=-1, run_start_date=2026-02-23 13:06:06.682007+00:00, run_end_date=2026-02-23 13:06:07.064336+00:00, run_duration=0.382329, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=185, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:06:04.000229+00:00, queued_by_job_id=164, pid=8395
[2026-02-23T21:06:11.600+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1p1n1n_2_DAG2, task_id=consumer_1p1n1n_2_TASK, run_id=dataset_triggered__2026-02-23T13:06:00.451215+00:00, map_index=-1, run_start_date=2026-02-23 13:06:10.512219+00:00, run_end_date=2026-02-23 13:06:10.882436+00:00, run_duration=0.370217, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=186, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:06:04.000229+00:00, queued_by_job_id=164, pid=8397
[2026-02-23T21:06:11.630+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:06:14.246+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_DAG2 @ 2026-02-23 13:06:00.449957+00:00: dataset_triggered__2026-02-23T13:06:00.449957+00:00, state:running, queued_at: 2026-02-23 13:06:03.945720+00:00. externally triggered: False> successful
[2026-02-23T21:06:14.247+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_DAG2, execution_date=2026-02-23 13:06:00.449957+00:00, run_id=dataset_triggered__2026-02-23T13:06:00.449957+00:00, run_start_date=2026-02-23 13:06:03.969085+00:00, run_end_date=2026-02-23 13:06:14.247827+00:00, run_duration=10.278742, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:05:54.745009+00:00, data_interval_end=2026-02-23 13:05:54.745009+00:00, dag_hash=55a5868a6acc4dd0b1aaea6c342baf1f
[2026-02-23T21:06:14.252+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1p1n1n_2_DAG2 @ 2026-02-23 13:06:00.451215+00:00: dataset_triggered__2026-02-23T13:06:00.451215+00:00, state:running, queued_at: 2026-02-23 13:06:03.954555+00:00. externally triggered: False> successful
[2026-02-23T21:06:14.253+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1p1n1n_2_DAG2, execution_date=2026-02-23 13:06:00.451215+00:00, run_id=dataset_triggered__2026-02-23T13:06:00.451215+00:00, run_start_date=2026-02-23 13:06:03.969224+00:00, run_end_date=2026-02-23 13:06:14.253253+00:00, run_duration=10.284029, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:05:54.745009+00:00, data_interval_end=2026-02-23 13:05:54.745009+00:00, dag_hash=00ef0ba92dda97741bce243dcc0589ef
[2026-02-23T21:11:13.281+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:16:15.112+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:17:43.111+0800] {manager.py:537} INFO - DAG producer_1p1n1n_DAG2 is missing and will be deactivated.
[2026-02-23T21:17:43.112+0800] {manager.py:537} INFO - DAG consumer_1p1n1n_DAG2 is missing and will be deactivated.
[2026-02-23T21:17:43.112+0800] {manager.py:537} INFO - DAG consumer_1p1n1n_2_DAG2 is missing and will be deactivated.
[2026-02-23T21:17:43.115+0800] {manager.py:549} INFO - Deactivated 3 DAGs which are no longer present in file.
[2026-02-23T21:17:43.119+0800] {manager.py:553} INFO - Deleted DAG consumer_1p1n1n_DAG2 in serialized_dag table
[2026-02-23T21:17:43.122+0800] {manager.py:553} INFO - Deleted DAG consumer_1p1n1n_2_DAG2 in serialized_dag table
[2026-02-23T21:17:43.126+0800] {manager.py:553} INFO - Deleted DAG producer_1p1n1n_DAG2 in serialized_dag table
[2026-02-23T21:18:50.210+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:18:49.137091+00:00 [scheduled]>
[2026-02-23T21:18:50.211+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p1n1n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:18:50.212+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:18:49.137091+00:00 [scheduled]>
[2026-02-23T21:18:50.215+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:18:49.137091+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:18:50.217+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:18:49.137091+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:18:50.218+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:18:49.137091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:18:50.220+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:18:49.137091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:18:52.355+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:18:53.355+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:18:49.137091+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:18:54.394+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:18:49.137091+00:00', try_number=1, map_index=-1)
[2026-02-23T21:18:54.409+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p1n1n_DAGs, task_id=p_1p1n1n_TASK, run_id=manual__2026-02-23T13:18:49.137091+00:00, map_index=-1, run_start_date=2026-02-23 13:18:53.426174+00:00, run_end_date=2026-02-23 13:18:53.793030+00:00, run_duration=0.366856, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=187, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:18:50.214607+00:00, queued_by_job_id=164, pid=8813
[2026-02-23T21:18:57.414+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:18:57.454+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p1n1n_DAGs @ 2026-02-23 13:18:49.137091+00:00: manual__2026-02-23T13:18:49.137091+00:00, state:running, queued_at: 2026-02-23 13:18:49.148228+00:00. externally triggered: True> successful
[2026-02-23T21:18:57.455+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p1n1n_DAGs, execution_date=2026-02-23 13:18:49.137091+00:00, run_id=manual__2026-02-23T13:18:49.137091+00:00, run_start_date=2026-02-23 13:18:50.190129+00:00, run_end_date=2026-02-23 13:18:57.455109+00:00, run_duration=7.26498, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:18:49.137091+00:00, data_interval_end=2026-02-23 13:18:49.137091+00:00, dag_hash=27f14b92c1fec47bd0d9b691229d87b8
[2026-02-23T21:18:57.465+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:18:53.809970+00:00 [scheduled]>
[2026-02-23T21:18:57.466+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_1_DAGs has 0/16 running and queued tasks
[2026-02-23T21:18:57.467+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:18:53.809970+00:00 [scheduled]>
[2026-02-23T21:18:57.469+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:18:53.809970+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:18:57.470+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_1_DAGs', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:18:53.809970+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:18:57.471+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_1_DAGs', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:18:53.809970+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:18:57.473+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_1_DAGs', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:18:53.809970+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:18:59.586+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:19:00.239+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:18:53.809970+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:19:01.272+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_1_DAGs', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:18:53.809970+00:00', try_number=1, map_index=-1)
[2026-02-23T21:19:01.282+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_1_DAGs, task_id=c_1p1n1n_TASK1, run_id=dataset_triggered__2026-02-23T13:18:53.809970+00:00, map_index=-1, run_start_date=2026-02-23 13:19:00.304786+00:00, run_end_date=2026-02-23 13:19:00.690528+00:00, run_duration=0.385742, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=188, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:18:57.468149+00:00, queued_by_job_id=164, pid=8818
[2026-02-23T21:19:04.413+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:04.430+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_1_DAGs @ 2026-02-23 13:18:53.809970+00:00: dataset_triggered__2026-02-23T13:18:53.809970+00:00, state:running, queued_at: 2026-02-23 13:18:57.424835+00:00. externally triggered: False> successful
[2026-02-23T21:19:04.432+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_1_DAGs, execution_date=2026-02-23 13:18:53.809970+00:00, run_id=dataset_triggered__2026-02-23T13:18:53.809970+00:00, run_start_date=2026-02-23 13:18:57.441030+00:00, run_end_date=2026-02-23 13:19:04.432380+00:00, run_duration=6.99135, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 13:18:49.137091+00:00, dag_hash=0c0646a7d506eccf16b3d0f38735f60f
[2026-02-23T21:19:08.723+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:13.033+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:16.776+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:20.794+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:24.997+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:28.959+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:33.425+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:37.739+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:41.533+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:44.529+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:48.305+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p1n1n_DAGs' not found in serialized_dag table
[2026-02-23T21:19:48.472+0800] {manager.py:537} INFO - DAG c_1p1n1n_DAGs is missing and will be deactivated.
[2026-02-23T21:19:48.475+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-23T21:19:48.477+0800] {manager.py:553} INFO - Deleted DAG c_1p1n1n_DAGs in serialized_dag table
[2026-02-23T21:20:33.632+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:20:30.038826+00:00 [scheduled]>
[2026-02-23T21:20:33.633+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p1n1n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:20:33.634+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:20:30.038826+00:00 [scheduled]>
[2026-02-23T21:20:33.636+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:20:30.038826+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:20:33.637+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:20:30.038826+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:20:33.638+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:20:30.038826+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:33.640+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:20:30.038826+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:35.687+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:20:36.554+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:20:30.038826+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:20:37.759+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:20:30.038826+00:00', try_number=1, map_index=-1)
[2026-02-23T21:20:37.770+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p1n1n_DAGs, task_id=p_1p1n1n_TASK, run_id=manual__2026-02-23T13:20:30.038826+00:00, map_index=-1, run_start_date=2026-02-23 13:20:36.626837+00:00, run_end_date=2026-02-23 13:20:37.068065+00:00, run_duration=0.441228, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=189, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:20:33.635423+00:00, queued_by_job_id=164, pid=8877
[2026-02-23T21:20:40.472+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p1n1n_DAGs @ 2026-02-23 13:20:30.038826+00:00: manual__2026-02-23T13:20:30.038826+00:00, state:running, queued_at: 2026-02-23 13:20:30.050255+00:00. externally triggered: True> successful
[2026-02-23T21:20:40.473+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p1n1n_DAGs, execution_date=2026-02-23 13:20:30.038826+00:00, run_id=manual__2026-02-23T13:20:30.038826+00:00, run_start_date=2026-02-23 13:20:33.612090+00:00, run_end_date=2026-02-23 13:20:40.473035+00:00, run_duration=6.860945, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:20:30.038826+00:00, data_interval_end=2026-02-23 13:20:30.038826+00:00, dag_hash=27f14b92c1fec47bd0d9b691229d87b8
[2026-02-23T21:20:40.483+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: c_1p1n1n_2_DAGs.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:20:37.085461+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:20:37.087075+00:00 [scheduled]>
[2026-02-23T21:20:40.484+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_2_DAGs has 0/16 running and queued tasks
[2026-02-23T21:20:40.484+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_1_DAGs has 0/16 running and queued tasks
[2026-02-23T21:20:40.485+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p1n1n_2_DAGs.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:20:37.085461+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:20:37.087075+00:00 [scheduled]>
[2026-02-23T21:20:40.488+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p1n1n_2_DAGs.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:20:37.085461+00:00 [scheduled]>, <TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:20:37.087075+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:20:40.489+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_2_DAGs', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:20:37.085461+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:20:40.490+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_2_DAGs', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:20:37.085461+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:40.490+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_1_DAGs', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:20:37.087075+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:20:40.491+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_1_DAGs', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:20:37.087075+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:40.493+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_2_DAGs', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:20:37.085461+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:42.539+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:20:43.188+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_2_DAGs.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:20:37.085461+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:20:44.216+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_1_DAGs', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:20:37.087075+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:20:46.261+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:20:46.861+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_1_DAGs.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:20:37.087075+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:20:47.995+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_2_DAGs', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:20:37.085461+00:00', try_number=1, map_index=-1)
[2026-02-23T21:20:47.999+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_1_DAGs', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:20:37.087075+00:00', try_number=1, map_index=-1)
[2026-02-23T21:20:48.008+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_1_DAGs, task_id=c_1p1n1n_TASK1, run_id=dataset_triggered__2026-02-23T13:20:37.087075+00:00, map_index=-1, run_start_date=2026-02-23 13:20:46.931299+00:00, run_end_date=2026-02-23 13:20:47.313717+00:00, run_duration=0.382418, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=191, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:20:40.486847+00:00, queued_by_job_id=164, pid=8882
[2026-02-23T21:20:48.010+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_2_DAGs, task_id=c_1p1n1n_TASK2, run_id=dataset_triggered__2026-02-23T13:20:37.085461+00:00, map_index=-1, run_start_date=2026-02-23 13:20:43.257948+00:00, run_end_date=2026-02-23 13:20:43.658625+00:00, run_duration=0.400677, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=190, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:20:40.486847+00:00, queued_by_job_id=164, pid=8880
[2026-02-23T21:20:50.961+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_2_DAGs @ 2026-02-23 13:20:37.085461+00:00: dataset_triggered__2026-02-23T13:20:37.085461+00:00, state:running, queued_at: 2026-02-23 13:20:40.440889+00:00. externally triggered: False> successful
[2026-02-23T21:20:50.962+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_2_DAGs, execution_date=2026-02-23 13:20:37.085461+00:00, run_id=dataset_triggered__2026-02-23T13:20:37.085461+00:00, run_start_date=2026-02-23 13:20:40.455103+00:00, run_end_date=2026-02-23 13:20:50.962424+00:00, run_duration=10.507321, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 13:20:30.038826+00:00, dag_hash=8016e1e417c02ba35d885e5df125f601
[2026-02-23T21:20:50.970+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_1_DAGs @ 2026-02-23 13:20:37.087075+00:00: dataset_triggered__2026-02-23T13:20:37.087075+00:00, state:running, queued_at: 2026-02-23 13:20:40.427938+00:00. externally triggered: False> successful
[2026-02-23T21:20:50.971+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_1_DAGs, execution_date=2026-02-23 13:20:37.087075+00:00, run_id=dataset_triggered__2026-02-23T13:20:37.087075+00:00, run_start_date=2026-02-23 13:20:40.455208+00:00, run_end_date=2026-02-23 13:20:50.971687+00:00, run_duration=10.516479, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:20:30.038826+00:00, data_interval_end=2026-02-23 13:20:30.038826+00:00, dag_hash=0c0646a7d506eccf16b3d0f38735f60f
[2026-02-23T21:20:52.003+0800] {manager.py:537} INFO - DAG c_1p1n1n_2_DAG2 is missing and will be deactivated.
[2026-02-23T21:20:52.007+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-23T21:20:52.010+0800] {manager.py:553} INFO - Deleted DAG c_1p1n1n_2_DAG2 in serialized_dag table
[2026-02-23T21:21:18.164+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:22:53.768+0800] {manager.py:537} INFO - DAG consumer_1p_1n_1n_1_DAG is missing and will be deactivated.
[2026-02-23T21:22:53.770+0800] {manager.py:537} INFO - DAG producer_1p_1n_1n_DAG is missing and will be deactivated.
[2026-02-23T21:22:53.772+0800] {manager.py:549} INFO - Deactivated 2 DAGs which are no longer present in file.
[2026-02-23T21:22:53.776+0800] {manager.py:553} INFO - Deleted DAG consumer_1p_1n_1n_1_DAG in serialized_dag table
[2026-02-23T21:22:53.780+0800] {manager.py:553} INFO - Deleted DAG producer_1p_1n_1n_DAG in serialized_dag table
[2026-02-23T21:26:18.420+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:29:19.453+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:29:18.234249+00:00 [scheduled]>
[2026-02-23T21:29:19.454+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p2n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:29:19.455+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:29:18.234249+00:00 [scheduled]>
[2026-02-23T21:29:19.458+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:29:18.234249+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:29:19.459+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:29:18.234249+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:29:19.460+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:29:18.234249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:29:19.464+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:29:18.234249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:29:21.693+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:29:22.853+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:29:18.234249+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:29:23.883+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:29:18.234249+00:00', try_number=1, map_index=-1)
[2026-02-23T21:29:23.896+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p2n_DAGs, task_id=p_1p2n_TASK, run_id=manual__2026-02-23T13:29:18.234249+00:00, map_index=-1, run_start_date=2026-02-23 13:29:22.928509+00:00, run_end_date=2026-02-23 13:29:23.324867+00:00, run_duration=0.396358, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=192, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:29:19.457028+00:00, queued_by_job_id=164, pid=9161
[2026-02-23T21:29:27.104+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p2n_DAGs @ 2026-02-23 13:29:18.234249+00:00: manual__2026-02-23T13:29:18.234249+00:00, state:running, queued_at: 2026-02-23 13:29:18.247767+00:00. externally triggered: True> successful
[2026-02-23T21:29:27.105+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p2n_DAGs, execution_date=2026-02-23 13:29:18.234249+00:00, run_id=manual__2026-02-23T13:29:18.234249+00:00, run_start_date=2026-02-23 13:29:19.433367+00:00, run_end_date=2026-02-23 13:29:27.105695+00:00, run_duration=7.672328, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:29:18.234249+00:00, data_interval_end=2026-02-23 13:29:18.234249+00:00, dag_hash=bcc79dc6c0926a8d58a1f7ccd0ba8898
[2026-02-23T21:31:19.598+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:32:29.876+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:32:26.175750+00:00 [scheduled]>
[2026-02-23T21:32:29.877+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p2n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:32:29.878+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:32:26.175750+00:00 [scheduled]>
[2026-02-23T21:32:29.880+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:32:26.175750+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:32:29.881+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:32:26.175750+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:32:29.882+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:32:26.175750+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:29.884+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:32:26.175750+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:31.868+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:32:32.988+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:32:26.175750+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:32:34.105+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:32:26.175750+00:00', try_number=1, map_index=-1)
[2026-02-23T21:32:34.115+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p2n_DAGs, task_id=p_1p2n_TASK, run_id=manual__2026-02-23T13:32:26.175750+00:00, map_index=-1, run_start_date=2026-02-23 13:32:33.064195+00:00, run_end_date=2026-02-23 13:32:33.442117+00:00, run_duration=0.377922, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=193, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:32:29.879206+00:00, queued_by_job_id=164, pid=9270
[2026-02-23T21:32:37.073+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p2n_DAGs @ 2026-02-23 13:32:26.175750+00:00: manual__2026-02-23T13:32:26.175750+00:00, state:running, queued_at: 2026-02-23 13:32:26.190045+00:00. externally triggered: True> successful
[2026-02-23T21:32:37.074+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p2n_DAGs, execution_date=2026-02-23 13:32:26.175750+00:00, run_id=manual__2026-02-23T13:32:26.175750+00:00, run_start_date=2026-02-23 13:32:29.856149+00:00, run_end_date=2026-02-23 13:32:37.074262+00:00, run_duration=7.218113, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:32:26.175750+00:00, data_interval_end=2026-02-23 13:32:26.175750+00:00, dag_hash=bcc79dc6c0926a8d58a1f7ccd0ba8898
[2026-02-23T21:32:37.083+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:32:33.460676+00:00 [scheduled]>
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:32:33.462182+00:00 [scheduled]>
[2026-02-23T21:32:37.084+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:32:37.085+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG1 has 0/16 running and queued tasks
[2026-02-23T21:32:37.086+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:32:33.460676+00:00 [scheduled]>
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:32:33.462182+00:00 [scheduled]>
[2026-02-23T21:32:37.088+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:32:33.460676+00:00 [scheduled]>, <TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:32:33.462182+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:32:37.089+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:32:33.460676+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:32:37.090+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:32:33.460676+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:37.091+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:32:33.462182+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:32:37.092+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:32:33.462182+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:37.094+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:32:33.460676+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:39.237+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:32:39.876+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:32:33.460676+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:32:40.937+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:32:33.462182+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:32:42.945+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:32:43.538+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:32:33.462182+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:32:44.603+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:32:33.460676+00:00', try_number=1, map_index=-1)
[2026-02-23T21:32:44.605+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:32:33.462182+00:00', try_number=1, map_index=-1)
[2026-02-23T21:32:44.613+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG2, task_id=c_1p2n_TASK2, run_id=dataset_triggered__2026-02-23T13:32:33.460676+00:00, map_index=-1, run_start_date=2026-02-23 13:32:39.941758+00:00, run_end_date=2026-02-23 13:32:40.333680+00:00, run_duration=0.391922, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=194, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:32:37.087183+00:00, queued_by_job_id=164, pid=9273
[2026-02-23T21:32:44.615+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG1, task_id=c_1p2n_TASK1, run_id=dataset_triggered__2026-02-23T13:32:33.462182+00:00, map_index=-1, run_start_date=2026-02-23 13:32:43.601951+00:00, run_end_date=2026-02-23 13:32:43.968855+00:00, run_duration=0.366904, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=195, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:32:37.087183+00:00, queued_by_job_id=164, pid=9275
[2026-02-23T21:32:47.388+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG2 @ 2026-02-23 13:32:33.460676+00:00: dataset_triggered__2026-02-23T13:32:33.460676+00:00, state:running, queued_at: 2026-02-23 13:32:37.033464+00:00. externally triggered: False> successful
[2026-02-23T21:32:47.389+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG2, execution_date=2026-02-23 13:32:33.460676+00:00, run_id=dataset_triggered__2026-02-23T13:32:33.460676+00:00, run_start_date=2026-02-23 13:32:37.055947+00:00, run_end_date=2026-02-23 13:32:47.388884+00:00, run_duration=10.332937, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:29:18.234249+00:00, data_interval_end=2026-02-23 13:32:26.175750+00:00, dag_hash=354e8496efd6843df9de6fdd1d1833e5
[2026-02-23T21:32:47.394+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG1 @ 2026-02-23 13:32:33.462182+00:00: dataset_triggered__2026-02-23T13:32:33.462182+00:00, state:running, queued_at: 2026-02-23 13:32:37.041668+00:00. externally triggered: False> successful
[2026-02-23T21:32:47.395+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG1, execution_date=2026-02-23 13:32:33.462182+00:00, run_id=dataset_triggered__2026-02-23T13:32:33.462182+00:00, run_start_date=2026-02-23 13:32:37.056054+00:00, run_end_date=2026-02-23 13:32:47.395757+00:00, run_duration=10.339703, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:29:18.234249+00:00, data_interval_end=2026-02-23 13:32:26.175750+00:00, dag_hash=4a31cc71e5398c53fad0c9cea1feb380
[2026-02-23T21:33:10.740+0800] {manager.py:537} INFO - DAG c_1p2n_2_DAGs is missing and will be deactivated.
[2026-02-23T21:33:10.741+0800] {manager.py:537} INFO - DAG c_1p2n_1_DAGs is missing and will be deactivated.
[2026-02-23T21:33:10.744+0800] {manager.py:549} INFO - Deactivated 2 DAGs which are no longer present in file.
[2026-02-23T21:33:10.747+0800] {manager.py:553} INFO - Deleted DAG c_1p2n_2_DAGs in serialized_dag table
[2026-02-23T21:33:10.750+0800] {manager.py:553} INFO - Deleted DAG c_1p2n_1_DAGs in serialized_dag table
[2026-02-23T21:34:38.752+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:34:36.159302+00:00 [scheduled]>
[2026-02-23T21:34:38.753+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p1n1n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:34:38.753+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:34:36.159302+00:00 [scheduled]>
[2026-02-23T21:34:38.756+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:34:36.159302+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:34:38.757+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:34:36.159302+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:34:38.758+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:34:36.159302+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:38.760+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:34:36.159302+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:40.754+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:34:41.492+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:34:36.159302+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:34:42.626+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:34:36.159302+00:00', try_number=1, map_index=-1)
[2026-02-23T21:34:42.636+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p1n1n_DAGs, task_id=p_1p1n1n_TASK, run_id=manual__2026-02-23T13:34:36.159302+00:00, map_index=-1, run_start_date=2026-02-23 13:34:41.565426+00:00, run_end_date=2026-02-23 13:34:41.928499+00:00, run_duration=0.363073, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=196, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:34:38.755521+00:00, queued_by_job_id=164, pid=9350
[2026-02-23T21:34:45.465+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p1n1n_DAGs @ 2026-02-23 13:34:36.159302+00:00: manual__2026-02-23T13:34:36.159302+00:00, state:running, queued_at: 2026-02-23 13:34:36.172852+00:00. externally triggered: True> successful
[2026-02-23T21:34:45.466+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p1n1n_DAGs, execution_date=2026-02-23 13:34:36.159302+00:00, run_id=manual__2026-02-23T13:34:36.159302+00:00, run_start_date=2026-02-23 13:34:38.732650+00:00, run_end_date=2026-02-23 13:34:45.466376+00:00, run_duration=6.733726, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:34:36.159302+00:00, data_interval_end=2026-02-23 13:34:36.159302+00:00, dag_hash=27f14b92c1fec47bd0d9b691229d87b8
[2026-02-23T21:34:45.475+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:34:41.945741+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:34:41.947304+00:00 [scheduled]>
[2026-02-23T21:34:45.477+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_DAG1 has 0/16 running and queued tasks
[2026-02-23T21:34:45.477+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:34:45.478+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:34:41.945741+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:34:41.947304+00:00 [scheduled]>
[2026-02-23T21:34:45.481+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:34:41.945741+00:00 [scheduled]>, <TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:34:41.947304+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:34:45.482+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_DAG1', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:34:41.945741+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:34:45.483+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG1', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:34:41.945741+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:45.484+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_DAG2', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:34:41.947304+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:34:45.485+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG2', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:34:41.947304+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:45.487+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG1', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:34:41.945741+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:47.480+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:34:48.073+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:34:41.945741+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:34:49.126+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG2', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:34:41.947304+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:34:51.313+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:34:51.926+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:34:41.947304+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:34:52.958+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_DAG1', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:34:41.945741+00:00', try_number=1, map_index=-1)
[2026-02-23T21:34:52.960+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_DAG2', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:34:41.947304+00:00', try_number=1, map_index=-1)
[2026-02-23T21:34:52.968+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_DAG1, task_id=c_1p1n1n_TASK1, run_id=dataset_triggered__2026-02-23T13:34:41.945741+00:00, map_index=-1, run_start_date=2026-02-23 13:34:48.139521+00:00, run_end_date=2026-02-23 13:34:48.517613+00:00, run_duration=0.378092, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=197, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:34:45.479801+00:00, queued_by_job_id=164, pid=9353
[2026-02-23T21:34:52.969+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_DAG2, task_id=c_1p1n1n_TASK2, run_id=dataset_triggered__2026-02-23T13:34:41.947304+00:00, map_index=-1, run_start_date=2026-02-23 13:34:51.989866+00:00, run_end_date=2026-02-23 13:34:52.352972+00:00, run_duration=0.363106, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=198, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:34:45.479801+00:00, queued_by_job_id=164, pid=9355
[2026-02-23T21:34:55.659+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_DAG1 @ 2026-02-23 13:34:41.945741+00:00: dataset_triggered__2026-02-23T13:34:41.945741+00:00, state:running, queued_at: 2026-02-23 13:34:45.425948+00:00. externally triggered: False> successful
[2026-02-23T21:34:55.660+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_DAG1, execution_date=2026-02-23 13:34:41.945741+00:00, run_id=dataset_triggered__2026-02-23T13:34:41.945741+00:00, run_start_date=2026-02-23 13:34:45.447230+00:00, run_end_date=2026-02-23 13:34:55.660567+00:00, run_duration=10.213337, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 13:34:36.159302+00:00, dag_hash=c3144e87e37f9ff1a9f0e648fbb0f19d
[2026-02-23T21:34:55.665+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_DAG2 @ 2026-02-23 13:34:41.947304+00:00: dataset_triggered__2026-02-23T13:34:41.947304+00:00, state:running, queued_at: 2026-02-23 13:34:45.433527+00:00. externally triggered: False> successful
[2026-02-23T21:34:55.666+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_DAG2, execution_date=2026-02-23 13:34:41.947304+00:00, run_id=dataset_triggered__2026-02-23T13:34:41.947304+00:00, run_start_date=2026-02-23 13:34:45.447332+00:00, run_end_date=2026-02-23 13:34:55.666192+00:00, run_duration=10.21886, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 12:36:42.067898+00:00, data_interval_end=2026-02-23 13:34:36.159302+00:00, dag_hash=0a3439ce80be51d3a7ac788a7442a9b6
[2026-02-23T21:36:05.856+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:36:03.025632+00:00 [scheduled]>
[2026-02-23T21:36:05.857+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p1n1n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:36:05.858+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:36:03.025632+00:00 [scheduled]>
[2026-02-23T21:36:05.861+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:36:03.025632+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:36:05.862+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:36:03.025632+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:36:05.862+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:36:03.025632+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:05.865+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p1n1n_DAGs', 'p_1p1n1n_TASK', 'manual__2026-02-23T13:36:03.025632+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:08.067+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:36:08.721+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p1n1n_DAGs.p_1p1n1n_TASK manual__2026-02-23T13:36:03.025632+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:36:09.833+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p1n1n_DAGs', task_id='p_1p1n1n_TASK', run_id='manual__2026-02-23T13:36:03.025632+00:00', try_number=1, map_index=-1)
[2026-02-23T21:36:09.843+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p1n1n_DAGs, task_id=p_1p1n1n_TASK, run_id=manual__2026-02-23T13:36:03.025632+00:00, map_index=-1, run_start_date=2026-02-23 13:36:08.792358+00:00, run_end_date=2026-02-23 13:36:09.166914+00:00, run_duration=0.374556, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=199, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:36:05.859763+00:00, queued_by_job_id=164, pid=9417
[2026-02-23T21:36:12.487+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p1n1n_DAGs @ 2026-02-23 13:36:03.025632+00:00: manual__2026-02-23T13:36:03.025632+00:00, state:running, queued_at: 2026-02-23 13:36:03.038215+00:00. externally triggered: True> successful
[2026-02-23T21:36:12.488+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p1n1n_DAGs, execution_date=2026-02-23 13:36:03.025632+00:00, run_id=manual__2026-02-23T13:36:03.025632+00:00, run_start_date=2026-02-23 13:36:05.836577+00:00, run_end_date=2026-02-23 13:36:12.488322+00:00, run_duration=6.651745, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:36:03.025632+00:00, data_interval_end=2026-02-23 13:36:03.025632+00:00, dag_hash=27f14b92c1fec47bd0d9b691229d87b8
[2026-02-23T21:36:12.497+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:36:09.183970+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:36:09.186015+00:00 [scheduled]>
[2026-02-23T21:36:12.498+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:36:12.499+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p1n1n_DAG1 has 0/16 running and queued tasks
[2026-02-23T21:36:12.500+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:36:09.183970+00:00 [scheduled]>
	<TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:36:09.186015+00:00 [scheduled]>
[2026-02-23T21:36:12.503+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:36:09.183970+00:00 [scheduled]>, <TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:36:09.186015+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:36:12.504+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_DAG2', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:36:09.183970+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:36:12.504+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG2', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:36:09.183970+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:12.505+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p1n1n_DAG1', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:36:09.186015+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:36:12.506+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG1', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:36:09.186015+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:12.508+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG2', 'c_1p1n1n_TASK2', 'dataset_triggered__2026-02-23T13:36:09.183970+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:14.525+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:36:15.120+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_DAG2.c_1p1n1n_TASK2 dataset_triggered__2026-02-23T13:36:09.183970+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:36:16.264+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p1n1n_DAG1', 'c_1p1n1n_TASK1', 'dataset_triggered__2026-02-23T13:36:09.186015+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p1n1n.py']
[2026-02-23T21:36:18.522+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p1n1n.py
[2026-02-23T21:36:19.144+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p1n1n_DAG1.c_1p1n1n_TASK1 dataset_triggered__2026-02-23T13:36:09.186015+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:36:20.272+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_DAG2', task_id='c_1p1n1n_TASK2', run_id='dataset_triggered__2026-02-23T13:36:09.183970+00:00', try_number=1, map_index=-1)
[2026-02-23T21:36:20.275+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p1n1n_DAG1', task_id='c_1p1n1n_TASK1', run_id='dataset_triggered__2026-02-23T13:36:09.186015+00:00', try_number=1, map_index=-1)
[2026-02-23T21:36:20.285+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_DAG1, task_id=c_1p1n1n_TASK1, run_id=dataset_triggered__2026-02-23T13:36:09.186015+00:00, map_index=-1, run_start_date=2026-02-23 13:36:19.215000+00:00, run_end_date=2026-02-23 13:36:19.616450+00:00, run_duration=0.40145, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=201, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:36:12.501718+00:00, queued_by_job_id=164, pid=9422
[2026-02-23T21:36:20.286+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p1n1n_DAG2, task_id=c_1p1n1n_TASK2, run_id=dataset_triggered__2026-02-23T13:36:09.183970+00:00, map_index=-1, run_start_date=2026-02-23 13:36:15.190298+00:00, run_end_date=2026-02-23 13:36:15.588045+00:00, run_duration=0.397747, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=200, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:36:12.501718+00:00, queued_by_job_id=164, pid=9420
[2026-02-23T21:36:20.329+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:36:22.988+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_DAG2 @ 2026-02-23 13:36:09.183970+00:00: dataset_triggered__2026-02-23T13:36:09.183970+00:00, state:running, queued_at: 2026-02-23 13:36:12.448754+00:00. externally triggered: False> successful
[2026-02-23T21:36:22.989+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_DAG2, execution_date=2026-02-23 13:36:09.183970+00:00, run_id=dataset_triggered__2026-02-23T13:36:09.183970+00:00, run_start_date=2026-02-23 13:36:12.470753+00:00, run_end_date=2026-02-23 13:36:22.989775+00:00, run_duration=10.519022, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:36:03.025632+00:00, data_interval_end=2026-02-23 13:36:03.025632+00:00, dag_hash=0a3439ce80be51d3a7ac788a7442a9b6
[2026-02-23T21:36:22.994+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p1n1n_DAG1 @ 2026-02-23 13:36:09.186015+00:00: dataset_triggered__2026-02-23T13:36:09.186015+00:00, state:running, queued_at: 2026-02-23 13:36:12.457059+00:00. externally triggered: False> successful
[2026-02-23T21:36:22.995+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p1n1n_DAG1, execution_date=2026-02-23 13:36:09.186015+00:00, run_id=dataset_triggered__2026-02-23T13:36:09.186015+00:00, run_start_date=2026-02-23 13:36:12.470866+00:00, run_end_date=2026-02-23 13:36:22.995336+00:00, run_duration=10.52447, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:36:03.025632+00:00, data_interval_end=2026-02-23 13:36:03.025632+00:00, dag_hash=c3144e87e37f9ff1a9f0e648fbb0f19d
[2026-02-23T21:41:20.642+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:46:22.282+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:51:24.964+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T21:51:28.928+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:51:24.624654+00:00 [scheduled]>
[2026-02-23T21:51:28.928+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p2n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:51:28.929+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:51:24.624654+00:00 [scheduled]>
[2026-02-23T21:51:28.932+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:51:24.624654+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:51:28.933+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:51:24.624654+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:51:28.933+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:51:24.624654+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:51:28.936+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:51:24.624654+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:51:30.927+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:51:31.941+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:51:24.624654+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:51:33.176+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:51:24.624654+00:00', try_number=1, map_index=-1)
[2026-02-23T21:51:33.185+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p2n_DAGs, task_id=p_1p2n_TASK, run_id=manual__2026-02-23T13:51:24.624654+00:00, map_index=-1, run_start_date=2026-02-23 13:51:32.021032+00:00, run_end_date=2026-02-23 13:51:32.515704+00:00, run_duration=0.494672, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=202, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:51:28.931029+00:00, queued_by_job_id=164, pid=9847
[2026-02-23T21:51:36.009+0800] {dagbag.py:242} WARNING - Serialized DAG c_1p2n_DAG2 no longer exists
[2026-02-23T21:51:36.010+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p2n_DAG2' not found in serialized_dag table
[2026-02-23T21:51:36.045+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p2n_DAGs @ 2026-02-23 13:51:24.624654+00:00: manual__2026-02-23T13:51:24.624654+00:00, state:running, queued_at: 2026-02-23 13:51:24.638049+00:00. externally triggered: True> successful
[2026-02-23T21:51:36.046+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p2n_DAGs, execution_date=2026-02-23 13:51:24.624654+00:00, run_id=manual__2026-02-23T13:51:24.624654+00:00, run_start_date=2026-02-23 13:51:28.906710+00:00, run_end_date=2026-02-23 13:51:36.046923+00:00, run_duration=7.140213, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:51:24.624654+00:00, data_interval_end=2026-02-23 13:51:24.624654+00:00, dag_hash=bcc79dc6c0926a8d58a1f7ccd0ba8898
[2026-02-23T21:51:36.056+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:51:32.536269+00:00 [scheduled]>
[2026-02-23T21:51:36.057+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG1 has 0/16 running and queued tasks
[2026-02-23T21:51:36.058+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:51:32.536269+00:00 [scheduled]>
[2026-02-23T21:51:36.061+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:51:32.536269+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:51:36.061+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:51:32.536269+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:51:36.062+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:51:32.536269+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:51:36.065+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:51:32.536269+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:51:38.286+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:51:38.953+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:51:32.536269+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:51:39.977+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:51:32.536269+00:00', try_number=1, map_index=-1)
[2026-02-23T21:51:39.988+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG1, task_id=c_1p2n_TASK1, run_id=dataset_triggered__2026-02-23T13:51:32.536269+00:00, map_index=-1, run_start_date=2026-02-23 13:51:39.029429+00:00, run_end_date=2026-02-23 13:51:39.408262+00:00, run_duration=0.378833, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=203, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:51:36.059556+00:00, queued_by_job_id=164, pid=9850
[2026-02-23T21:51:42.693+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p2n_DAG2' not found in serialized_dag table
[2026-02-23T21:51:42.706+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG1 @ 2026-02-23 13:51:32.536269+00:00: dataset_triggered__2026-02-23T13:51:32.536269+00:00, state:running, queued_at: 2026-02-23 13:51:36.017109+00:00. externally triggered: False> successful
[2026-02-23T21:51:42.706+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG1, execution_date=2026-02-23 13:51:32.536269+00:00, run_id=dataset_triggered__2026-02-23T13:51:32.536269+00:00, run_start_date=2026-02-23 13:51:36.032735+00:00, run_end_date=2026-02-23 13:51:42.706896+00:00, run_duration=6.674161, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:51:24.624654+00:00, data_interval_end=2026-02-23 13:51:24.624654+00:00, dag_hash=4a31cc71e5398c53fad0c9cea1feb380
[2026-02-23T21:51:46.490+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p2n_DAG2' not found in serialized_dag table
[2026-02-23T21:51:50.419+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p2n_DAG2' not found in serialized_dag table
[2026-02-23T21:51:54.682+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'c_1p2n_DAG2' not found in serialized_dag table
[2026-02-23T21:51:55.021+0800] {manager.py:537} INFO - DAG c_1p2n_DAG2 is missing and will be deactivated.
[2026-02-23T21:51:55.025+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-23T21:51:55.029+0800] {manager.py:553} INFO - Deleted DAG c_1p2n_DAG2 in serialized_dag table
[2026-02-23T21:52:12.955+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:51:32.534137+00:00 [scheduled]>
[2026-02-23T21:52:12.956+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:52:12.957+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:51:32.534137+00:00 [scheduled]>
[2026-02-23T21:52:12.959+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:51:32.534137+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:52:12.960+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:51:32.534137+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:52:12.961+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:51:32.534137+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:12.964+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:51:32.534137+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:15.386+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:52:16.064+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:51:32.534137+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:52:17.275+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:51:32.534137+00:00', try_number=1, map_index=-1)
[2026-02-23T21:52:17.285+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG2, task_id=c_1p2n_TASK2, run_id=dataset_triggered__2026-02-23T13:51:32.534137+00:00, map_index=-1, run_start_date=2026-02-23 13:52:16.141349+00:00, run_end_date=2026-02-23 13:52:16.556638+00:00, run_duration=0.415289, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=204, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:52:12.958854+00:00, queued_by_job_id=164, pid=9884
[2026-02-23T21:52:20.376+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG2 @ 2026-02-23 13:51:32.534137+00:00: dataset_triggered__2026-02-23T13:51:32.534137+00:00, state:running, queued_at: 2026-02-23 13:52:12.923844+00:00. externally triggered: False> successful
[2026-02-23T21:52:20.377+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG2, execution_date=2026-02-23 13:51:32.534137+00:00, run_id=dataset_triggered__2026-02-23T13:51:32.534137+00:00, run_start_date=2026-02-23 13:52:12.936886+00:00, run_end_date=2026-02-23 13:52:20.377448+00:00, run_duration=7.440562, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:51:24.624654+00:00, data_interval_end=2026-02-23 13:51:24.624654+00:00, dag_hash=f0222c6725e6517a46dc479b0aa002a2
[2026-02-23T21:52:20.389+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:52:19.053228+00:00 [scheduled]>
[2026-02-23T21:52:20.389+0800] {scheduler_job_runner.py:507} INFO - DAG p_1p2n_DAGs has 0/16 running and queued tasks
[2026-02-23T21:52:20.390+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:52:19.053228+00:00 [scheduled]>
[2026-02-23T21:52:20.393+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:52:19.053228+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:52:20.394+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:52:19.053228+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:52:20.395+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:52:19.053228+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:20.398+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'p_1p2n_DAGs', 'p_1p2n_TASK', 'manual__2026-02-23T13:52:19.053228+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:22.520+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:52:23.113+0800] {task_command.py:467} INFO - Running <TaskInstance: p_1p2n_DAGs.p_1p2n_TASK manual__2026-02-23T13:52:19.053228+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:52:24.251+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='p_1p2n_DAGs', task_id='p_1p2n_TASK', run_id='manual__2026-02-23T13:52:19.053228+00:00', try_number=1, map_index=-1)
[2026-02-23T21:52:24.260+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=p_1p2n_DAGs, task_id=p_1p2n_TASK, run_id=manual__2026-02-23T13:52:19.053228+00:00, map_index=-1, run_start_date=2026-02-23 13:52:23.187193+00:00, run_end_date=2026-02-23 13:52:23.559310+00:00, run_duration=0.372117, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=205, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:52:20.391915+00:00, queued_by_job_id=164, pid=9887
[2026-02-23T21:52:26.841+0800] {dagrun.py:854} INFO - Marking run <DagRun p_1p2n_DAGs @ 2026-02-23 13:52:19.053228+00:00: manual__2026-02-23T13:52:19.053228+00:00, state:running, queued_at: 2026-02-23 13:52:19.065167+00:00. externally triggered: True> successful
[2026-02-23T21:52:26.843+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=p_1p2n_DAGs, execution_date=2026-02-23 13:52:19.053228+00:00, run_id=manual__2026-02-23T13:52:19.053228+00:00, run_start_date=2026-02-23 13:52:20.357736+00:00, run_end_date=2026-02-23 13:52:26.843131+00:00, run_duration=6.485395, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-23 13:52:19.053228+00:00, data_interval_end=2026-02-23 13:52:19.053228+00:00, dag_hash=bcc79dc6c0926a8d58a1f7ccd0ba8898
[2026-02-23T21:52:26.854+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:52:23.576229+00:00 [scheduled]>
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:52:23.577764+00:00 [scheduled]>
[2026-02-23T21:52:26.854+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG2 has 0/16 running and queued tasks
[2026-02-23T21:52:26.855+0800] {scheduler_job_runner.py:507} INFO - DAG c_1p2n_DAG1 has 0/16 running and queued tasks
[2026-02-23T21:52:26.856+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:52:23.576229+00:00 [scheduled]>
	<TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:52:23.577764+00:00 [scheduled]>
[2026-02-23T21:52:26.859+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:52:23.576229+00:00 [scheduled]>, <TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:52:23.577764+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-23T21:52:26.860+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:52:23.576229+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:52:26.861+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:52:23.576229+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:26.862+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:52:23.577764+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-23T21:52:26.862+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:52:23.577764+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:26.865+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG2', 'c_1p2n_TASK2', 'dataset_triggered__2026-02-23T13:52:23.576229+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:28.986+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:52:29.582+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG2.c_1p2n_TASK2 dataset_triggered__2026-02-23T13:52:23.576229+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:52:30.683+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'c_1p2n_DAG1', 'c_1p2n_TASK1', 'dataset_triggered__2026-02-23T13:52:23.577764+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1p2n.py']
[2026-02-23T21:52:32.803+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1p2n.py
[2026-02-23T21:52:33.470+0800] {task_command.py:467} INFO - Running <TaskInstance: c_1p2n_DAG1.c_1p2n_TASK1 dataset_triggered__2026-02-23T13:52:23.577764+00:00 [queued]> on host localhost-2.local
[2026-02-23T21:52:34.755+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG2', task_id='c_1p2n_TASK2', run_id='dataset_triggered__2026-02-23T13:52:23.576229+00:00', try_number=1, map_index=-1)
[2026-02-23T21:52:34.759+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='c_1p2n_DAG1', task_id='c_1p2n_TASK1', run_id='dataset_triggered__2026-02-23T13:52:23.577764+00:00', try_number=1, map_index=-1)
[2026-02-23T21:52:34.770+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG2, task_id=c_1p2n_TASK2, run_id=dataset_triggered__2026-02-23T13:52:23.576229+00:00, map_index=-1, run_start_date=2026-02-23 13:52:29.652625+00:00, run_end_date=2026-02-23 13:52:30.028010+00:00, run_duration=0.375385, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=206, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:52:26.857434+00:00, queued_by_job_id=164, pid=9890
[2026-02-23T21:52:34.771+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=c_1p2n_DAG1, task_id=c_1p2n_TASK1, run_id=dataset_triggered__2026-02-23T13:52:23.577764+00:00, map_index=-1, run_start_date=2026-02-23 13:52:33.543342+00:00, run_end_date=2026-02-23 13:52:33.938541+00:00, run_duration=0.395199, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=207, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-23 13:52:26.857434+00:00, queued_by_job_id=164, pid=9892
[2026-02-23T21:52:38.129+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG2 @ 2026-02-23 13:52:23.576229+00:00: dataset_triggered__2026-02-23T13:52:23.576229+00:00, state:running, queued_at: 2026-02-23 13:52:26.800079+00:00. externally triggered: False> successful
[2026-02-23T21:52:38.130+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG2, execution_date=2026-02-23 13:52:23.576229+00:00, run_id=dataset_triggered__2026-02-23T13:52:23.576229+00:00, run_start_date=2026-02-23 13:52:26.823591+00:00, run_end_date=2026-02-23 13:52:38.130142+00:00, run_duration=11.306551, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:52:19.053228+00:00, data_interval_end=2026-02-23 13:52:19.053228+00:00, dag_hash=f0222c6725e6517a46dc479b0aa002a2
[2026-02-23T21:52:38.135+0800] {dagrun.py:854} INFO - Marking run <DagRun c_1p2n_DAG1 @ 2026-02-23 13:52:23.577764+00:00: dataset_triggered__2026-02-23T13:52:23.577764+00:00, state:running, queued_at: 2026-02-23 13:52:26.808496+00:00. externally triggered: False> successful
[2026-02-23T21:52:38.136+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=c_1p2n_DAG1, execution_date=2026-02-23 13:52:23.577764+00:00, run_id=dataset_triggered__2026-02-23T13:52:23.577764+00:00, run_start_date=2026-02-23 13:52:26.823719+00:00, run_end_date=2026-02-23 13:52:38.136523+00:00, run_duration=11.312804, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-23 13:52:19.053228+00:00, data_interval_end=2026-02-23 13:52:19.053228+00:00, dag_hash=4a31cc71e5398c53fad0c9cea1feb380
[2026-02-23T21:56:25.208+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T22:01:25.425+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T22:06:25.633+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T22:11:26.742+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-23T22:17:00.314+0800] {job.py:229} INFO - Heartbeat recovered after 249.18 seconds
[2026-02-23T22:20:30.502+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
