[2026-02-13T19:50:57.816+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-13T19:50:58.423+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-13T19:50:58.424+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-13T19:50:58.430+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 1967
[2026-02-13T19:50:58.434+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T19:51:00.774+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-13T19:51:00.826+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-13T19:51:01.134+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-13T19:55:58.497+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:00:58.534+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:05:43.697+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>
[2026-02-13T20:05:43.698+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T20:05:43.699+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>
[2026-02-13T20:05:43.702+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:05:43.703+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:05:42.321399+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:05:43.704+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:05:42.321399+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:05:43.706+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:05:42.321399+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:05:45.857+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:05:46.607+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:05:47.540+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:05:42.321399+00:00', try_number=1, map_index=-1)
[2026-02-13T20:05:47.554+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:05:42.321399+00:00, map_index=-1, run_start_date=2026-02-13 12:05:46.682805+00:00, run_end_date=2026-02-13 12:05:46.990071+00:00, run_duration=0.307266, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=59, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:05:43.701172+00:00, queued_by_job_id=58, pid=3698
[2026-02-13T20:05:50.213+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:05:42.321399+00:00: manual__2026-02-13T12:05:42.321399+00:00, state:running, queued_at: 2026-02-13 12:05:42.344376+00:00. externally triggered: True> successful
[2026-02-13T20:05:50.215+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:05:42.321399+00:00, run_id=manual__2026-02-13T12:05:42.321399+00:00, run_start_date=2026-02-13 12:05:43.663188+00:00, run_end_date=2026-02-13 12:05:50.215457+00:00, run_duration=6.552269, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:05:42.321399+00:00, data_interval_end=2026-02-13 12:05:42.321399+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T20:05:50.227+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>
[2026-02-13T20:05:50.228+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T20:05:50.228+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>
[2026-02-13T20:05:50.231+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:05:50.231+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:05:47.008811+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:05:50.232+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:05:47.008811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:05:50.235+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:05:47.008811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:05:52.346+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:05:52.975+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:05:53.881+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:05:47.008811+00:00', try_number=1, map_index=-1)
[2026-02-13T20:05:53.890+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:05:47.008811+00:00, map_index=-1, run_start_date=2026-02-13 12:05:53.046067+00:00, run_end_date=2026-02-13 12:05:53.334332+00:00, run_duration=0.288265, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=60, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:05:50.229712+00:00, queued_by_job_id=58, pid=3703
[2026-02-13T20:05:56.508+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:05:47.008811+00:00: dataset_triggered__2026-02-13T12:05:47.008811+00:00, state:running, queued_at: 2026-02-13 12:05:50.177063+00:00. externally triggered: False> successful
[2026-02-13T20:05:56.510+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:05:47.008811+00:00, run_id=dataset_triggered__2026-02-13T12:05:47.008811+00:00, run_start_date=2026-02-13 12:05:50.200171+00:00, run_end_date=2026-02-13 12:05:56.510050+00:00, run_duration=6.309879, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:05:42.321399+00:00, data_interval_end=2026-02-13 12:05:42.321399+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
[2026-02-13T20:05:58.580+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:09:33.541+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>
[2026-02-13T20:09:33.542+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T20:09:33.543+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>
[2026-02-13T20:09:33.545+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:09:33.547+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:09:29.883245+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:09:33.547+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:09:29.883245+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:09:33.550+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:09:29.883245+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:09:35.544+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:09:36.285+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:09:37.223+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:09:29.883245+00:00', try_number=1, map_index=-1)
[2026-02-13T20:09:37.235+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:09:29.883245+00:00, map_index=-1, run_start_date=2026-02-13 12:09:36.390309+00:00, run_end_date=2026-02-13 12:09:36.700380+00:00, run_duration=0.310071, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:09:33.544335+00:00, queued_by_job_id=58, pid=4043
[2026-02-13T20:09:40.006+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:09:29.883245+00:00: manual__2026-02-13T12:09:29.883245+00:00, state:running, queued_at: 2026-02-13 12:09:29.901006+00:00. externally triggered: True> successful
[2026-02-13T20:09:40.007+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:09:29.883245+00:00, run_id=manual__2026-02-13T12:09:29.883245+00:00, run_start_date=2026-02-13 12:09:33.517270+00:00, run_end_date=2026-02-13 12:09:40.007568+00:00, run_duration=6.490298, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:09:29.883245+00:00, data_interval_end=2026-02-13 12:09:29.883245+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T20:09:40.017+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>
[2026-02-13T20:09:40.018+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T20:09:40.019+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>
[2026-02-13T20:09:40.021+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:09:40.022+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:09:36.719651+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:09:40.023+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:09:36.719651+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:09:40.025+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:09:36.719651+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:09:41.997+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:09:42.654+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:09:43.612+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:09:36.719651+00:00', try_number=1, map_index=-1)
[2026-02-13T20:09:43.623+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:09:36.719651+00:00, map_index=-1, run_start_date=2026-02-13 12:09:42.727518+00:00, run_end_date=2026-02-13 12:09:43.057478+00:00, run_duration=0.32996, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:09:40.020429+00:00, queued_by_job_id=58, pid=4046
[2026-02-13T20:09:43.675+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:09:36.719651+00:00: dataset_triggered__2026-02-13T12:09:36.719651+00:00, state:running, queued_at: 2026-02-13 12:09:39.972173+00:00. externally triggered: False> successful
[2026-02-13T20:09:43.676+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:09:36.719651+00:00, run_id=dataset_triggered__2026-02-13T12:09:36.719651+00:00, run_start_date=2026-02-13 12:09:39.990831+00:00, run_end_date=2026-02-13 12:09:43.676394+00:00, run_duration=3.685563, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:09:29.883245+00:00, data_interval_end=2026-02-13 12:09:29.883245+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
[2026-02-13T20:10:58.656+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:13:51.739+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>
[2026-02-13T20:13:51.741+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T20:13:51.742+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>
[2026-02-13T20:13:51.745+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:13:51.746+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:13:50.192029+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:13:51.747+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:13:50.192029+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:13:51.751+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:13:50.192029+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:13:54.182+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:13:54.986+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:13:55.931+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:13:50.192029+00:00', try_number=1, map_index=-1)
[2026-02-13T20:13:55.940+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:13:50.192029+00:00, map_index=-1, run_start_date=2026-02-13 12:13:55.058956+00:00, run_end_date=2026-02-13 12:13:55.377104+00:00, run_duration=0.318148, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=63, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:51.743848+00:00, queued_by_job_id=58, pid=4129
[2026-02-13T20:13:58.602+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:13:50.192029+00:00: manual__2026-02-13T12:13:50.192029+00:00, state:running, queued_at: 2026-02-13 12:13:50.210479+00:00. externally triggered: True> successful
[2026-02-13T20:13:58.603+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:13:50.192029+00:00, run_id=manual__2026-02-13T12:13:50.192029+00:00, run_start_date=2026-02-13 12:13:51.717860+00:00, run_end_date=2026-02-13 12:13:58.603159+00:00, run_duration=6.885299, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:13:50.192029+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T20:13:58.614+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>
[2026-02-13T20:13:58.615+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T20:13:58.615+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_stat has 0/16 running and queued tasks
[2026-02-13T20:13:58.616+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>
[2026-02-13T20:13:58.620+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>, <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:13:58.621+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.394014+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:13:58.622+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:13:55.394014+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:13:58.623+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.395586+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:13:58.624+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:13:55.395586+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:13:58.626+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:13:55.394014+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:14:00.526+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:14:01.100+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:14:02.003+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:13:55.395586+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:14:04.030+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:14:04.651+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:14:05.778+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.394014+00:00', try_number=1, map_index=-1)
[2026-02-13T20:14:05.781+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.395586+00:00', try_number=1, map_index=-1)
[2026-02-13T20:14:05.792+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T12:13:55.395586+00:00, map_index=-1, run_start_date=2026-02-13 12:14:04.714562+00:00, run_end_date=2026-02-13 12:14:05.087446+00:00, run_duration=0.372884, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:58.618017+00:00, queued_by_job_id=58, pid=4137
[2026-02-13T20:14:05.794+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:13:55.394014+00:00, map_index=-1, run_start_date=2026-02-13 12:14:01.165983+00:00, run_end_date=2026-02-13 12:14:01.461394+00:00, run_duration=0.295411, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=64, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:58.618017+00:00, queued_by_job_id=58, pid=4135
[2026-02-13T20:14:08.181+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:13:55.394014+00:00: dataset_triggered__2026-02-13T12:13:55.394014+00:00, state:running, queued_at: 2026-02-13 12:13:58.566381+00:00. externally triggered: False> successful
[2026-02-13T20:14:08.182+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:13:55.394014+00:00, run_id=dataset_triggered__2026-02-13T12:13:55.394014+00:00, run_start_date=2026-02-13 12:13:58.580294+00:00, run_end_date=2026-02-13 12:14:08.182001+00:00, run_duration=9.601707, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:13:50.192029+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=812fdc4eaaef0b4b0ecbeb4a4a10f6ba
[2026-02-13T20:14:08.186+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_stat @ 2026-02-13 12:13:55.395586+00:00: dataset_triggered__2026-02-13T12:13:55.395586+00:00, state:running, queued_at: 2026-02-13 12:13:58.543810+00:00. externally triggered: False> successful
[2026-02-13T20:14:08.187+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_stat, execution_date=2026-02-13 12:13:55.395586+00:00, run_id=dataset_triggered__2026-02-13T12:13:55.395586+00:00, run_start_date=2026-02-13 12:13:58.580476+00:00, run_end_date=2026-02-13 12:14:08.187176+00:00, run_duration=9.6067, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=c48b04123af906598e7caee12eaa4367
[2026-02-13T20:15:58.690+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:20:58.733+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:21:51.040+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>
[2026-02-13T20:21:51.041+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T20:21:51.042+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>
[2026-02-13T20:21:51.044+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:21:51.046+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:21:48.664790+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:21:51.046+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:21:48.664790+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:21:51.049+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:21:48.664790+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:21:52.943+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:21:53.906+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:21:54.887+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:21:48.664790+00:00', try_number=1, map_index=-1)
[2026-02-13T20:21:54.896+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:21:48.664790+00:00, map_index=-1, run_start_date=2026-02-13 12:21:53.977335+00:00, run_end_date=2026-02-13 12:21:54.269269+00:00, run_duration=0.291934, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:51.043245+00:00, queued_by_job_id=58, pid=4284
[2026-02-13T20:21:54.978+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:21:48.664790+00:00: manual__2026-02-13T12:21:48.664790+00:00, state:running, queued_at: 2026-02-13 12:21:48.677286+00:00. externally triggered: True> successful
[2026-02-13T20:21:54.979+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:21:48.664790+00:00, run_id=manual__2026-02-13T12:21:48.664790+00:00, run_start_date=2026-02-13 12:21:51.020627+00:00, run_end_date=2026-02-13 12:21:54.979822+00:00, run_duration=3.959195, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T20:21:56.044+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>
[2026-02-13T20:21:56.045+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T20:21:56.046+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_stat has 0/16 running and queued tasks
[2026-02-13T20:21:56.046+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>
[2026-02-13T20:21:56.049+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>, <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T20:21:56.051+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.286709+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:21:56.051+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:21:54.286709+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:21:56.052+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.288058+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T20:21:56.053+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:21:54.288058+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:21:56.056+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:21:54.286709+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:21:58.089+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:21:58.664+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:21:59.594+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:21:54.288058+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T20:22:01.627+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T20:22:02.313+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [queued]> on host localhost-2.local
[2026-02-13T20:22:03.258+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.286709+00:00', try_number=1, map_index=-1)
[2026-02-13T20:22:03.261+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.288058+00:00', try_number=1, map_index=-1)
[2026-02-13T20:22:03.270+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T12:21:54.288058+00:00, map_index=-1, run_start_date=2026-02-13 12:22:02.385933+00:00, run_end_date=2026-02-13 12:22:02.704072+00:00, run_duration=0.318139, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:56.047787+00:00, queued_by_job_id=58, pid=4291
[2026-02-13T20:22:03.271+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:21:54.286709+00:00, map_index=-1, run_start_date=2026-02-13 12:21:58.731908+00:00, run_end_date=2026-02-13 12:21:59.040815+00:00, run_duration=0.308907, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:56.047787+00:00, queued_by_job_id=58, pid=4286
[2026-02-13T20:22:03.319+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:21:54.286709+00:00: dataset_triggered__2026-02-13T12:21:54.286709+00:00, state:running, queued_at: 2026-02-13 12:21:54.960377+00:00. externally triggered: False> successful
[2026-02-13T20:22:03.320+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:21:54.286709+00:00, run_id=dataset_triggered__2026-02-13T12:21:54.286709+00:00, run_start_date=2026-02-13 12:21:56.016376+00:00, run_end_date=2026-02-13 12:22:03.320464+00:00, run_duration=7.304088, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=812fdc4eaaef0b4b0ecbeb4a4a10f6ba
[2026-02-13T20:22:03.326+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_stat @ 2026-02-13 12:21:54.288058+00:00: dataset_triggered__2026-02-13T12:21:54.288058+00:00, state:running, queued_at: 2026-02-13 12:21:54.951524+00:00. externally triggered: False> successful
[2026-02-13T20:22:03.326+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_stat, execution_date=2026-02-13 12:21:54.288058+00:00, run_id=dataset_triggered__2026-02-13T12:21:54.288058+00:00, run_start_date=2026-02-13 12:21:56.016534+00:00, run_end_date=2026-02-13 12:22:03.326873+00:00, run_duration=7.310339, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=c48b04123af906598e7caee12eaa4367
[2026-02-13T20:25:58.762+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:30:59.716+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:35:47.170+0800] {job.py:229} INFO - Heartbeat recovered after 52.16 seconds
[2026-02-13T20:36:43.312+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T20:46:08.120+0800] {job.py:229} INFO - Heartbeat recovered after 566.01 seconds
[2026-02-13T20:56:12.320+0800] {job.py:229} INFO - Heartbeat recovered after 564.29 seconds
[2026-02-13T21:03:46.056+0800] {job.py:229} INFO - Heartbeat recovered after 412.95 seconds
[2026-02-13T21:07:02.437+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:12:02.479+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:17:02.520+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:18:13.620+0800] {manager.py:537} INFO - DAG consumer_variable_clean is missing and will be deactivated.
[2026-02-13T21:18:13.628+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-13T21:18:13.632+0800] {manager.py:553} INFO - Deleted DAG consumer_variable_clean in serialized_dag table
[2026-02-13T21:19:19.678+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>
[2026-02-13T21:19:19.679+0800] {scheduler_job_runner.py:507} INFO - DAG producer_variable_sync has 0/16 running and queued tasks
[2026-02-13T21:19:19.680+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>
[2026-02-13T21:19:19.683+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:19:19.684+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='sync_order_data', run_id='manual__2026-02-13T13:19:17.469369+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:19:19.685+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'sync_order_data', 'manual__2026-02-13T13:19:17.469369+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:19:19.687+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'sync_order_data', 'manual__2026-02-13T13:19:17.469369+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:19:21.677+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:19:22.427+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:19:23.528+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='sync_order_data', run_id='manual__2026-02-13T13:19:17.469369+00:00', try_number=1, map_index=-1)
[2026-02-13T21:19:23.537+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=sync_order_data, run_id=manual__2026-02-13T13:19:17.469369+00:00, map_index=-1, run_start_date=2026-02-13 13:19:22.495248+00:00, run_end_date=2026-02-13 13:19:22.877515+00:00, run_duration=0.382267, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:19:19.681950+00:00, queued_by_job_id=58, pid=5226
[2026-02-13T21:19:25.891+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:19:17.469369+00:00: manual__2026-02-13T13:19:17.469369+00:00, state:running, queued_at: 2026-02-13 13:19:17.493292+00:00. externally triggered: True> successful
[2026-02-13T21:19:25.892+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:19:17.469369+00:00, run_id=manual__2026-02-13T13:19:17.469369+00:00, run_start_date=2026-02-13 13:19:19.656584+00:00, run_end_date=2026-02-13 13:19:25.892123+00:00, run_duration=6.235539, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:19:17.469369+00:00, data_interval_end=2026-02-13 13:19:17.469369+00:00, dag_hash=2a5b3cdcb15949c1bf7f46b81e786a9a
[2026-02-13T21:19:25.902+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>
[2026-02-13T21:19:25.903+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_variable has 0/16 running and queued tasks
[2026-02-13T21:19:25.903+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>
[2026-02-13T21:19:25.905+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:19:25.906+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T13:19:22.894249+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:19:25.907+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'clean_order_data', 'dataset_triggered__2026-02-13T13:19:22.894249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:19:25.910+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'clean_order_data', 'dataset_triggered__2026-02-13T13:19:22.894249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:19:27.822+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:19:28.361+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:19:29.339+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T13:19:22.894249+00:00', try_number=1, map_index=-1)
[2026-02-13T21:19:29.346+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T13:19:22.894249+00:00, map_index=-1, run_start_date=2026-02-13 13:19:28.423596+00:00, run_end_date=2026-02-13 13:19:28.783442+00:00, run_duration=0.359846, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:19:25.904698+00:00, queued_by_job_id=58, pid=5229
[2026-02-13T21:19:31.724+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:19:22.894249+00:00: dataset_triggered__2026-02-13T13:19:22.894249+00:00, state:running, queued_at: 2026-02-13 13:19:25.861534+00:00. externally triggered: False> successful
[2026-02-13T21:19:31.725+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:19:22.894249+00:00, run_id=dataset_triggered__2026-02-13T13:19:22.894249+00:00, run_start_date=2026-02-13 13:19:25.879140+00:00, run_end_date=2026-02-13 13:19:31.725056+00:00, run_duration=5.845916, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 13:19:17.469369+00:00, dag_hash=dc521a83a3ec097ba48717ae4aff0641
[2026-02-13T21:22:04.558+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:27:05.895+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:28:31.067+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>
[2026-02-13T21:28:31.068+0800] {scheduler_job_runner.py:507} INFO - DAG producer_variable_sync has 0/16 running and queued tasks
[2026-02-13T21:28:31.070+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>
[2026-02-13T21:28:31.072+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:28:31.073+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:28:30.405028+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:28:31.074+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:28:30.405028+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:28:31.076+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:28:30.405028+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:28:33.190+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:28:34.182+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:28:35.156+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:28:30.405028+00:00', try_number=1, map_index=-1)
[2026-02-13T21:28:35.166+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=producer_variable_save_data, run_id=manual__2026-02-13T13:28:30.405028+00:00, map_index=-1, run_start_date=2026-02-13 13:28:34.258092+00:00, run_end_date=2026-02-13 13:28:34.611299+00:00, run_duration=0.353207, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:28:31.071275+00:00, queued_by_job_id=58, pid=5475
[2026-02-13T21:28:37.689+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:28:30.405028+00:00: manual__2026-02-13T13:28:30.405028+00:00, state:running, queued_at: 2026-02-13 13:28:30.415552+00:00. externally triggered: True> successful
[2026-02-13T21:28:37.690+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:28:30.405028+00:00, run_id=manual__2026-02-13T13:28:30.405028+00:00, run_start_date=2026-02-13 13:28:31.048918+00:00, run_end_date=2026-02-13 13:28:37.690420+00:00, run_duration=6.641502, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:28:30.405028+00:00, data_interval_end=2026-02-13 13:28:30.405028+00:00, dag_hash=47cb4504eff6f0d33ccb9eb1ea3c16ae
[2026-02-13T21:28:37.700+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>
[2026-02-13T21:28:37.701+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_variable has 0/16 running and queued tasks
[2026-02-13T21:28:37.702+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>
[2026-02-13T21:28:37.704+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:28:37.705+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:28:34.631014+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:28:37.706+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:28:34.631014+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:28:37.708+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:28:34.631014+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:28:39.725+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:28:40.291+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:28:41.345+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:28:34.631014+00:00', try_number=1, map_index=-1)
[2026-02-13T21:28:41.354+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=consumer_variable_read_data, run_id=dataset_triggered__2026-02-13T13:28:34.631014+00:00, map_index=-1, run_start_date=2026-02-13 13:28:40.356836+00:00, run_end_date=2026-02-13 13:28:40.752507+00:00, run_duration=0.395671, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=72, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:28:37.702957+00:00, queued_by_job_id=58, pid=5478
[2026-02-13T21:28:43.942+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:28:34.631014+00:00: dataset_triggered__2026-02-13T13:28:34.631014+00:00, state:running, queued_at: 2026-02-13 13:28:37.659479+00:00. externally triggered: False> successful
[2026-02-13T21:28:43.943+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:28:34.631014+00:00, run_id=dataset_triggered__2026-02-13T13:28:34.631014+00:00, run_start_date=2026-02-13 13:28:37.675820+00:00, run_end_date=2026-02-13 13:28:43.943383+00:00, run_duration=6.267563, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:28:30.405028+00:00, data_interval_end=2026-02-13 13:28:30.405028+00:00, dag_hash=ea6f50333c860200cf7319bc15b25ef1
[2026-02-13T21:31:43.552+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>
[2026-02-13T21:31:43.553+0800] {scheduler_job_runner.py:507} INFO - DAG producer_variable_sync has 0/16 running and queued tasks
[2026-02-13T21:31:43.553+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>
[2026-02-13T21:31:43.556+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:31:43.557+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:31:41.629277+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:31:43.558+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:31:41.629277+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:31:43.560+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:31:41.629277+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:31:45.767+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:31:46.547+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:31:47.575+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:31:41.629277+00:00', try_number=1, map_index=-1)
[2026-02-13T21:31:47.585+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=producer_variable_save_data, run_id=manual__2026-02-13T13:31:41.629277+00:00, map_index=-1, run_start_date=2026-02-13 13:31:46.632761+00:00, run_end_date=2026-02-13 13:31:47.023060+00:00, run_duration=0.390299, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:31:43.554757+00:00, queued_by_job_id=58, pid=5562
[2026-02-13T21:31:47.689+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:31:41.629277+00:00: manual__2026-02-13T13:31:41.629277+00:00, state:running, queued_at: 2026-02-13 13:31:41.647968+00:00. externally triggered: True> successful
[2026-02-13T21:31:47.690+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:31:41.629277+00:00, run_id=manual__2026-02-13T13:31:41.629277+00:00, run_start_date=2026-02-13 13:31:43.532261+00:00, run_end_date=2026-02-13 13:31:47.690795+00:00, run_duration=4.158534, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:31:41.629277+00:00, data_interval_end=2026-02-13 13:31:41.629277+00:00, dag_hash=47cb4504eff6f0d33ccb9eb1ea3c16ae
[2026-02-13T21:31:48.750+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>
[2026-02-13T21:31:48.751+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_variable has 0/16 running and queued tasks
[2026-02-13T21:31:48.752+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>
[2026-02-13T21:31:48.755+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:31:48.756+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:31:47.043088+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:31:48.757+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:31:47.043088+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:31:48.759+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:31:47.043088+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
[2026-02-13T21:31:50.818+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_variable.py
[2026-02-13T21:31:51.382+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:31:52.346+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:31:47.043088+00:00', try_number=1, map_index=-1)
[2026-02-13T21:31:52.356+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=consumer_variable_read_data, run_id=dataset_triggered__2026-02-13T13:31:47.043088+00:00, map_index=-1, run_start_date=2026-02-13 13:31:51.450539+00:00, run_end_date=2026-02-13 13:31:51.793322+00:00, run_duration=0.342783, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:31:48.753858+00:00, queued_by_job_id=58, pid=5565
[2026-02-13T21:31:52.397+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:31:47.043088+00:00: dataset_triggered__2026-02-13T13:31:47.043088+00:00, state:running, queued_at: 2026-02-13 13:31:47.668267+00:00. externally triggered: False> successful
[2026-02-13T21:31:52.398+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:31:47.043088+00:00, run_id=dataset_triggered__2026-02-13T13:31:47.043088+00:00, run_start_date=2026-02-13 13:31:48.728389+00:00, run_end_date=2026-02-13 13:31:52.398312+00:00, run_duration=3.669923, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:31:41.629277+00:00, data_interval_end=2026-02-13 13:31:41.629277+00:00, dag_hash=ea6f50333c860200cf7319bc15b25ef1
[2026-02-13T21:32:05.939+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:37:07.679+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:39:02.998+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>
[2026-02-13T21:39:02.999+0800] {scheduler_job_runner.py:507} INFO - DAG producer_object_sync has 0/16 running and queued tasks
[2026-02-13T21:39:03.000+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>
[2026-02-13T21:39:03.002+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:39:03.003+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:38:59.836012+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:39:03.005+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:38:59.836012+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:39:03.007+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:38:59.836012+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:39:05.145+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_object.py
[2026-02-13T21:39:05.873+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:39:06.811+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:38:59.836012+00:00', try_number=1, map_index=-1)
[2026-02-13T21:39:06.821+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:38:59.836012+00:00, map_index=-1, run_start_date=2026-02-13 13:39:05.940543+00:00, run_end_date=2026-02-13 13:39:06.286740+00:00, run_duration=0.346197, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:39:03.001168+00:00, queued_by_job_id=58, pid=5812
[2026-02-13T21:39:09.593+0800] {dagrun.py:823} ERROR - Marking run <DagRun producer_object_sync @ 2026-02-13 13:38:59.836012+00:00: manual__2026-02-13T13:38:59.836012+00:00, state:running, queued_at: 2026-02-13 13:38:59.851509+00:00. externally triggered: True> failed
[2026-02-13T21:39:09.594+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:38:59.836012+00:00, run_id=manual__2026-02-13T13:38:59.836012+00:00, run_start_date=2026-02-13 13:39:02.978128+00:00, run_end_date=2026-02-13 13:39:09.594334+00:00, run_duration=6.616206, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:38:59.836012+00:00, data_interval_end=2026-02-13 13:38:59.836012+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
[2026-02-13T21:42:07.722+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:42:54.108+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>
[2026-02-13T21:42:54.110+0800] {scheduler_job_runner.py:507} INFO - DAG producer_object_sync has 0/16 running and queued tasks
[2026-02-13T21:42:54.111+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>
[2026-02-13T21:42:54.115+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:42:54.116+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:42:53.973560+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:42:54.117+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:42:53.973560+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:42:54.120+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:42:53.973560+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:42:56.321+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_object.py
[2026-02-13T21:42:57.089+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:42:58.061+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:42:53.973560+00:00', try_number=1, map_index=-1)
[2026-02-13T21:42:58.072+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:42:53.973560+00:00, map_index=-1, run_start_date=2026-02-13 13:42:57.153377+00:00, run_end_date=2026-02-13 13:42:57.481345+00:00, run_duration=0.327968, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:42:54.113360+00:00, queued_by_job_id=58, pid=5890
[2026-02-13T21:42:58.121+0800] {dagrun.py:823} ERROR - Marking run <DagRun producer_object_sync @ 2026-02-13 13:42:53.973560+00:00: manual__2026-02-13T13:42:53.973560+00:00, state:running, queued_at: 2026-02-13 13:42:53.990856+00:00. externally triggered: True> failed
[2026-02-13T21:42:58.122+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:42:53.973560+00:00, run_id=manual__2026-02-13T13:42:53.973560+00:00, run_start_date=2026-02-13 13:42:54.085628+00:00, run_end_date=2026-02-13 13:42:58.122290+00:00, run_duration=4.036662, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:42:53.973560+00:00, data_interval_end=2026-02-13 13:42:53.973560+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
[2026-02-13T21:43:38.119+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>
[2026-02-13T21:43:38.119+0800] {scheduler_job_runner.py:507} INFO - DAG producer_object_sync has 0/16 running and queued tasks
[2026-02-13T21:43:38.120+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>
[2026-02-13T21:43:38.122+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:43:38.123+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:43:37.197651+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:43:38.124+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:43:37.197651+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:43:38.127+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:43:37.197651+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:43:40.164+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_object.py
[2026-02-13T21:43:40.780+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:43:41.738+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:43:37.197651+00:00', try_number=1, map_index=-1)
[2026-02-13T21:43:41.753+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:43:37.197651+00:00, map_index=-1, run_start_date=2026-02-13 13:43:40.845454+00:00, run_end_date=2026-02-13 13:43:41.203293+00:00, run_duration=0.357839, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:43:38.121725+00:00, queued_by_job_id=58, pid=5930
[2026-02-13T21:43:41.819+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_object_sync @ 2026-02-13 13:43:37.197651+00:00: manual__2026-02-13T13:43:37.197651+00:00, state:running, queued_at: 2026-02-13 13:43:37.209001+00:00. externally triggered: True> successful
[2026-02-13T21:43:41.820+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:43:37.197651+00:00, run_id=manual__2026-02-13T13:43:37.197651+00:00, run_start_date=2026-02-13 13:43:38.088254+00:00, run_end_date=2026-02-13 13:43:41.820298+00:00, run_duration=3.732044, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:43:37.197651+00:00, data_interval_end=2026-02-13 13:43:37.197651+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
[2026-02-13T21:43:45.429+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>
[2026-02-13T21:43:45.430+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_object has 0/16 running and queued tasks
[2026-02-13T21:43:45.431+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>
[2026-02-13T21:43:45.433+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:43:45.434+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_object', task_id='consumer_object_read_data', run_id='dataset_triggered__2026-02-13T13:43:41.221425+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:43:45.434+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_object', 'consumer_object_read_data', 'dataset_triggered__2026-02-13T13:43:41.221425+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:43:45.437+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_object', 'consumer_object_read_data', 'dataset_triggered__2026-02-13T13:43:41.221425+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
[2026-02-13T21:43:47.370+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_object.py
[2026-02-13T21:43:47.934+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:43:48.831+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_object', task_id='consumer_object_read_data', run_id='dataset_triggered__2026-02-13T13:43:41.221425+00:00', try_number=1, map_index=-1)
[2026-02-13T21:43:48.841+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_object, task_id=consumer_object_read_data, run_id=dataset_triggered__2026-02-13T13:43:41.221425+00:00, map_index=-1, run_start_date=2026-02-13 13:43:47.997174+00:00, run_end_date=2026-02-13 13:43:48.343217+00:00, run_duration=0.346043, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:43:45.431998+00:00, queued_by_job_id=58, pid=5934
[2026-02-13T21:43:51.539+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_object @ 2026-02-13 13:43:41.221425+00:00: dataset_triggered__2026-02-13T13:43:41.221425+00:00, state:running, queued_at: 2026-02-13 13:43:41.800292+00:00. externally triggered: False> successful
[2026-02-13T21:43:51.540+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_object, execution_date=2026-02-13 13:43:41.221425+00:00, run_id=dataset_triggered__2026-02-13T13:43:41.221425+00:00, run_start_date=2026-02-13 13:43:45.407571+00:00, run_end_date=2026-02-13 13:43:51.540699+00:00, run_duration=6.133128, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:43:37.197651+00:00, data_interval_end=2026-02-13 13:43:37.197651+00:00, dag_hash=9b4f52719dbf1e041ad0cea210d36db5
[2026-02-13T21:47:07.760+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:51:55.988+0800] {job.py:229} INFO - Heartbeat recovered after 31.24 seconds
[2026-02-13T21:52:07.535+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>
[2026-02-13T21:52:07.535+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T21:52:07.536+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>
[2026-02-13T21:52:07.538+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:52:07.539+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:52:06.718290+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:52:07.540+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:52:06.718290+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:52:07.542+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:52:06.718290+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:52:09.585+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:52:10.335+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:52:11.323+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:52:06.718290+00:00', try_number=1, map_index=-1)
[2026-02-13T21:52:11.333+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:52:06.718290+00:00, map_index=-1, run_start_date=2026-02-13 13:52:10.400351+00:00, run_end_date=2026-02-13 13:52:10.729860+00:00, run_duration=0.329509, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:52:07.537647+00:00, queued_by_job_id=58, pid=6332
[2026-02-13T21:52:11.362+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:52:14.130+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:14.142+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:52:06.718290+00:00: manual__2026-02-13T13:52:06.718290+00:00, state:running, queued_at: 2026-02-13 13:52:06.732084+00:00. externally triggered: True> successful
[2026-02-13T21:52:14.143+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:52:06.718290+00:00, run_id=manual__2026-02-13T13:52:06.718290+00:00, run_start_date=2026-02-13 13:52:07.514774+00:00, run_end_date=2026-02-13 13:52:14.143258+00:00, run_duration=6.628484, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:52:06.718290+00:00, data_interval_end=2026-02-13 13:52:06.718290+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T21:52:17.893+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:21.713+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:22.763+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:22.946+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:23.993+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:25.034+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:28.558+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:29.614+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:33.248+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:36.508+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:40.325+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:41.368+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:41.564+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:42.627+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:43.293+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:43.610+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:47.287+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:50.962+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:54.576+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:55.650+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:56.691+0800] {scheduler_job_runner.py:1426} ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
[2026-02-13T21:52:57.718+0800] {manager.py:537} INFO - DAG consumer_dagRun is missing and will be deactivated.
[2026-02-13T21:52:57.721+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-13T21:52:57.725+0800] {manager.py:553} INFO - Deleted DAG consumer_dagRun in serialized_dag table
[2026-02-13T21:54:49.428+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>
[2026-02-13T21:54:49.429+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T21:54:49.429+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>
[2026-02-13T21:54:49.432+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:54:49.433+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:52:10.746398+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:54:49.434+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:52:10.746398+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:54:49.436+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:52:10.746398+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:54:51.423+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:54:52.144+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:54:53.066+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:52:10.746398+00:00', try_number=1, map_index=-1)
[2026-02-13T21:54:53.076+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:52:10.746398+00:00, map_index=-1, run_start_date=2026-02-13 13:54:52.208560+00:00, run_end_date=2026-02-13 13:54:52.544442+00:00, run_duration=0.335882, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:54:49.430891+00:00, queued_by_job_id=58, pid=6449
[2026-02-13T21:54:56.219+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:52:10.746398+00:00: dataset_triggered__2026-02-13T13:52:10.746398+00:00, state:running, queued_at: 2026-02-13 13:54:49.394621+00:00. externally triggered: False> failed
[2026-02-13T21:54:56.220+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:52:10.746398+00:00, run_id=dataset_triggered__2026-02-13T13:52:10.746398+00:00, run_start_date=2026-02-13 13:54:49.408553+00:00, run_end_date=2026-02-13 13:54:56.220773+00:00, run_duration=6.81222, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:52:06.718290+00:00, data_interval_end=2026-02-13 13:52:06.718290+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T21:55:14.007+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>
[2026-02-13T21:55:14.009+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T21:55:14.009+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>
[2026-02-13T21:55:14.013+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:55:14.014+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:55:12.814268+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:55:14.015+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:55:12.814268+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:55:14.017+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:55:12.814268+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:55:16.021+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:55:16.637+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:55:17.669+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:55:12.814268+00:00', try_number=1, map_index=-1)
[2026-02-13T21:55:17.678+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:55:12.814268+00:00, map_index=-1, run_start_date=2026-02-13 13:55:16.717090+00:00, run_end_date=2026-02-13 13:55:17.073853+00:00, run_duration=0.356763, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:55:14.011004+00:00, queued_by_job_id=58, pid=6502
[2026-02-13T21:55:20.368+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:55:12.814268+00:00: manual__2026-02-13T13:55:12.814268+00:00, state:running, queued_at: 2026-02-13 13:55:12.825993+00:00. externally triggered: True> successful
[2026-02-13T21:55:20.369+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:55:12.814268+00:00, run_id=manual__2026-02-13T13:55:12.814268+00:00, run_start_date=2026-02-13 13:55:13.982701+00:00, run_end_date=2026-02-13 13:55:20.368974+00:00, run_duration=6.386273, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:55:12.814268+00:00, data_interval_end=2026-02-13 13:55:12.814268+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T21:55:20.379+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>
[2026-02-13T21:55:20.380+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T21:55:20.381+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>
[2026-02-13T21:55:20.384+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:55:20.385+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:55:17.093128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:55:20.385+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:55:17.093128+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:55:20.388+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:55:17.093128+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:55:22.759+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:55:23.330+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:55:24.307+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:55:17.093128+00:00', try_number=1, map_index=-1)
[2026-02-13T21:55:24.325+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:55:17.093128+00:00, map_index=-1, run_start_date=2026-02-13 13:55:23.391484+00:00, run_end_date=2026-02-13 13:55:23.734496+00:00, run_duration=0.343012, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:55:20.382361+00:00, queued_by_job_id=58, pid=6513
[2026-02-13T21:55:27.141+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:55:17.093128+00:00: dataset_triggered__2026-02-13T13:55:17.093128+00:00, state:running, queued_at: 2026-02-13 13:55:20.336102+00:00. externally triggered: False> failed
[2026-02-13T21:55:27.142+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:55:17.093128+00:00, run_id=dataset_triggered__2026-02-13T13:55:17.093128+00:00, run_start_date=2026-02-13 13:55:20.353740+00:00, run_end_date=2026-02-13 13:55:27.142224+00:00, run_duration=6.788484, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:55:12.814268+00:00, data_interval_end=2026-02-13 13:55:12.814268+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T21:57:11.554+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T21:57:49.410+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>
[2026-02-13T21:57:49.411+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T21:57:49.411+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>
[2026-02-13T21:57:49.414+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:57:49.415+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:57:46.936479+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:57:49.415+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:57:46.936479+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:57:49.418+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:57:46.936479+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:57:51.310+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:57:52.050+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:57:53.060+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:57:46.936479+00:00', try_number=1, map_index=-1)
[2026-02-13T21:57:53.069+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:57:46.936479+00:00, map_index=-1, run_start_date=2026-02-13 13:57:52.121692+00:00, run_end_date=2026-02-13 13:57:52.477999+00:00, run_duration=0.356307, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:57:49.412761+00:00, queued_by_job_id=58, pid=6600
[2026-02-13T21:57:55.764+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:57:46.936479+00:00: manual__2026-02-13T13:57:46.936479+00:00, state:running, queued_at: 2026-02-13 13:57:46.947402+00:00. externally triggered: True> successful
[2026-02-13T21:57:55.765+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:57:46.936479+00:00, run_id=manual__2026-02-13T13:57:46.936479+00:00, run_start_date=2026-02-13 13:57:49.390250+00:00, run_end_date=2026-02-13 13:57:55.765280+00:00, run_duration=6.37503, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:57:46.936479+00:00, data_interval_end=2026-02-13 13:57:46.936479+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T21:57:55.775+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>
[2026-02-13T21:57:55.776+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T21:57:55.776+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>
[2026-02-13T21:57:55.779+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T21:57:55.780+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:57:52.497306+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T21:57:55.780+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:57:52.497306+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:57:55.783+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:57:52.497306+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T21:57:57.679+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T21:57:58.235+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [queued]> on host localhost-2.local
[2026-02-13T21:57:59.135+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:57:52.497306+00:00', try_number=1, map_index=-1)
[2026-02-13T21:57:59.145+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:57:52.497306+00:00, map_index=-1, run_start_date=2026-02-13 13:57:58.299452+00:00, run_end_date=2026-02-13 13:57:58.648892+00:00, run_duration=0.34944, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:57:55.777895+00:00, queued_by_job_id=58, pid=6605
[2026-02-13T21:58:01.737+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:57:52.497306+00:00: dataset_triggered__2026-02-13T13:57:52.497306+00:00, state:running, queued_at: 2026-02-13 13:57:55.735901+00:00. externally triggered: False> successful
[2026-02-13T21:58:01.739+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:57:52.497306+00:00, run_id=dataset_triggered__2026-02-13T13:57:52.497306+00:00, run_start_date=2026-02-13 13:57:55.751523+00:00, run_end_date=2026-02-13 13:58:01.738990+00:00, run_duration=5.987467, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:57:46.936479+00:00, data_interval_end=2026-02-13 13:57:46.936479+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:02:12.674+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T22:06:36.977+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>
[2026-02-13T22:06:36.978+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T22:06:36.979+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>
[2026-02-13T22:06:36.982+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:06:36.983+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:06:35.989891+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:06:36.984+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:06:35.989891+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:06:36.987+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:06:35.989891+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:06:38.962+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:06:39.654+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:06:40.628+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:06:35.989891+00:00', try_number=1, map_index=-1)
[2026-02-13T22:06:40.637+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:06:35.989891+00:00, map_index=-1, run_start_date=2026-02-13 14:06:39.723961+00:00, run_end_date=2026-02-13 14:06:40.072376+00:00, run_duration=0.348415, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:06:36.980892+00:00, queued_by_job_id=58, pid=6815
[2026-02-13T22:06:40.707+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:06:35.989891+00:00: manual__2026-02-13T14:06:35.989891+00:00, state:running, queued_at: 2026-02-13 14:06:36.008553+00:00. externally triggered: True> successful
[2026-02-13T22:06:40.708+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:06:35.989891+00:00, run_id=manual__2026-02-13T14:06:35.989891+00:00, run_start_date=2026-02-13 14:06:36.954082+00:00, run_end_date=2026-02-13 14:06:40.708210+00:00, run_duration=3.754128, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:06:35.989891+00:00, data_interval_end=2026-02-13 14:06:35.989891+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T22:06:41.764+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>
[2026-02-13T22:06:41.765+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T22:06:41.766+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>
[2026-02-13T22:06:41.768+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:06:41.769+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:06:40.090635+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:06:41.770+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:06:40.090635+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:06:41.772+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:06:40.090635+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:06:43.809+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:06:44.434+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:06:45.440+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:06:40.090635+00:00', try_number=1, map_index=-1)
[2026-02-13T22:06:45.449+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:06:40.090635+00:00, map_index=-1, run_start_date=2026-02-13 14:06:44.502527+00:00, run_end_date=2026-02-13 14:06:44.869335+00:00, run_duration=0.366808, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:06:41.767241+00:00, queued_by_job_id=58, pid=6818
[2026-02-13T22:06:47.915+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:06:40.090635+00:00: dataset_triggered__2026-02-13T14:06:40.090635+00:00, state:running, queued_at: 2026-02-13 14:06:40.689164+00:00. externally triggered: False> successful
[2026-02-13T22:06:47.916+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:06:40.090635+00:00, run_id=dataset_triggered__2026-02-13T14:06:40.090635+00:00, run_start_date=2026-02-13 14:06:41.744462+00:00, run_end_date=2026-02-13 14:06:47.916201+00:00, run_duration=6.171739, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:06:35.989891+00:00, data_interval_end=2026-02-13 14:06:35.989891+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:07:12.550+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T22:10:21.604+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>
[2026-02-13T22:10:21.605+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T22:10:21.605+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>
[2026-02-13T22:10:21.608+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:10:21.609+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:10:18.122537+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:10:21.609+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:10:18.122537+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:10:21.612+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:10:18.122537+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:10:23.498+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:10:24.278+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:10:25.302+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:10:18.122537+00:00', try_number=1, map_index=-1)
[2026-02-13T22:10:25.313+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:10:18.122537+00:00, map_index=-1, run_start_date=2026-02-13 14:10:24.346755+00:00, run_end_date=2026-02-13 14:10:24.735967+00:00, run_duration=0.389212, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:10:21.607014+00:00, queued_by_job_id=58, pid=6947
[2026-02-13T22:10:27.802+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:10:18.122537+00:00: manual__2026-02-13T14:10:18.122537+00:00, state:running, queued_at: 2026-02-13 14:10:18.133454+00:00. externally triggered: True> successful
[2026-02-13T22:10:27.803+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:10:18.122537+00:00, run_id=manual__2026-02-13T14:10:18.122537+00:00, run_start_date=2026-02-13 14:10:21.584849+00:00, run_end_date=2026-02-13 14:10:27.803156+00:00, run_duration=6.218307, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:10:18.122537+00:00, data_interval_end=2026-02-13 14:10:18.122537+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T22:10:27.812+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>
[2026-02-13T22:10:27.812+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T22:10:27.813+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>
[2026-02-13T22:10:27.815+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:10:27.816+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:10:24.755287+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:10:27.817+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:10:24.755287+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:10:27.819+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:10:24.755287+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:10:29.706+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:10:30.261+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:10:31.354+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:10:24.755287+00:00', try_number=1, map_index=-1)
[2026-02-13T22:10:31.365+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:10:24.755287+00:00, map_index=-1, run_start_date=2026-02-13 14:10:30.335444+00:00, run_end_date=2026-02-13 14:10:30.752163+00:00, run_duration=0.416719, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:10:27.814468+00:00, queued_by_job_id=58, pid=6950
[2026-02-13T22:10:33.872+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:10:24.755287+00:00: dataset_triggered__2026-02-13T14:10:24.755287+00:00, state:running, queued_at: 2026-02-13 14:10:27.773909+00:00. externally triggered: False> successful
[2026-02-13T22:10:33.873+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:10:24.755287+00:00, run_id=dataset_triggered__2026-02-13T14:10:24.755287+00:00, run_start_date=2026-02-13 14:10:27.789025+00:00, run_end_date=2026-02-13 14:10:33.873466+00:00, run_duration=6.084441, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:10:18.122537+00:00, data_interval_end=2026-02-13 14:10:18.122537+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:12:13.179+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T22:13:59.439+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>
[2026-02-13T22:13:59.440+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T22:13:59.441+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>
[2026-02-13T22:13:59.443+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:13:59.444+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:13:57.660372+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:13:59.445+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:13:57.660372+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:13:59.447+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:13:57.660372+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:14:01.349+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:14:02.064+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:14:02.992+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:13:57.660372+00:00', try_number=1, map_index=-1)
[2026-02-13T22:14:03.002+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:13:57.660372+00:00, map_index=-1, run_start_date=2026-02-13 14:14:02.134441+00:00, run_end_date=2026-02-13 14:14:02.467478+00:00, run_duration=0.333037, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:13:59.442521+00:00, queued_by_job_id=58, pid=7044
[2026-02-13T22:14:05.542+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:13:57.660372+00:00: manual__2026-02-13T14:13:57.660372+00:00, state:running, queued_at: 2026-02-13 14:13:57.671428+00:00. externally triggered: True> successful
[2026-02-13T22:14:05.543+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:13:57.660372+00:00, run_id=manual__2026-02-13T14:13:57.660372+00:00, run_start_date=2026-02-13 14:13:59.418194+00:00, run_end_date=2026-02-13 14:14:05.543694+00:00, run_duration=6.1255, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:13:57.660372+00:00, data_interval_end=2026-02-13 14:13:57.660372+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T22:14:05.554+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>
[2026-02-13T22:14:05.555+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T22:14:05.555+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>
[2026-02-13T22:14:05.558+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:14:05.559+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:14:02.484868+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:14:05.560+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:14:02.484868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:14:05.562+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:14:02.484868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:14:07.691+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:14:08.261+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:14:09.256+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:14:02.484868+00:00', try_number=1, map_index=-1)
[2026-02-13T22:14:09.266+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:14:02.484868+00:00, map_index=-1, run_start_date=2026-02-13 14:14:08.328089+00:00, run_end_date=2026-02-13 14:14:08.672120+00:00, run_duration=0.344031, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:14:05.557023+00:00, queued_by_job_id=58, pid=7047
[2026-02-13T22:14:11.726+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:14:02.484868+00:00: dataset_triggered__2026-02-13T14:14:02.484868+00:00, state:running, queued_at: 2026-02-13 14:14:05.513229+00:00. externally triggered: False> failed
[2026-02-13T22:14:11.727+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:14:02.484868+00:00, run_id=dataset_triggered__2026-02-13T14:14:02.484868+00:00, run_start_date=2026-02-13 14:14:05.528608+00:00, run_end_date=2026-02-13 14:14:11.727269+00:00, run_duration=6.198661, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:13:57.660372+00:00, data_interval_end=2026-02-13 14:13:57.660372+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:16:13.521+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>
[2026-02-13T22:16:13.523+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T22:16:13.524+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>
[2026-02-13T22:16:13.530+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:16:13.532+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:16:12.947371+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:16:13.535+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:16:12.947371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:16:13.538+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:16:12.947371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:16:15.641+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:16:16.370+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:16:17.388+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:16:12.947371+00:00', try_number=1, map_index=-1)
[2026-02-13T22:16:17.397+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:16:12.947371+00:00, map_index=-1, run_start_date=2026-02-13 14:16:16.438898+00:00, run_end_date=2026-02-13 14:16:16.785992+00:00, run_duration=0.347094, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:16:13.528314+00:00, queued_by_job_id=58, pid=7109
[2026-02-13T22:16:19.827+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:16:12.947371+00:00: manual__2026-02-13T14:16:12.947371+00:00, state:running, queued_at: 2026-02-13 14:16:12.958807+00:00. externally triggered: True> successful
[2026-02-13T22:16:19.828+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:16:12.947371+00:00, run_id=manual__2026-02-13T14:16:12.947371+00:00, run_start_date=2026-02-13 14:16:13.494028+00:00, run_end_date=2026-02-13 14:16:19.828416+00:00, run_duration=6.334388, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:16:12.947371+00:00, data_interval_end=2026-02-13 14:16:12.947371+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T22:16:19.839+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>
[2026-02-13T22:16:19.840+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T22:16:19.841+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>
[2026-02-13T22:16:19.843+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:16:19.845+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:16:16.802861+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:16:19.845+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:16:16.802861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:16:19.859+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:16:16.802861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:16:21.881+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:16:22.449+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:16:23.430+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:16:16.802861+00:00', try_number=1, map_index=-1)
[2026-02-13T22:16:23.440+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:16:16.802861+00:00, map_index=-1, run_start_date=2026-02-13 14:16:22.517118+00:00, run_end_date=2026-02-13 14:16:22.859699+00:00, run_duration=0.342581, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:16:19.842621+00:00, queued_by_job_id=58, pid=7112
[2026-02-13T22:16:25.793+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:16:16.802861+00:00: dataset_triggered__2026-02-13T14:16:16.802861+00:00, state:running, queued_at: 2026-02-13 14:16:19.797692+00:00. externally triggered: False> failed
[2026-02-13T22:16:25.794+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:16:16.802861+00:00, run_id=dataset_triggered__2026-02-13T14:16:16.802861+00:00, run_start_date=2026-02-13 14:16:19.813628+00:00, run_end_date=2026-02-13 14:16:25.794565+00:00, run_duration=5.980937, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:16:12.947371+00:00, data_interval_end=2026-02-13 14:16:12.947371+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:17:15.957+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T22:18:36.852+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>
[2026-02-13T22:18:36.853+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
[2026-02-13T22:18:36.854+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>
[2026-02-13T22:18:36.856+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:18:36.857+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:18:33.086772+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:18:36.857+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:18:33.086772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:18:36.860+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:18:33.086772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:18:38.744+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:18:39.307+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:18:40.386+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:18:33.086772+00:00', try_number=1, map_index=-1)
[2026-02-13T22:18:40.396+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:18:33.086772+00:00, map_index=-1, run_start_date=2026-02-13 14:18:39.380049+00:00, run_end_date=2026-02-13 14:18:39.772192+00:00, run_duration=0.392143, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=93, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:18:36.855081+00:00, queued_by_job_id=58, pid=7167
[2026-02-13T22:18:42.913+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:18:33.086772+00:00: manual__2026-02-13T14:18:33.086772+00:00, state:running, queued_at: 2026-02-13 14:18:33.097380+00:00. externally triggered: True> successful
[2026-02-13T22:18:42.915+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:18:33.086772+00:00, run_id=manual__2026-02-13T14:18:33.086772+00:00, run_start_date=2026-02-13 14:18:36.832831+00:00, run_end_date=2026-02-13 14:18:42.914955+00:00, run_duration=6.082124, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:18:33.086772+00:00, data_interval_end=2026-02-13 14:18:33.086772+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
[2026-02-13T22:18:42.923+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>
[2026-02-13T22:18:42.924+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dagRun has 0/16 running and queued tasks
[2026-02-13T22:18:42.925+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>
[2026-02-13T22:18:42.927+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T22:18:42.928+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:18:39.791063+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T22:18:42.929+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:18:39.791063+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:18:42.931+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:18:39.791063+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
[2026-02-13T22:18:44.822+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_dagRun.py
[2026-02-13T22:18:45.386+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [queued]> on host localhost-2.local
[2026-02-13T22:18:46.451+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:18:39.791063+00:00', try_number=1, map_index=-1)
[2026-02-13T22:18:46.462+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:18:39.791063+00:00, map_index=-1, run_start_date=2026-02-13 14:18:45.469212+00:00, run_end_date=2026-02-13 14:18:45.856053+00:00, run_duration=0.386841, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=94, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:18:42.926427+00:00, queued_by_job_id=58, pid=7170
[2026-02-13T22:18:48.993+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:18:39.791063+00:00: dataset_triggered__2026-02-13T14:18:39.791063+00:00, state:running, queued_at: 2026-02-13 14:18:42.889311+00:00. externally triggered: False> successful
[2026-02-13T22:18:48.994+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:18:39.791063+00:00, run_id=dataset_triggered__2026-02-13T14:18:39.791063+00:00, run_start_date=2026-02-13 14:18:42.901593+00:00, run_end_date=2026-02-13 14:18:48.994842+00:00, run_duration=6.093249, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:18:33.086772+00:00, data_interval_end=2026-02-13 14:18:33.086772+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
[2026-02-13T22:22:17.944+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
