[2026-02-14T00:05:11.444+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-14T00:05:11.974+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-14T00:05:11.975+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-14T00:05:11.981+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 8376
[2026-02-14T00:05:11.986+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T00:05:13.992+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-14T00:05:14.040+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-14T00:05:14.314+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-14T00:05:58.194+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>
[2026-02-14T00:05:58.195+0800] {scheduler_job_runner.py:507} INFO - DAG producer_xcom_sync has 0/16 running and queued tasks
[2026-02-14T00:05:58.196+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>
[2026-02-14T00:05:58.200+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T00:05:58.200+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_xcom_sync', task_id='producer_xcom_save_data', run_id='manual__2026-02-13T16:05:55.895216+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:05:58.201+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_xcom_sync', 'producer_xcom_save_data', 'manual__2026-02-13T16:05:55.895216+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:05:58.204+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_xcom_sync', 'producer_xcom_save_data', 'manual__2026-02-13T16:05:55.895216+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:06:00.100+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_xcom.py
[2026-02-14T00:06:00.689+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:06:01.825+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_xcom_sync', task_id='producer_xcom_save_data', run_id='manual__2026-02-13T16:05:55.895216+00:00', try_number=1, map_index=-1)
[2026-02-14T00:06:01.839+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_xcom_sync, task_id=producer_xcom_save_data, run_id=manual__2026-02-13T16:05:55.895216+00:00, map_index=-1, run_start_date=2026-02-13 16:06:00.749982+00:00, run_end_date=2026-02-13 16:06:01.099449+00:00, run_duration=0.349467, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=99, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:05:58.197995+00:00, queued_by_job_id=98, pid=8426
[2026-02-14T00:06:04.561+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_xcom_sync @ 2026-02-13 16:05:55.895216+00:00: manual__2026-02-13T16:05:55.895216+00:00, state:running, queued_at: 2026-02-13 16:05:55.916304+00:00. externally triggered: True> successful
[2026-02-14T00:06:04.562+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_xcom_sync, execution_date=2026-02-13 16:05:55.895216+00:00, run_id=manual__2026-02-13T16:05:55.895216+00:00, run_start_date=2026-02-13 16:05:58.158330+00:00, run_end_date=2026-02-13 16:06:04.562528+00:00, run_duration=6.404198, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=122444dd0c644311f38ec0d9e938cb3b
[2026-02-14T00:06:04.572+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>
	<TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>
[2026-02-14T00:06:04.573+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_xcom2 has 0/16 running and queued tasks
[2026-02-14T00:06:04.574+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_xcom1 has 0/16 running and queued tasks
[2026-02-14T00:06:04.575+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>
	<TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>
[2026-02-14T00:06:04.578+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>, <TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T00:06:04.579+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_xcom2', task_id='consumer_xcom_read_data2', run_id='dataset_triggered__2026-02-13T16:06:01.114736+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:06:04.579+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_xcom2', 'consumer_xcom_read_data2', 'dataset_triggered__2026-02-13T16:06:01.114736+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:06:04.580+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_xcom1', task_id='consumer_xcom_read_data1', run_id='dataset_triggered__2026-02-13T16:06:01.116213+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:06:04.581+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_xcom1', 'consumer_xcom_read_data1', 'dataset_triggered__2026-02-13T16:06:01.116213+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:06:04.583+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_xcom2', 'consumer_xcom_read_data2', 'dataset_triggered__2026-02-13T16:06:01.114736+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:06:06.457+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_xcom.py
[2026-02-14T00:06:07.033+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:06:08.150+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_xcom1', 'consumer_xcom_read_data1', 'dataset_triggered__2026-02-13T16:06:01.116213+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
[2026-02-14T00:06:10.240+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_xcom.py
[2026-02-14T00:06:10.821+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:06:11.952+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_xcom2', task_id='consumer_xcom_read_data2', run_id='dataset_triggered__2026-02-13T16:06:01.114736+00:00', try_number=1, map_index=-1)
[2026-02-14T00:06:11.955+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_xcom1', task_id='consumer_xcom_read_data1', run_id='dataset_triggered__2026-02-13T16:06:01.116213+00:00', try_number=1, map_index=-1)
[2026-02-14T00:06:11.966+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_xcom2, task_id=consumer_xcom_read_data2, run_id=dataset_triggered__2026-02-13T16:06:01.114736+00:00, map_index=-1, run_start_date=2026-02-13 16:06:07.095447+00:00, run_end_date=2026-02-13 16:06:07.457738+00:00, run_duration=0.362291, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=100, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:06:04.576236+00:00, queued_by_job_id=98, pid=8429
[2026-02-14T00:06:11.968+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_xcom1, task_id=consumer_xcom_read_data1, run_id=dataset_triggered__2026-02-13T16:06:01.116213+00:00, map_index=-1, run_start_date=2026-02-13 16:06:10.881821+00:00, run_end_date=2026-02-13 16:06:11.256517+00:00, run_duration=0.374696, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=101, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:06:04.576236+00:00, queued_by_job_id=98, pid=8431
[2026-02-14T00:06:14.399+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_xcom2 @ 2026-02-13 16:06:01.114736+00:00: dataset_triggered__2026-02-13T16:06:01.114736+00:00, state:running, queued_at: 2026-02-13 16:06:04.530766+00:00. externally triggered: False> successful
[2026-02-14T00:06:14.400+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_xcom2, execution_date=2026-02-13 16:06:01.114736+00:00, run_id=dataset_triggered__2026-02-13T16:06:01.114736+00:00, run_start_date=2026-02-13 16:06:04.545102+00:00, run_end_date=2026-02-13 16:06:14.400651+00:00, run_duration=9.855549, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=18e83b8aaf201b23343a6d45144041ed
[2026-02-14T00:06:14.405+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_xcom1 @ 2026-02-13 16:06:01.116213+00:00: dataset_triggered__2026-02-13T16:06:01.116213+00:00, state:running, queued_at: 2026-02-13 16:06:04.518235+00:00. externally triggered: False> successful
[2026-02-14T00:06:14.406+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_xcom1, execution_date=2026-02-13 16:06:01.116213+00:00, run_id=dataset_triggered__2026-02-13T16:06:01.116213+00:00, run_start_date=2026-02-13 16:06:04.545210+00:00, run_end_date=2026-02-13 16:06:14.406239+00:00, run_duration=9.861029, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=5e2a1ff8e090009575648e79d38671d9
[2026-02-14T00:10:13.490+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T00:15:13.529+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T00:20:13.567+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T00:25:15.945+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T00:25:19.569+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>
[2026-02-14T00:25:19.570+0800] {scheduler_job_runner.py:507} INFO - DAG producer_DIPOA_dw_order_raw_sync has 0/16 running and queued tasks
[2026-02-14T00:25:19.570+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>
[2026-02-14T00:25:19.573+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T00:25:19.574+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_DIPOA_dw_order_raw_sync', task_id='sync_raw_order_data', run_id='manual__2026-02-13T16:25:15.518091+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:25:19.574+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_DIPOA_dw_order_raw_sync', 'sync_raw_order_data', 'manual__2026-02-13T16:25:15.518091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:19.577+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_DIPOA_dw_order_raw_sync', 'sync_raw_order_data', 'manual__2026-02-13T16:25:15.518091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:21.546+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_DIPOA.py
[2026-02-14T00:25:22.314+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:25:23.379+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_DIPOA_dw_order_raw_sync', task_id='sync_raw_order_data', run_id='manual__2026-02-13T16:25:15.518091+00:00', try_number=1, map_index=-1)
[2026-02-14T00:25:23.390+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_DIPOA_dw_order_raw_sync, task_id=sync_raw_order_data, run_id=manual__2026-02-13T16:25:15.518091+00:00, map_index=-1, run_start_date=2026-02-13 16:25:22.386615+00:00, run_end_date=2026-02-13 16:25:22.769273+00:00, run_duration=0.382658, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=102, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:19.571822+00:00, queued_by_job_id=98, pid=8989
[2026-02-14T00:25:25.975+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_DIPOA_dw_order_raw_sync @ 2026-02-13 16:25:15.518091+00:00: manual__2026-02-13T16:25:15.518091+00:00, state:running, queued_at: 2026-02-13 16:25:15.533854+00:00. externally triggered: True> successful
[2026-02-14T00:25:25.977+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_DIPOA_dw_order_raw_sync, execution_date=2026-02-13 16:25:15.518091+00:00, run_id=manual__2026-02-13T16:25:15.518091+00:00, run_start_date=2026-02-13 16:25:19.549371+00:00, run_end_date=2026-02-13 16:25:25.976934+00:00, run_duration=6.427563, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=24391711b5e16fd626025bfde1dadd75
[2026-02-14T00:25:25.986+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>
[2026-02-14T00:25:25.987+0800] {scheduler_job_runner.py:507} INFO - DAG consumer1_order_clean has 0/16 running and queued tasks
[2026-02-14T00:25:25.987+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>
[2026-02-14T00:25:25.990+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T00:25:25.991+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer1_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T16:25:22.789030+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:25:25.992+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer1_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T16:25:22.789030+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:25.994+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer1_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T16:25:22.789030+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:27.916+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_DIPOA.py
[2026-02-14T00:25:28.528+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:25:29.627+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer1_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T16:25:22.789030+00:00', try_number=1, map_index=-1)
[2026-02-14T00:25:29.638+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer1_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T16:25:22.789030+00:00, map_index=-1, run_start_date=2026-02-13 16:25:28.594463+00:00, run_end_date=2026-02-13 16:25:28.990966+00:00, run_duration=0.396503, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=103, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:25.988916+00:00, queued_by_job_id=98, pid=8992
[2026-02-14T00:25:32.110+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer1_order_clean @ 2026-02-13 16:25:22.789030+00:00: dataset_triggered__2026-02-13T16:25:22.789030+00:00, state:running, queued_at: 2026-02-13 16:25:25.946870+00:00. externally triggered: False> successful
[2026-02-14T00:25:32.111+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer1_order_clean, execution_date=2026-02-13 16:25:22.789030+00:00, run_id=dataset_triggered__2026-02-13T16:25:22.789030+00:00, run_start_date=2026-02-13 16:25:25.963573+00:00, run_end_date=2026-02-13 16:25:32.111196+00:00, run_duration=6.147623, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=3829db3dbf828c470c9a060bc9bfe7f1
[2026-02-14T00:25:32.120+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>
[2026-02-14T00:25:32.121+0800] {scheduler_job_runner.py:507} INFO - DAG consumer2_order_stat has 0/16 running and queued tasks
[2026-02-14T00:25:32.122+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>
[2026-02-14T00:25:32.124+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T00:25:32.126+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer2_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T16:25:29.009309+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T00:25:32.126+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer2_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T16:25:29.009309+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:32.129+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer2_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T16:25:29.009309+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
[2026-02-14T00:25:34.084+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /dataset_DIPOA.py
[2026-02-14T00:25:34.684+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [queued]> on host localhost-2.local
[2026-02-14T00:25:35.745+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer2_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T16:25:29.009309+00:00', try_number=1, map_index=-1)
[2026-02-14T00:25:35.757+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer2_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T16:25:29.009309+00:00, map_index=-1, run_start_date=2026-02-13 16:25:34.752886+00:00, run_end_date=2026-02-13 16:25:35.142861+00:00, run_duration=0.389975, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=104, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:32.123201+00:00, queued_by_job_id=98, pid=8995
[2026-02-14T00:25:38.337+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer2_order_stat @ 2026-02-13 16:25:29.009309+00:00: dataset_triggered__2026-02-13T16:25:29.009309+00:00, state:running, queued_at: 2026-02-13 16:25:32.084110+00:00. externally triggered: False> successful
[2026-02-14T00:25:38.338+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer2_order_stat, execution_date=2026-02-13 16:25:29.009309+00:00, run_id=dataset_triggered__2026-02-13T16:25:29.009309+00:00, run_start_date=2026-02-13 16:25:32.096788+00:00, run_end_date=2026-02-13 16:25:38.338114+00:00, run_duration=6.241326, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=366c5b167887ebcb721d4da129564e00
[2026-02-14T00:30:18.234+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
