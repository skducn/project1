[2026-02-13T10:58:20.272+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-13T10:58:20.874+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-13T10:58:20.877+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-13T10:58:20.887+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 6580
[2026-02-13T10:58:20.897+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T10:58:23.035+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-13T10:58:23.088+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-13T10:58:23.399+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-13T11:00:57.496+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>
[2026-02-13T11:00:57.497+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:00:57.497+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>
[2026-02-13T11:00:57.501+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:00:57.502+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:00:56.955760+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:00:57.503+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:00:56.955760+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:00:57.506+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:00:56.955760+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:00:59.932+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:01:00.696+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:01:01.698+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:00:56.955760+00:00', try_number=1, map_index=-1)
[2026-02-13T11:01:01.712+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:00:56.955760+00:00, map_index=-1, run_start_date=2026-02-13 03:01:00.771346+00:00, run_end_date=2026-02-13 03:01:01.167277+00:00, run_duration=0.395931, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:00:57.499727+00:00, queued_by_job_id=41, pid=6637
[2026-02-13T11:01:01.759+0800] {dagrun.py:823} ERROR - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:00:56.955760+00:00: manual__2026-02-13T03:00:56.955760+00:00, state:running, queued_at: 2026-02-13 03:00:56.975748+00:00. externally triggered: True> failed
[2026-02-13T11:01:01.761+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:00:56.955760+00:00, run_id=manual__2026-02-13T03:00:56.955760+00:00, run_start_date=2026-02-13 03:00:57.463039+00:00, run_end_date=2026-02-13 03:01:01.761192+00:00, run_duration=4.298153, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:00:56.955760+00:00, data_interval_end=2026-02-13 03:00:56.955760+00:00, dag_hash=292d5ad65135765608fc09000942b435
[2026-02-13T11:03:01.854+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>
[2026-02-13T11:03:01.855+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:03:01.856+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>
[2026-02-13T11:03:01.859+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:03:01.860+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:03:00.730591+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:03:01.861+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:03:00.730591+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:03:01.863+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:03:00.730591+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:03:04.395+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:03:05.212+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:03:06.370+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:03:00.730591+00:00', try_number=1, map_index=-1)
[2026-02-13T11:03:06.381+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:03:00.730591+00:00, map_index=-1, run_start_date=2026-02-13 03:03:05.280088+00:00, run_end_date=2026-02-13 03:03:05.732314+00:00, run_duration=0.452226, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:03:01.857929+00:00, queued_by_job_id=41, pid=6697
[2026-02-13T11:03:09.188+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:03:00.730591+00:00: manual__2026-02-13T03:03:00.730591+00:00, state:running, queued_at: 2026-02-13 03:03:00.744300+00:00. externally triggered: True> successful
[2026-02-13T11:03:09.189+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:03:00.730591+00:00, run_id=manual__2026-02-13T03:03:00.730591+00:00, run_start_date=2026-02-13 03:03:01.834575+00:00, run_end_date=2026-02-13 03:03:09.189026+00:00, run_duration=7.354451, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 03:03:00.730591+00:00, dag_hash=292d5ad65135765608fc09000942b435
[2026-02-13T11:03:09.252+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>
[2026-02-13T11:03:09.263+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:03:09.264+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>
[2026-02-13T11:03:09.267+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:03:09.269+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:03:05.749067+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:03:09.269+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:03:05.749067+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:03:09.288+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:03:05.749067+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:03:11.829+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:03:12.532+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:03:13.851+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:03:05.749067+00:00', try_number=1, map_index=-1)
[2026-02-13T11:03:13.865+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:03:05.749067+00:00, map_index=-1, run_start_date=2026-02-13 03:03:12.595873+00:00, run_end_date=2026-02-13 03:03:12.993313+00:00, run_duration=0.39744, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:03:09.266267+00:00, queued_by_job_id=41, pid=6700
[2026-02-13T11:03:16.803+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:03:05.749067+00:00: dataset_triggered__2026-02-13T03:03:05.749067+00:00, state:running, queued_at: 2026-02-13 03:03:09.151978+00:00. externally triggered: False> successful
[2026-02-13T11:03:16.805+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:03:05.749067+00:00, run_id=dataset_triggered__2026-02-13T03:03:05.749067+00:00, run_start_date=2026-02-13 03:03:09.172019+00:00, run_end_date=2026-02-13 03:03:16.805082+00:00, run_duration=7.633063, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 03:03:00.730591+00:00, dag_hash=336021f9586175064690381be57e6c20
[2026-02-13T11:03:20.949+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:08:22.300+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:12:43.207+0800] {manager.py:537} INFO - DAG consumer_dw_order_clean is missing and will be deactivated.
[2026-02-13T11:12:43.209+0800] {manager.py:537} INFO - DAG producer_dw_order_sync is missing and will be deactivated.
[2026-02-13T11:12:43.214+0800] {manager.py:549} INFO - Deactivated 2 DAGs which are no longer present in file.
[2026-02-13T11:12:43.217+0800] {manager.py:553} INFO - Deleted DAG producer_dw_order_sync in serialized_dag table
[2026-02-13T11:12:43.221+0800] {manager.py:553} INFO - Deleted DAG consumer_dw_order_clean in serialized_dag table
[2026-02-13T11:13:23.601+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:15:30.076+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>
[2026-02-13T11:15:30.077+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:15:30.078+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>
[2026-02-13T11:15:30.082+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:15:30.086+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:15:28.766581+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:15:30.087+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:15:28.766581+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:15:30.090+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:15:28.766581+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:15:32.436+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:15:33.656+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:15:34.771+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:15:28.766581+00:00', try_number=1, map_index=-1)
[2026-02-13T11:15:34.780+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:15:28.766581+00:00, map_index=-1, run_start_date=2026-02-13 03:15:33.736246+00:00, run_end_date=2026-02-13 03:15:34.213945+00:00, run_duration=0.477699, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:15:30.079798+00:00, queued_by_job_id=41, pid=6940
[2026-02-13T11:15:37.777+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:15:28.766581+00:00: manual__2026-02-13T03:15:28.766581+00:00, state:running, queued_at: 2026-02-13 03:15:28.788425+00:00. externally triggered: True> successful
[2026-02-13T11:15:37.778+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:15:28.766581+00:00, run_id=manual__2026-02-13T03:15:28.766581+00:00, run_start_date=2026-02-13 03:15:30.048480+00:00, run_end_date=2026-02-13 03:15:37.778437+00:00, run_duration=7.729957, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:15:28.766581+00:00, data_interval_end=2026-02-13 03:15:28.766581+00:00, dag_hash=292d5ad65135765608fc09000942b435
[2026-02-13T11:15:37.788+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>
[2026-02-13T11:15:37.789+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:15:37.789+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>
[2026-02-13T11:15:37.792+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:15:37.793+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:15:34.232141+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:15:37.793+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:15:34.232141+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:15:37.796+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:15:34.232141+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:15:39.899+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:15:40.570+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:15:41.571+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:15:34.232141+00:00', try_number=1, map_index=-1)
[2026-02-13T11:15:41.579+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:15:34.232141+00:00, map_index=-1, run_start_date=2026-02-13 03:15:40.639840+00:00, run_end_date=2026-02-13 03:15:40.968642+00:00, run_duration=0.328802, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:15:37.790849+00:00, queued_by_job_id=41, pid=6950
[2026-02-13T11:15:44.402+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:15:34.232141+00:00: dataset_triggered__2026-02-13T03:15:34.232141+00:00, state:running, queued_at: 2026-02-13 03:15:37.748990+00:00. externally triggered: False> successful
[2026-02-13T11:15:44.403+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:15:34.232141+00:00, run_id=dataset_triggered__2026-02-13T03:15:34.232141+00:00, run_start_date=2026-02-13 03:15:37.762831+00:00, run_end_date=2026-02-13 03:15:44.403607+00:00, run_duration=6.640776, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:15:28.766581+00:00, data_interval_end=2026-02-13 03:15:28.766581+00:00, dag_hash=336021f9586175064690381be57e6c20
[2026-02-13T11:18:23.607+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:20:51.068+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>
[2026-02-13T11:20:51.070+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:20:51.070+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>
[2026-02-13T11:20:51.073+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:20:51.076+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:20:50.500982+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:20:51.077+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:20:50.500982+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:20:51.079+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:20:50.500982+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:20:53.299+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:20:54.284+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:20:55.321+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:20:50.500982+00:00', try_number=1, map_index=-1)
[2026-02-13T11:20:55.333+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:20:50.500982+00:00, map_index=-1, run_start_date=2026-02-13 03:20:54.354533+00:00, run_end_date=2026-02-13 03:20:54.789942+00:00, run_duration=0.435409, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:20:51.072024+00:00, queued_by_job_id=41, pid=7038
[2026-02-13T11:20:55.403+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:20:50.500982+00:00: manual__2026-02-13T03:20:50.500982+00:00, state:running, queued_at: 2026-02-13 03:20:50.514067+00:00. externally triggered: True> successful
[2026-02-13T11:20:55.404+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:20:50.500982+00:00, run_id=manual__2026-02-13T03:20:50.500982+00:00, run_start_date=2026-02-13 03:20:51.039154+00:00, run_end_date=2026-02-13 03:20:55.404373+00:00, run_duration=4.365219, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:20:50.500982+00:00, data_interval_end=2026-02-13 03:20:50.500982+00:00, dag_hash=292d5ad65135765608fc09000942b435
[2026-02-13T11:20:55.414+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>
[2026-02-13T11:20:55.415+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:20:55.416+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>
[2026-02-13T11:20:55.418+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:20:55.419+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:20:54.810387+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:20:55.420+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:20:54.810387+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:20:55.422+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:20:54.810387+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:20:57.478+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:20:58.106+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:20:58.946+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:20:54.810387+00:00', try_number=1, map_index=-1)
[2026-02-13T11:20:58.954+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:20:54.810387+00:00, map_index=-1, run_start_date=2026-02-13 03:20:58.179565+00:00, run_end_date=2026-02-13 03:20:58.478434+00:00, run_duration=0.298869, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:20:55.417432+00:00, queued_by_job_id=41, pid=7041
[2026-02-13T11:20:58.998+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:20:54.810387+00:00: dataset_triggered__2026-02-13T03:20:54.810387+00:00, state:running, queued_at: 2026-02-13 03:20:55.377690+00:00. externally triggered: False> successful
[2026-02-13T11:20:58.999+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:20:54.810387+00:00, run_id=dataset_triggered__2026-02-13T03:20:54.810387+00:00, run_start_date=2026-02-13 03:20:55.389670+00:00, run_end_date=2026-02-13 03:20:58.999297+00:00, run_duration=3.609627, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:20:50.500982+00:00, data_interval_end=2026-02-13 03:20:50.500982+00:00, dag_hash=336021f9586175064690381be57e6c20
[2026-02-13T11:23:26.021+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:28:26.768+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:33:29.567+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:36:01.032+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>
[2026-02-13T11:36:01.033+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:36:01.033+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>
[2026-02-13T11:36:01.036+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:36:01.038+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:35:59.119446+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:36:01.038+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:35:59.119446+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:36:01.041+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:35:59.119446+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:36:03.251+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:36:04.240+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:36:05.394+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:35:59.119446+00:00', try_number=1, map_index=-1)
[2026-02-13T11:36:05.403+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:35:59.119446+00:00, map_index=-1, run_start_date=2026-02-13 03:36:04.315076+00:00, run_end_date=2026-02-13 03:36:04.807601+00:00, run_duration=0.492525, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:36:01.035143+00:00, queued_by_job_id=41, pid=7372
[2026-02-13T11:36:05.485+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:35:59.119446+00:00: manual__2026-02-13T03:35:59.119446+00:00, state:running, queued_at: 2026-02-13 03:35:59.143100+00:00. externally triggered: True> successful
[2026-02-13T11:36:05.486+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:35:59.119446+00:00, run_id=manual__2026-02-13T03:35:59.119446+00:00, run_start_date=2026-02-13 03:36:01.007029+00:00, run_end_date=2026-02-13 03:36:05.486652+00:00, run_duration=4.479623, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:35:59.119446+00:00, data_interval_end=2026-02-13 03:35:59.119446+00:00, dag_hash=292d5ad65135765608fc09000942b435
[2026-02-13T11:36:05.495+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>
[2026-02-13T11:36:05.496+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:36:05.497+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>
[2026-02-13T11:36:05.500+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:36:05.501+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:36:04.836355+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:36:05.503+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:36:04.836355+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:36:05.505+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:36:04.836355+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:36:07.919+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:36:08.704+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:36:09.727+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:36:04.836355+00:00', try_number=1, map_index=-1)
[2026-02-13T11:36:09.739+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:36:04.836355+00:00, map_index=-1, run_start_date=2026-02-13 03:36:08.780611+00:00, run_end_date=2026-02-13 03:36:09.134619+00:00, run_duration=0.354008, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:36:05.498785+00:00, queued_by_job_id=41, pid=7374
[2026-02-13T11:36:12.405+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:36:04.836355+00:00: dataset_triggered__2026-02-13T03:36:04.836355+00:00, state:running, queued_at: 2026-02-13 03:36:05.455807+00:00. externally triggered: False> successful
[2026-02-13T11:36:12.406+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:36:04.836355+00:00, run_id=dataset_triggered__2026-02-13T03:36:04.836355+00:00, run_start_date=2026-02-13 03:36:05.471261+00:00, run_end_date=2026-02-13 03:36:12.406530+00:00, run_duration=6.935269, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:35:59.119446+00:00, data_interval_end=2026-02-13 03:35:59.119446+00:00, dag_hash=336021f9586175064690381be57e6c20
[2026-02-13T11:38:31.668+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:43:34.324+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:46:30.369+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>
[2026-02-13T11:46:30.370+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:46:30.371+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>
[2026-02-13T11:46:30.373+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:46:30.375+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:46:26.998824+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:46:30.375+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:46:26.998824+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:46:30.378+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:46:26.998824+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:46:32.528+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:46:33.649+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:46:34.819+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:46:26.998824+00:00', try_number=1, map_index=-1)
[2026-02-13T11:46:34.830+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:46:26.998824+00:00, map_index=-1, run_start_date=2026-02-13 03:46:33.729665+00:00, run_end_date=2026-02-13 03:46:34.216917+00:00, run_duration=0.487252, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:46:30.372500+00:00, queued_by_job_id=41, pid=7653
[2026-02-13T11:46:34.916+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:46:26.998824+00:00: manual__2026-02-13T03:46:26.998824+00:00, state:running, queued_at: 2026-02-13 03:46:27.011741+00:00. externally triggered: True> successful
[2026-02-13T11:46:34.917+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:46:26.998824+00:00, run_id=manual__2026-02-13T03:46:26.998824+00:00, run_start_date=2026-02-13 03:46:30.350067+00:00, run_end_date=2026-02-13 03:46:34.917490+00:00, run_duration=4.567423, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:46:26.998824+00:00, data_interval_end=2026-02-13 03:46:26.998824+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T11:46:35.986+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>
[2026-02-13T11:46:35.987+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:46:35.988+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>
[2026-02-13T11:46:35.990+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:46:35.991+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:46:34.235716+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:46:35.992+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:46:34.235716+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:46:35.995+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:46:34.235716+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:46:38.066+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:46:38.680+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:46:39.588+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:46:34.235716+00:00', try_number=1, map_index=-1)
[2026-02-13T11:46:39.598+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:46:34.235716+00:00, map_index=-1, run_start_date=2026-02-13 03:46:38.748184+00:00, run_end_date=2026-02-13 03:46:39.042286+00:00, run_duration=0.294102, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:46:35.989011+00:00, queued_by_job_id=41, pid=7658
[2026-02-13T11:46:39.637+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:46:34.235716+00:00: dataset_triggered__2026-02-13T03:46:34.235716+00:00, state:running, queued_at: 2026-02-13 03:46:34.891969+00:00. externally triggered: False> successful
[2026-02-13T11:46:39.638+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:46:34.235716+00:00, run_id=dataset_triggered__2026-02-13T03:46:34.235716+00:00, run_start_date=2026-02-13 03:46:35.955545+00:00, run_end_date=2026-02-13 03:46:39.638248+00:00, run_duration=3.682703, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:46:26.998824+00:00, data_interval_end=2026-02-13 03:46:26.998824+00:00, dag_hash=4f5cee350ba129ab524aa1ccabba746a
[2026-02-13T11:48:34.351+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:53:34.384+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T11:58:14.988+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>
[2026-02-13T11:58:14.990+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T11:58:14.990+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>
[2026-02-13T11:58:14.993+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:58:14.995+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:58:13.633239+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:58:14.996+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:58:13.633239+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:58:14.998+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:58:13.633239+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:58:17.086+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:58:18.164+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:58:19.405+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:58:13.633239+00:00', try_number=1, map_index=-1)
[2026-02-13T11:58:19.413+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:58:13.633239+00:00, map_index=-1, run_start_date=2026-02-13 03:58:18.240372+00:00, run_end_date=2026-02-13 03:58:18.829312+00:00, run_duration=0.58894, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:58:14.992155+00:00, queued_by_job_id=41, pid=7917
[2026-02-13T11:58:19.491+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:58:13.633239+00:00: manual__2026-02-13T03:58:13.633239+00:00, state:running, queued_at: 2026-02-13 03:58:13.650127+00:00. externally triggered: True> successful
[2026-02-13T11:58:19.492+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:58:13.633239+00:00, run_id=manual__2026-02-13T03:58:13.633239+00:00, run_start_date=2026-02-13 03:58:14.959849+00:00, run_end_date=2026-02-13 03:58:19.492612+00:00, run_duration=4.532763, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:58:13.633239+00:00, data_interval_end=2026-02-13 03:58:13.633239+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T11:58:19.502+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>
[2026-02-13T11:58:19.503+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T11:58:19.504+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>
[2026-02-13T11:58:19.506+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T11:58:19.507+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:58:18.846380+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T11:58:19.508+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:58:18.846380+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:58:19.510+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:58:18.846380+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T11:58:21.741+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T11:58:22.401+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [queued]> on host localhost-2.local
[2026-02-13T11:58:23.280+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:58:18.846380+00:00', try_number=1, map_index=-1)
[2026-02-13T11:58:23.290+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:58:18.846380+00:00, map_index=-1, run_start_date=2026-02-13 03:58:22.470670+00:00, run_end_date=2026-02-13 03:58:22.760214+00:00, run_duration=0.289544, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:58:19.505294+00:00, queued_by_job_id=41, pid=7928
[2026-02-13T11:58:25.935+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:58:18.846380+00:00: dataset_triggered__2026-02-13T03:58:18.846380+00:00, state:running, queued_at: 2026-02-13 03:58:19.464908+00:00. externally triggered: False> failed
[2026-02-13T11:58:25.937+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:58:18.846380+00:00, run_id=dataset_triggered__2026-02-13T03:58:18.846380+00:00, run_start_date=2026-02-13 03:58:19.478973+00:00, run_end_date=2026-02-13 03:58:25.937314+00:00, run_duration=6.458341, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:58:13.633239+00:00, data_interval_end=2026-02-13 03:58:13.633239+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
[2026-02-13T11:58:36.153+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-13T12:00:37.867+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>
[2026-02-13T12:00:37.869+0800] {scheduler_job_runner.py:507} INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
[2026-02-13T12:00:37.870+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>
[2026-02-13T12:00:37.873+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T12:00:37.874+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T04:00:36.991126+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T12:00:37.875+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T04:00:36.991126+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T12:00:37.877+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T04:00:36.991126+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T12:00:40.524+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T12:00:41.310+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [queued]> on host localhost-2.local
[2026-02-13T12:00:42.458+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T04:00:36.991126+00:00', try_number=1, map_index=-1)
[2026-02-13T12:00:42.467+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T04:00:36.991126+00:00, map_index=-1, run_start_date=2026-02-13 04:00:41.386106+00:00, run_end_date=2026-02-13 04:00:41.894495+00:00, run_duration=0.508389, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 04:00:37.871705+00:00, queued_by_job_id=41, pid=7993
[2026-02-13T12:00:44.981+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 04:00:36.991126+00:00: manual__2026-02-13T04:00:36.991126+00:00, state:running, queued_at: 2026-02-13 04:00:37.010364+00:00. externally triggered: True> successful
[2026-02-13T12:00:44.982+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 04:00:36.991126+00:00, run_id=manual__2026-02-13T04:00:36.991126+00:00, run_start_date=2026-02-13 04:00:37.845866+00:00, run_end_date=2026-02-13 04:00:44.982304+00:00, run_duration=7.136438, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 04:00:36.991126+00:00, data_interval_end=2026-02-13 04:00:36.991126+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
[2026-02-13T12:00:44.992+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>
[2026-02-13T12:00:44.993+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
[2026-02-13T12:00:44.994+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>
[2026-02-13T12:00:44.997+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-13T12:00:44.998+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T04:00:41.913811+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-13T12:00:44.999+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T04:00:41.913811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T12:00:45.002+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T04:00:41.913811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
[2026-02-13T12:00:47.144+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/dataset_demo.py
[2026-02-13T12:00:47.751+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [queued]> on host localhost-2.local
[2026-02-13T12:00:48.637+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T04:00:41.913811+00:00', try_number=1, map_index=-1)
[2026-02-13T12:00:48.646+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T04:00:41.913811+00:00, map_index=-1, run_start_date=2026-02-13 04:00:47.820152+00:00, run_end_date=2026-02-13 04:00:48.130048+00:00, run_duration=0.309896, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 04:00:44.995486+00:00, queued_by_job_id=41, pid=7996
[2026-02-13T12:00:51.386+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 04:00:41.913811+00:00: dataset_triggered__2026-02-13T04:00:41.913811+00:00, state:running, queued_at: 2026-02-13 04:00:44.951380+00:00. externally triggered: False> successful
[2026-02-13T12:00:51.388+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 04:00:41.913811+00:00, run_id=dataset_triggered__2026-02-13T04:00:41.913811+00:00, run_start_date=2026-02-13 04:00:44.966340+00:00, run_end_date=2026-02-13 04:00:51.387965+00:00, run_duration=6.421625, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 04:00:36.991126+00:00, data_interval_end=2026-02-13 04:00:36.991126+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
