[2026-02-14T11:07:57.862+0800] {executor_loader.py:258} INFO - Loaded executor: SequentialExecutor
[2026-02-14T11:07:58.408+0800] {scheduler_job_runner.py:950} INFO - Starting the scheduler
[2026-02-14T11:07:58.409+0800] {scheduler_job_runner.py:957} INFO - Processing each file at most -1 times
[2026-02-14T11:07:58.416+0800] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 1047
[2026-02-14T11:07:58.420+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:08:00.236+0800] {settings.py:63} INFO - Configured default timezone Asia/Shanghai
[2026-02-14T11:08:00.283+0800] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2026-02-14T11:08:00.542+0800] {core.py:50} INFO - Starting log server on http://[::]:8793
[2026-02-14T11:13:00.155+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:18:00.182+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:23:00.301+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:28:00.470+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:32:47.572+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>
[2026-02-14T11:32:47.573+0800] {scheduler_job_runner.py:507} INFO - DAG producer_1_N_sync has 0/16 running and queued tasks
[2026-02-14T11:32:47.574+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>
[2026-02-14T11:32:47.577+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:32:47.577+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_1_N_sync', task_id='producer_1_N_save_data', run_id='manual__2026-02-14T03:32:46.003371+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:32:47.578+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1_N_sync', 'producer_1_N_save_data', 'manual__2026-02-14T03:32:46.003371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:32:47.581+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1_N_sync', 'producer_1_N_save_data', 'manual__2026-02-14T03:32:46.003371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:32:49.920+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer.py
[2026-02-14T11:32:50.632+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:32:51.778+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1_N_sync', task_id='producer_1_N_save_data', run_id='manual__2026-02-14T03:32:46.003371+00:00', try_number=1, map_index=-1)
[2026-02-14T11:32:51.791+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_1_N_sync, task_id=producer_1_N_save_data, run_id=manual__2026-02-14T03:32:46.003371+00:00, map_index=-1, run_start_date=2026-02-14 03:32:50.698525+00:00, run_end_date=2026-02-14 03:32:51.091467+00:00, run_duration=0.392942, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=106, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:47.575365+00:00, queued_by_job_id=105, pid=2428
[2026-02-14T11:32:51.826+0800] {manager.py:537} INFO - DAG producer_dw_user_sync is missing and will be deactivated.
[2026-02-14T11:32:51.831+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-14T11:32:51.833+0800] {manager.py:553} INFO - Deleted DAG producer_dw_user_sync in serialized_dag table
[2026-02-14T11:32:54.743+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_1_N_sync @ 2026-02-14 03:32:46.003371+00:00: manual__2026-02-14T03:32:46.003371+00:00, state:running, queued_at: 2026-02-14 03:32:46.030701+00:00. externally triggered: True> successful
[2026-02-14T11:32:54.745+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_1_N_sync, execution_date=2026-02-14 03:32:46.003371+00:00, run_id=manual__2026-02-14T03:32:46.003371+00:00, run_start_date=2026-02-14 03:32:47.531979+00:00, run_end_date=2026-02-14 03:32:54.745296+00:00, run_duration=7.213317, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=9c94ea8be6439fe2a4c875752cf2dfbb
[2026-02-14T11:32:54.756+0800] {scheduler_job_runner.py:435} INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>
	<TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>
[2026-02-14T11:32:54.757+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1_N_stat_order_data has 0/16 running and queued tasks
[2026-02-14T11:32:54.757+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_1_N_clean_order_data has 0/16 running and queued tasks
[2026-02-14T11:32:54.758+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>
	<TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>
[2026-02-14T11:32:54.762+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>, <TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:32:54.763+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1_N_stat_order_data', task_id='consumer_1_N_read_stat_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.110002+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:32:54.763+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1_N_stat_order_data', 'consumer_1_N_read_stat_order_data', 'dataset_triggered__2026-02-14T03:32:51.110002+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:32:54.764+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_1_N_clean_order_data', task_id='consumer_1_N_read_clean_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.112812+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:32:54.765+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_data', 'consumer_1_N_read_clean_order_data', 'dataset_triggered__2026-02-14T03:32:51.112812+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:32:54.767+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1_N_stat_order_data', 'consumer_1_N_read_stat_order_data', 'dataset_triggered__2026-02-14T03:32:51.110002+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:32:56.740+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer.py
[2026-02-14T11:32:57.329+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:32:58.392+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_data', 'consumer_1_N_read_clean_order_data', 'dataset_triggered__2026-02-14T03:32:51.112812+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
[2026-02-14T11:33:00.745+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /1_producer_N_consumer.py
[2026-02-14T11:33:01.395+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:33:02.475+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1_N_stat_order_data', task_id='consumer_1_N_read_stat_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.110002+00:00', try_number=1, map_index=-1)
[2026-02-14T11:33:02.477+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1_N_clean_order_data', task_id='consumer_1_N_read_clean_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.112812+00:00', try_number=1, map_index=-1)
[2026-02-14T11:33:02.491+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1_N_stat_order_data, task_id=consumer_1_N_read_stat_order_data, run_id=dataset_triggered__2026-02-14T03:32:51.110002+00:00, map_index=-1, run_start_date=2026-02-14 03:32:57.388387+00:00, run_end_date=2026-02-14 03:32:57.754514+00:00, run_duration=0.366127, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:54.760069+00:00, queued_by_job_id=105, pid=2432
[2026-02-14T11:33:02.492+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_1_N_clean_order_data, task_id=consumer_1_N_read_clean_order_data, run_id=dataset_triggered__2026-02-14T03:32:51.112812+00:00, map_index=-1, run_start_date=2026-02-14 03:33:01.472039+00:00, run_end_date=2026-02-14 03:33:01.863441+00:00, run_duration=0.391402, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=108, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:54.760069+00:00, queued_by_job_id=105, pid=2437
[2026-02-14T11:33:02.523+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:33:05.158+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1_N_stat_order_data @ 2026-02-14 03:32:51.110002+00:00: dataset_triggered__2026-02-14T03:32:51.110002+00:00, state:running, queued_at: 2026-02-14 03:32:54.715732+00:00. externally triggered: False> successful
[2026-02-14T11:33:05.159+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1_N_stat_order_data, execution_date=2026-02-14 03:32:51.110002+00:00, run_id=dataset_triggered__2026-02-14T03:32:51.110002+00:00, run_start_date=2026-02-14 03:32:54.727529+00:00, run_end_date=2026-02-14 03:33:05.158995+00:00, run_duration=10.431466, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=94f55b14533c43cd663b6f903564f319
[2026-02-14T11:33:05.164+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_1_N_clean_order_data @ 2026-02-14 03:32:51.112812+00:00: dataset_triggered__2026-02-14T03:32:51.112812+00:00, state:running, queued_at: 2026-02-14 03:32:54.699044+00:00. externally triggered: False> successful
[2026-02-14T11:33:05.165+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_1_N_clean_order_data, execution_date=2026-02-14 03:32:51.112812+00:00, run_id=dataset_triggered__2026-02-14T03:32:51.112812+00:00, run_start_date=2026-02-14 03:32:54.727628+00:00, run_end_date=2026-02-14 03:33:05.165330+00:00, run_duration=10.437702, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=c61c84e3308591cb31098ffc7795ade0
[2026-02-14T11:35:57.938+0800] {manager.py:537} INFO - DAG consumer_order_user_analysis is missing and will be deactivated.
[2026-02-14T11:35:57.941+0800] {manager.py:549} INFO - Deactivated 1 DAGs which are no longer present in file.
[2026-02-14T11:35:57.943+0800] {manager.py:553} INFO - Deleted DAG consumer_order_user_analysis in serialized_dag table
[2026-02-14T11:36:03.831+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>
[2026-02-14T11:36:03.832+0800] {scheduler_job_runner.py:507} INFO - DAG producer_N_1_dw_order_sync has 0/16 running and queued tasks
[2026-02-14T11:36:03.832+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>
[2026-02-14T11:36:03.835+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:36:03.836+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:36:00.776536+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:36:03.836+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:36:00.776536+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:03.839+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:36:00.776536+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:05.834+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:36:06.748+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:36:07.738+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:36:00.776536+00:00', try_number=1, map_index=-1)
[2026-02-14T11:36:07.752+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_N_1_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-14T03:36:00.776536+00:00, map_index=-1, run_start_date=2026-02-14 03:36:06.815869+00:00, run_end_date=2026-02-14 03:36:07.204195+00:00, run_duration=0.388326, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:03.833916+00:00, queued_by_job_id=105, pid=2607
[2026-02-14T11:36:10.397+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_N_1_dw_order_sync @ 2026-02-14 03:36:00.776536+00:00: manual__2026-02-14T03:36:00.776536+00:00, state:running, queued_at: 2026-02-14 03:36:00.799507+00:00. externally triggered: True> successful
[2026-02-14T11:36:10.398+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_N_1_dw_order_sync, execution_date=2026-02-14 03:36:00.776536+00:00, run_id=manual__2026-02-14T03:36:00.776536+00:00, run_start_date=2026-02-14 03:36:03.811605+00:00, run_end_date=2026-02-14 03:36:10.398453+00:00, run_duration=6.586848, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:36:00.776536+00:00, data_interval_end=2026-02-14 03:36:00.776536+00:00, dag_hash=c52fa1a388ba1be81c6c42bf2d418436
[2026-02-14T11:36:24.211+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>
[2026-02-14T11:36:24.213+0800] {scheduler_job_runner.py:507} INFO - DAG producer_N_1_dw_user_sync has 0/16 running and queued tasks
[2026-02-14T11:36:24.213+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>
[2026-02-14T11:36:24.216+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:36:24.217+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:36:20.021965+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:36:24.218+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:36:20.021965+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:24.220+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:36:20.021965+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:26.218+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:36:27.045+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:36:28.110+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:36:20.021965+00:00', try_number=1, map_index=-1)
[2026-02-14T11:36:28.123+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_N_1_dw_user_sync, task_id=sync_user_data, run_id=manual__2026-02-14T03:36:20.021965+00:00, map_index=-1, run_start_date=2026-02-14 03:36:27.111353+00:00, run_end_date=2026-02-14 03:36:27.495680+00:00, run_duration=0.384327, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=110, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:24.214954+00:00, queued_by_job_id=105, pid=2616
[2026-02-14T11:36:30.604+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_N_1_dw_user_sync @ 2026-02-14 03:36:20.021965+00:00: manual__2026-02-14T03:36:20.021965+00:00, state:running, queued_at: 2026-02-14 03:36:20.035784+00:00. externally triggered: True> successful
[2026-02-14T11:36:30.605+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_N_1_dw_user_sync, execution_date=2026-02-14 03:36:20.021965+00:00, run_id=manual__2026-02-14T03:36:20.021965+00:00, run_start_date=2026-02-14 03:36:24.193164+00:00, run_end_date=2026-02-14 03:36:30.605369+00:00, run_duration=6.412205, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:36:20.021965+00:00, data_interval_end=2026-02-14 03:36:20.021965+00:00, dag_hash=f21aec43673b2c968a2fc26e9e7f5cea
[2026-02-14T11:36:30.614+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>
[2026-02-14T11:36:30.615+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_N_1_order_user_analysis has 0/16 running and queued tasks
[2026-02-14T11:36:30.615+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>
[2026-02-14T11:36:30.618+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:36:30.619+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:36:27.516973+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:36:30.619+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:36:27.516973+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:30.622+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:36:27.516973+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:36:32.568+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:36:33.182+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:36:34.161+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:36:27.516973+00:00', try_number=1, map_index=-1)
[2026-02-14T11:36:34.171+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_N_1_order_user_analysis, task_id=analyze_order_user, run_id=dataset_triggered__2026-02-14T03:36:27.516973+00:00, map_index=-1, run_start_date=2026-02-14 03:36:33.250171+00:00, run_end_date=2026-02-14 03:36:33.631813+00:00, run_duration=0.381642, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=111, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:30.616775+00:00, queued_by_job_id=105, pid=2620
[2026-02-14T11:36:36.612+0800] {dagrun.py:823} ERROR - Marking run <DagRun consumer_N_1_order_user_analysis @ 2026-02-14 03:36:27.516973+00:00: dataset_triggered__2026-02-14T03:36:27.516973+00:00, state:running, queued_at: 2026-02-14 03:36:30.577081+00:00. externally triggered: False> failed
[2026-02-14T11:36:36.613+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_N_1_order_user_analysis, execution_date=2026-02-14 03:36:27.516973+00:00, run_id=dataset_triggered__2026-02-14T03:36:27.516973+00:00, run_start_date=2026-02-14 03:36:30.591444+00:00, run_end_date=2026-02-14 03:36:36.613428+00:00, run_duration=6.021984, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:36:00.776536+00:00, data_interval_end=2026-02-14 03:36:20.021965+00:00, dag_hash=6baf9b26ce8f026143cfbb65a8bc4568
[2026-02-14T11:38:03.284+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:38:57.771+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>
[2026-02-14T11:38:57.772+0800] {scheduler_job_runner.py:507} INFO - DAG producer_N_1_dw_order_sync has 0/16 running and queued tasks
[2026-02-14T11:38:57.773+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>
[2026-02-14T11:38:57.777+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:38:57.778+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:38:53.631710+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:38:57.779+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:38:53.631710+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:38:57.782+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:38:53.631710+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:39:00.228+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:39:01.199+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:39:02.244+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:38:53.631710+00:00', try_number=1, map_index=-1)
[2026-02-14T11:39:02.255+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_N_1_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-14T03:38:53.631710+00:00, map_index=-1, run_start_date=2026-02-14 03:39:01.278348+00:00, run_end_date=2026-02-14 03:39:01.646406+00:00, run_duration=0.368058, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=112, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:38:57.775611+00:00, queued_by_job_id=105, pid=2724
[2026-02-14T11:39:04.761+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_N_1_dw_order_sync @ 2026-02-14 03:38:53.631710+00:00: manual__2026-02-14T03:38:53.631710+00:00, state:running, queued_at: 2026-02-14 03:38:53.643631+00:00. externally triggered: True> successful
[2026-02-14T11:39:04.762+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_N_1_dw_order_sync, execution_date=2026-02-14 03:38:53.631710+00:00, run_id=manual__2026-02-14T03:38:53.631710+00:00, run_start_date=2026-02-14 03:38:57.749214+00:00, run_end_date=2026-02-14 03:39:04.762095+00:00, run_duration=7.012881, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:38:53.631710+00:00, data_interval_end=2026-02-14 03:38:53.631710+00:00, dag_hash=c52fa1a388ba1be81c6c42bf2d418436
[2026-02-14T11:39:04.771+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>
[2026-02-14T11:39:04.772+0800] {scheduler_job_runner.py:507} INFO - DAG producer_N_1_dw_user_sync has 0/16 running and queued tasks
[2026-02-14T11:39:04.773+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>
[2026-02-14T11:39:04.775+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:39:04.777+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:38:57.131057+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:39:04.778+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:38:57.131057+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:39:04.780+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:38:57.131057+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:39:06.767+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:39:07.330+0800] {task_command.py:467} INFO - Running <TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:39:08.354+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:38:57.131057+00:00', try_number=1, map_index=-1)
[2026-02-14T11:39:08.364+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=producer_N_1_dw_user_sync, task_id=sync_user_data, run_id=manual__2026-02-14T03:38:57.131057+00:00, map_index=-1, run_start_date=2026-02-14 03:39:07.398305+00:00, run_end_date=2026-02-14 03:39:07.752752+00:00, run_duration=0.354447, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=113, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:39:04.774486+00:00, queued_by_job_id=105, pid=2732
[2026-02-14T11:39:10.872+0800] {dagrun.py:854} INFO - Marking run <DagRun producer_N_1_dw_user_sync @ 2026-02-14 03:38:57.131057+00:00: manual__2026-02-14T03:38:57.131057+00:00, state:running, queued_at: 2026-02-14 03:38:57.144678+00:00. externally triggered: True> successful
[2026-02-14T11:39:10.873+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=producer_N_1_dw_user_sync, execution_date=2026-02-14 03:38:57.131057+00:00, run_id=manual__2026-02-14T03:38:57.131057+00:00, run_start_date=2026-02-14 03:39:04.743265+00:00, run_end_date=2026-02-14 03:39:10.873656+00:00, run_duration=6.130391, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:38:57.131057+00:00, data_interval_end=2026-02-14 03:38:57.131057+00:00, dag_hash=f21aec43673b2c968a2fc26e9e7f5cea
[2026-02-14T11:39:10.884+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>
[2026-02-14T11:39:10.885+0800] {scheduler_job_runner.py:507} INFO - DAG consumer_N_1_order_user_analysis has 0/16 running and queued tasks
[2026-02-14T11:39:10.886+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>
[2026-02-14T11:39:10.888+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T11:39:10.890+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:39:07.770729+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
[2026-02-14T11:39:10.890+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:39:07.770729+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:39:10.894+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:39:07.770729+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
[2026-02-14T11:39:12.878+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/producer_consumer /N_producer_1_consumer.py
[2026-02-14T11:39:13.439+0800] {task_command.py:467} INFO - Running <TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [queued]> on host localhost-2.local
[2026-02-14T11:39:14.462+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:39:07.770729+00:00', try_number=1, map_index=-1)
[2026-02-14T11:39:14.473+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=consumer_N_1_order_user_analysis, task_id=analyze_order_user, run_id=dataset_triggered__2026-02-14T03:39:07.770729+00:00, map_index=-1, run_start_date=2026-02-14 03:39:13.505551+00:00, run_end_date=2026-02-14 03:39:13.874952+00:00, run_duration=0.369401, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=114, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:39:10.887510+00:00, queued_by_job_id=105, pid=2736
[2026-02-14T11:39:17.035+0800] {dagrun.py:854} INFO - Marking run <DagRun consumer_N_1_order_user_analysis @ 2026-02-14 03:39:07.770729+00:00: dataset_triggered__2026-02-14T03:39:07.770729+00:00, state:running, queued_at: 2026-02-14 03:39:10.836944+00:00. externally triggered: False> successful
[2026-02-14T11:39:17.036+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=consumer_N_1_order_user_analysis, execution_date=2026-02-14 03:39:07.770729+00:00, run_id=dataset_triggered__2026-02-14T03:39:07.770729+00:00, run_start_date=2026-02-14 03:39:10.858349+00:00, run_end_date=2026-02-14 03:39:17.036580+00:00, run_duration=6.178231, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:38:53.631710+00:00, data_interval_end=2026-02-14 03:38:57.131057+00:00, dag_hash=6baf9b26ce8f026143cfbb65a8bc4568
[2026-02-14T11:43:05.661+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:49:34.163+0800] {job.py:229} INFO - Heartbeat recovered after 255.48 seconds
[2026-02-14T11:49:34.470+0800] {manager.py:537} INFO - DAG consumer_dw_order_clean is missing and will be deactivated.
[2026-02-14T11:49:34.471+0800] {manager.py:537} INFO - DAG producer_dw_order_sync is missing and will be deactivated.
[2026-02-14T11:49:34.471+0800] {manager.py:537} INFO - DAG consumer_dw_order_stat is missing and will be deactivated.
[2026-02-14T11:49:34.475+0800] {manager.py:549} INFO - Deactivated 3 DAGs which are no longer present in file.
[2026-02-14T11:49:34.478+0800] {manager.py:553} INFO - Deleted DAG consumer_dw_order_stat in serialized_dag table
[2026-02-14T11:49:34.483+0800] {manager.py:553} INFO - Deleted DAG producer_dw_order_sync in serialized_dag table
[2026-02-14T11:49:34.487+0800] {manager.py:553} INFO - Deleted DAG consumer_dw_order_clean in serialized_dag table
[2026-02-14T11:52:16.522+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T11:57:19.028+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:02:19.963+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:07:21.610+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:12:21.762+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:17:21.998+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:22:22.287+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:27:24.316+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:32:26.676+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:44:02.216+0800] {job.py:229} INFO - Heartbeat recovered after 568.58 seconds
[2026-02-14T12:48:32.153+0800] {job.py:229} INFO - Heartbeat recovered after 234.68 seconds
[2026-02-14T12:48:33.179+0800] {manager.py:537} INFO - DAG consumer_dw_order_clean is missing and will be deactivated.
[2026-02-14T12:48:33.180+0800] {manager.py:537} INFO - DAG producer_dw_order_sync is missing and will be deactivated.
[2026-02-14T12:48:33.180+0800] {manager.py:537} INFO - DAG consumer_dw_order_stat is missing and will be deactivated.
[2026-02-14T12:48:33.182+0800] {manager.py:549} INFO - Deactivated 3 DAGs which are no longer present in file.
[2026-02-14T12:48:33.184+0800] {manager.py:553} INFO - Deleted DAG consumer_dw_order_stat in serialized_dag table
[2026-02-14T12:48:33.187+0800] {manager.py:553} INFO - Deleted DAG producer_dw_order_sync in serialized_dag table
[2026-02-14T12:48:33.191+0800] {manager.py:553} INFO - Deleted DAG consumer_dw_order_clean in serialized_dag table
[2026-02-14T12:50:41.958+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T12:55:43.868+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T13:01:11.138+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T13:06:15.550+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T13:09:12.928+0800] {dag.py:4180} INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 05:00:00+00:00, run_after=2026-02-14 06:00:00+00:00
[2026-02-14T13:09:16.251+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
[2026-02-14T13:09:16.253+0800] {scheduler_job_runner.py:507} INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
[2026-02-14T13:09:16.254+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
[2026-02-14T13:09:16.257+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T13:09:16.258+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2026-02-14T13:09:16.258+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:16.261+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:18.663+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/分支任务动态下游/branchDAG.py
[2026-02-14T13:09:19.242+0800] {task_command.py:467} INFO - Running <TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [queued]> on host localhost-2.local
[2026-02-14T13:09:20.291+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1)
[2026-02-14T13:09:20.301+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=scheduled__2026-02-14T04:00:00+00:00, map_index=-1, run_start_date=2026-02-14 05:09:19.313693+00:00, run_end_date=2026-02-14 05:09:19.708239+00:00, run_duration=0.394546, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=115, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 05:09:16.255557+00:00, queued_by_job_id=105, pid=5146
[2026-02-14T13:09:23.142+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
[2026-02-14T13:09:23.143+0800] {scheduler_job_runner.py:507} INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
[2026-02-14T13:09:23.143+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
[2026-02-14T13:09:23.146+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T13:09:23.147+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2026-02-14T13:09:23.148+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:23.150+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:25.271+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/分支任务动态下游/branchDAG.py
[2026-02-14T13:09:25.858+0800] {task_command.py:467} INFO - Running <TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [queued]> on host localhost-2.local
[2026-02-14T13:09:26.861+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1)
[2026-02-14T13:09:26.871+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=scheduled__2026-02-14T04:00:00+00:00, map_index=-1, run_start_date=2026-02-14 05:09:25.932846+00:00, run_end_date=2026-02-14 05:09:26.284209+00:00, run_duration=0.351363, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=116, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 05:09:23.144899+00:00, queued_by_job_id=105, pid=5149
[2026-02-14T13:09:29.730+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
[2026-02-14T13:09:29.731+0800] {scheduler_job_runner.py:507} INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
[2026-02-14T13:09:29.731+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
[2026-02-14T13:09:29.734+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T13:09:29.735+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
[2026-02-14T13:09:29.736+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:29.738+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:31.782+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/分支任务动态下游/branchDAG.py
[2026-02-14T13:09:32.330+0800] {task_command.py:467} INFO - Running <TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [queued]> on host localhost-2.local
[2026-02-14T13:09:33.343+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1)
[2026-02-14T13:09:33.353+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=manual__2026-02-14T05:09:16.552321+00:00, map_index=-1, run_start_date=2026-02-14 05:09:32.398471+00:00, run_end_date=2026-02-14 05:09:32.757500+00:00, run_duration=0.359029, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=117, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 05:09:29.732802+00:00, queued_by_job_id=105, pid=5152
[2026-02-14T13:09:36.117+0800] {dagrun.py:854} INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 04:00:00+00:00: scheduled__2026-02-14T04:00:00+00:00, state:running, queued_at: 2026-02-14 05:09:12.917691+00:00. externally triggered: False> successful
[2026-02-14T13:09:36.119+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 04:00:00+00:00, run_id=scheduled__2026-02-14T04:00:00+00:00, run_start_date=2026-02-14 05:09:12.942178+00:00, run_end_date=2026-02-14 05:09:36.119027+00:00, run_duration=23.176849, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-14 04:00:00+00:00, data_interval_end=2026-02-14 05:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
[2026-02-14T13:09:36.126+0800] {dag.py:4180} INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 05:00:00+00:00, run_after=2026-02-14 06:00:00+00:00
[2026-02-14T13:09:36.139+0800] {scheduler_job_runner.py:435} INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
[2026-02-14T13:09:36.140+0800] {scheduler_job_runner.py:507} INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
[2026-02-14T13:09:36.140+0800] {scheduler_job_runner.py:646} INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
[2026-02-14T13:09:36.143+0800] {scheduler_job_runner.py:748} INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
[2026-02-14T13:09:36.145+0800] {scheduler_job_runner.py:692} INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
[2026-02-14T13:09:36.145+0800] {base_executor.py:169} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:36.148+0800] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER/分支任务动态下游/branchDAG.py']
[2026-02-14T13:09:38.179+0800] {dagbag.py:588} INFO - Filling up the DagBag from /Users/linghuchong/Downloads/51/Python/project/instance/airflow284/dags/分支任务动态下游/branchDAG.py
[2026-02-14T13:09:38.726+0800] {task_command.py:467} INFO - Running <TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [queued]> on host localhost-2.local
[2026-02-14T13:09:39.699+0800] {scheduler_job_runner.py:776} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1)
[2026-02-14T13:09:39.710+0800] {scheduler_job_runner.py:813} INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=manual__2026-02-14T05:09:16.552321+00:00, map_index=-1, run_start_date=2026-02-14 05:09:38.799845+00:00, run_end_date=2026-02-14 05:09:39.154774+00:00, run_duration=0.354929, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=118, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 05:09:36.141855+00:00, queued_by_job_id=105, pid=5155
[2026-02-14T13:09:46.088+0800] {dagrun.py:854} INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 05:09:16.552321+00:00: manual__2026-02-14T05:09:16.552321+00:00, state:running, queued_at: 2026-02-14 05:09:16.580469+00:00. externally triggered: True> successful
[2026-02-14T13:09:46.089+0800] {dagrun.py:905} INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 05:09:16.552321+00:00, run_id=manual__2026-02-14T05:09:16.552321+00:00, run_start_date=2026-02-14 05:09:23.107910+00:00, run_end_date=2026-02-14 05:09:46.089112+00:00, run_duration=22.981202, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 04:00:00+00:00, data_interval_end=2026-02-14 05:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
[2026-02-14T13:11:18.109+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
[2026-02-14T13:16:19.692+0800] {scheduler_job_runner.py:1949} INFO - Adopting or resetting orphaned tasks for active dag runs
