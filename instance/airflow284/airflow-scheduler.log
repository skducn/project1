2026-02-12 21:17:30,021 INFO - Task context logging is enabled
2026-02-12 21:17:30,023 INFO - Loaded executor: SequentialExecutor
2026-02-12 21:17:30,086 INFO - Starting the scheduler
2026-02-12 21:17:30,087 INFO - Processing each file at most -1 times
2026-02-12 21:17:30,093 INFO - Launched DagFileProcessorManager with pid: 26882
2026-02-12 21:17:30,098 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-12 21:17:30,164 INFO - DAG dw_order_sync is at (or above) max_active_runs (1 of 1), not creating any more runs
2026-02-12 21:17:30,223 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:30,224 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:17:30,224 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:30,227 WARNING - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved
2026-02-12 21:17:30,228 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2026-02-12 21:17:30,228 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:30,230 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:32,962 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:17:32,984 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:17:32.325739+00:00, run_end_date=2026-02-12 13:17:32.514673+00:00, run_duration=0.188934, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-12 13:17:30.225672+00:00, queued_by_job_id=6, pid=26888
2026-02-12 21:17:33,036 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:33,037 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:17:33,038 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:33,040 WARNING - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved
2026-02-12 21:17:33,042 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2026-02-12 21:17:33,044 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:33,046 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:35,973 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:17:35,983 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:17:35.106136+00:00, run_end_date=2026-02-12 13:17:35.558556+00:00, run_duration=0.45242, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-12 13:17:33.039501+00:00, queued_by_job_id=6, pid=26895
2026-02-12 21:17:36,042 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:36,043 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:17:36,044 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:17:36,047 WARNING - cannot record scheduled_duration for task run_automation_test2 because previous state change time has not been saved
2026-02-12 21:17:36,048 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2026-02-12 21:17:36,049 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:36,050 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:17:38,987 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:17:38,996 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:17:38.197138+00:00, run_end_date=2026-02-12 13:17:38.575304+00:00, run_duration=0.378166, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-12 13:17:36.045714+00:00, queued_by_job_id=6, pid=26903
2026-02-12 21:17:39,027 INFO - Marking run <DagRun dw_order_sync @ 2026-02-11 00:00:00+00:00: scheduled__2026-02-11T00:00:00+00:00, state:running, queued_at: 2026-02-12 13:17:30.153407+00:00. externally triggered: False> successful
2026-02-12 21:17:39,028 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 00:00:00+00:00, run_id=scheduled__2026-02-11T00:00:00+00:00, run_start_date=2026-02-12 13:17:30.177921+00:00, run_end_date=2026-02-12 13:17:39.028781+00:00, run_duration=8.85086, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-11 00:00:00+00:00, data_interval_end=2026-02-12 00:00:00+00:00, dag_hash=e351ac217fac2a089c8384eaffc0bd32
2026-02-12 21:17:39,035 INFO - Setting next_dagrun for dw_order_sync to 2026-02-12 00:00:00+00:00, run_after=2026-02-13 00:00:00+00:00
2026-02-12 21:19:25,070 INFO - Exiting gracefully upon receiving signal 15
2026-02-12 21:19:25,539 INFO - Sending 15 to group 26882. PIDs of all processes in the group: []
2026-02-12 21:19:25,540 INFO - Sending the signal 15 to group 26882
2026-02-12 21:19:25,540 INFO - Sending the signal 15 to process 26882 as process group is missing.
2026-02-12 21:19:25,552 INFO - Sending 15 to group 26882. PIDs of all processes in the group: []
2026-02-12 21:19:25,553 INFO - Sending the signal 15 to group 26882
2026-02-12 21:19:25,554 INFO - Sending the signal 15 to process 26882 as process group is missing.
2026-02-12 21:19:25,554 INFO - Exited execute loop
2026-02-12 21:19:45,682 INFO - Task context logging is enabled
2026-02-12 21:19:45,685 INFO - Loaded executor: SequentialExecutor
2026-02-12 21:19:45,750 INFO - Starting the scheduler
2026-02-12 21:19:45,751 INFO - Processing each file at most -1 times
2026-02-12 21:19:45,758 INFO - Launched DagFileProcessorManager with pid: 27056
2026-02-12 21:19:45,762 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-12 21:20:25,927 INFO - DAG dw_order_sync is at (or above) max_active_runs (1 of 1), not creating any more runs
2026-02-12 21:20:25,996 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:25,997 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:20:25,998 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:26,000 WARNING - cannot record scheduled_duration for task sync_order_data because previous state change time has not been saved
2026-02-12 21:20:26,002 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2026-02-12 21:20:26,003 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:26,006 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:29,161 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:20:29,174 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:20:28.520622+00:00, run_end_date=2026-02-12 13:20:28.719856+00:00, run_duration=0.199234, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-12 13:20:25.998848+00:00, queued_by_job_id=7, pid=27116
2026-02-12 21:20:29,226 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:29,227 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:20:29,228 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:29,230 WARNING - cannot record scheduled_duration for task run_automation_test because previous state change time has not been saved
2026-02-12 21:20:29,231 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2026-02-12 21:20:29,232 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:29,233 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:32,070 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:20:32,081 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:20:31.254210+00:00, run_end_date=2026-02-12 13:20:31.654600+00:00, run_duration=0.40039, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-12 13:20:29.229252+00:00, queued_by_job_id=7, pid=27122
2026-02-12 21:20:32,126 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:32,127 INFO - DAG dw_order_sync has 0/1 running and queued tasks
2026-02-12 21:20:32,127 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-11T00:00:00+00:00 [scheduled]>
2026-02-12 21:20:32,130 WARNING - cannot record scheduled_duration for task run_automation_test2 because previous state change time has not been saved
2026-02-12 21:20:32,131 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2026-02-12 21:20:32,132 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:32,134 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-11T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-12 21:20:35,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-11T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-12 21:20:35,176 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=scheduled__2026-02-11T00:00:00+00:00, map_index=-1, run_start_date=2026-02-12 13:20:34.352695+00:00, run_end_date=2026-02-12 13:20:34.740636+00:00, run_duration=0.387941, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-12 13:20:32.128735+00:00, queued_by_job_id=7, pid=27133
2026-02-12 21:20:35,216 INFO - Marking run <DagRun dw_order_sync @ 2026-02-11 00:00:00+00:00: scheduled__2026-02-11T00:00:00+00:00, state:running, queued_at: 2026-02-12 13:20:25.916977+00:00. externally triggered: False> successful
2026-02-12 21:20:35,218 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-11 00:00:00+00:00, run_id=scheduled__2026-02-11T00:00:00+00:00, run_start_date=2026-02-12 13:20:25.939835+00:00, run_end_date=2026-02-12 13:20:35.217933+00:00, run_duration=9.278098, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-11 00:00:00+00:00, data_interval_end=2026-02-12 00:00:00+00:00, dag_hash=e351ac217fac2a089c8384eaffc0bd32
2026-02-12 21:20:35,226 INFO - Setting next_dagrun for dw_order_sync to 2026-02-12 00:00:00+00:00, run_after=2026-02-13 00:00:00+00:00
2026-02-12 21:23:44,559 INFO - Exiting gracefully upon receiving signal 15
2026-02-12 21:23:45,001 INFO - Sending 15 to group 27056. PIDs of all processes in the group: []
2026-02-12 21:23:45,002 INFO - Sending the signal 15 to group 27056
2026-02-12 21:23:45,003 INFO - Sending the signal 15 to process 27056 as process group is missing.
2026-02-12 21:23:45,015 INFO - Sending 15 to group 27056. PIDs of all processes in the group: []
2026-02-12 21:23:45,016 INFO - Sending the signal 15 to group 27056
2026-02-12 21:23:45,016 INFO - Sending the signal 15 to process 27056 as process group is missing.
2026-02-12 21:23:45,017 INFO - Exited execute loop
2026-02-12 21:26:49,569 INFO - Task context logging is enabled
2026-02-12 21:26:49,572 INFO - Loaded executor: SequentialExecutor
2026-02-12 21:26:49,662 INFO - Starting the scheduler
2026-02-12 21:26:49,663 INFO - Processing each file at most -1 times
2026-02-12 21:26:49,673 INFO - Launched DagFileProcessorManager with pid: 27464
2026-02-12 21:26:49,678 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-12 21:27:37,587 INFO - Exiting gracefully upon receiving signal 15
2026-02-12 21:27:38,015 INFO - Sending 15 to group 27464. PIDs of all processes in the group: []
2026-02-12 21:27:38,016 INFO - Sending the signal 15 to group 27464
2026-02-12 21:27:38,017 INFO - Sending the signal 15 to process 27464 as process group is missing.
2026-02-12 21:27:38,028 INFO - Sending 15 to group 27464. PIDs of all processes in the group: []
2026-02-12 21:27:38,029 INFO - Sending the signal 15 to group 27464
2026-02-12 21:27:38,029 INFO - Sending the signal 15 to process 27464 as process group is missing.
2026-02-12 21:27:38,030 INFO - Exited execute loop
2026-02-12 21:27:41,429 INFO - Task context logging is enabled
2026-02-12 21:27:41,431 INFO - Loaded executor: SequentialExecutor
2026-02-12 21:27:41,511 INFO - Starting the scheduler
2026-02-12 21:27:41,513 INFO - Processing each file at most -1 times
2026-02-12 21:27:41,523 INFO - Launched DagFileProcessorManager with pid: 27544
2026-02-12 21:27:41,531 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-12 21:28:33,359 INFO - Exiting gracefully upon receiving signal 15
2026-02-12 21:28:33,811 INFO - Sending 15 to group 27544. PIDs of all processes in the group: []
2026-02-12 21:28:33,812 INFO - Sending the signal 15 to group 27544
2026-02-12 21:28:33,813 INFO - Sending the signal 15 to process 27544 as process group is missing.
2026-02-12 21:28:33,823 INFO - Sending 15 to group 27544. PIDs of all processes in the group: []
2026-02-12 21:28:33,824 INFO - Sending the signal 15 to group 27544
2026-02-12 21:28:33,824 INFO - Sending the signal 15 to process 27544 as process group is missing.
2026-02-12 21:28:33,825 INFO - Exited execute loop
2026-02-12 23:46:06,769 INFO - Loaded executor: SequentialExecutor
2026-02-12 23:52:57,159 INFO - Loaded executor: SequentialExecutor
2026-02-12 23:56:05,946 INFO - Loaded executor: SequentialExecutor
2026-02-13 00:06:42,462 INFO - Loaded executor: SequentialExecutor
2026-02-13 00:16:01,354 INFO - Loaded executor: SequentialExecutor
2026-02-13 00:35:48,895 INFO - Loaded executor: SequentialExecutor
2026-02-13 00:35:49,435 INFO - Starting the scheduler
2026-02-13 00:35:49,436 INFO - Processing each file at most -1 times
2026-02-13 00:35:49,442 INFO - Launched DagFileProcessorManager with pid: 3597
2026-02-13 00:35:49,447 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 00:42:12,985 INFO - Heartbeat recovered after 169.74 seconds
2026-02-13 00:43:05,883 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 00:43:06,886 INFO - Sending Signals.SIGTERM to group 3597. PIDs of all processes in the group: [3597]
2026-02-13 00:43:06,887 INFO - Sending the signal Signals.SIGTERM to group 3597
2026-02-13 00:43:06,903 INFO - Process psutil.Process(pid=3597, status='terminated', exitcode=<Negsignal.SIGTERM: -15>, started='00:35:49') (3597) terminated with exit code Negsignal.SIGTERM
2026-02-13 00:43:06,905 INFO - Sending Signals.SIGTERM to group 3597. PIDs of all processes in the group: []
2026-02-13 00:43:06,906 INFO - Sending the signal Signals.SIGTERM to group 3597
2026-02-13 00:43:06,907 INFO - Sending the signal Signals.SIGTERM to process 3597 as process group is missing.
2026-02-13 00:43:06,908 INFO - Exited execute loop
2026-02-13 08:12:37,398 INFO - Loaded executor: SequentialExecutor
2026-02-13 08:12:38,859 INFO - Starting the scheduler
2026-02-13 08:12:38,861 INFO - Processing each file at most -1 times
2026-02-13 08:12:38,877 INFO - Launched DagFileProcessorManager with pid: 1168
2026-02-13 08:12:38,885 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:12:49,995 INFO - Setting next_dagrun for dw_order_sync to 2026-02-13 00:00:00+00:00, run_after=2026-02-14 00:00:00+00:00
2026-02-13 08:12:50,338 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:12:50,339 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:12:50,340 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:12:50,344 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:12:50,346 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:12:50,348 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:12:50,350 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:12:55,572 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 08:12:55,592 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=scheduled__2026-02-12T00:00:00+00:00, map_index=-1, run_start_date=2026-02-13 00:12:54.275855+00:00, run_end_date=2026-02-13 00:12:54.689110+00:00, run_duration=0.413255, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:12:50.342111+00:00, queued_by_job_id=7, pid=1198
2026-02-13 08:12:55,683 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:12:55,684 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:12:55,685 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:12:55,688 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:12:55,689 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:12:55,690 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:12:55,693 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:13:27,391 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 08:13:27,401 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=scheduled__2026-02-12T00:00:00+00:00, map_index=-1, run_start_date=2026-02-13 00:12:59.657432+00:00, run_end_date=2026-02-13 00:13:26.757785+00:00, run_duration=27.100353, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:12:55.686573+00:00, queued_by_job_id=7, pid=1212
2026-02-13 08:13:27,423 INFO - Heartbeat recovered after 31.81 seconds
2026-02-13 08:13:30,110 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:13:30,111 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:13:30,112 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T00:00:00+00:00 [scheduled]>
2026-02-13 08:13:30,114 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T00:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:13:30,115 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:13:30,116 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:13:30,118 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-12T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:13:39,662 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-12T00:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 08:13:39,673 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=scheduled__2026-02-12T00:00:00+00:00, map_index=-1, run_start_date=2026-02-13 00:13:32.857265+00:00, run_end_date=2026-02-13 00:13:39.140010+00:00, run_duration=6.282745, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:13:30.113324+00:00, queued_by_job_id=7, pid=1281
2026-02-13 08:13:39,735 INFO - Marking run <DagRun dw_order_sync @ 2026-02-12 00:00:00+00:00: scheduled__2026-02-12T00:00:00+00:00, state:running, queued_at: 2026-02-13 00:12:49.964054+00:00. externally triggered: False> successful
2026-02-13 08:13:39,737 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-12 00:00:00+00:00, run_id=scheduled__2026-02-12T00:00:00+00:00, run_start_date=2026-02-13 00:12:50.025095+00:00, run_end_date=2026-02-13 00:13:39.737284+00:00, run_duration=49.712189, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-12 00:00:00+00:00, data_interval_end=2026-02-13 00:00:00+00:00, dag_hash=340b2787125831c612b39fbc22c30471
2026-02-13 08:13:39,742 INFO - Setting next_dagrun for dw_order_sync to 2026-02-13 00:00:00+00:00, run_after=2026-02-14 00:00:00+00:00
2026-02-13 08:17:38,958 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:18:26,045 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:26,046 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:18:26,047 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:26,049 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:18:26,050 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:18:26,050 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:26,053 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:29,844 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1)
2026-02-13 08:18:29,854 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:18:24.219436+00:00, map_index=-1, run_start_date=2026-02-13 00:18:28.969094+00:00, run_end_date=2026-02-13 00:18:29.318396+00:00, run_duration=0.349302, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:18:26.048266+00:00, queued_by_job_id=7, pid=1606
2026-02-13 08:18:29,904 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:29,905 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:18:29,906 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:29,908 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:18:29,909 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:18:29,910 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:29,912 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:44,568 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1)
2026-02-13 08:18:44,578 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-13T00:18:24.219436+00:00, map_index=-1, run_start_date=2026-02-13 00:18:32.487156+00:00, run_end_date=2026-02-13 00:18:44.040088+00:00, run_duration=11.552932, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:18:29.907098+00:00, queued_by_job_id=7, pid=1609
2026-02-13 08:18:44,647 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:44,648 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:18:44,648 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>
2026-02-13 08:18:44,650 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:18:24.219436+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:18:44,651 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:18:44,652 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:44,654 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:18:24.219436+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:18:55,247 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:18:24.219436+00:00', try_number=1, map_index=-1)
2026-02-13 08:18:55,256 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=manual__2026-02-13T00:18:24.219436+00:00, map_index=-1, run_start_date=2026-02-13 00:18:47.083853+00:00, run_end_date=2026-02-13 00:18:54.735331+00:00, run_duration=7.651478, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:18:44.649721+00:00, queued_by_job_id=7, pid=1634
2026-02-13 08:18:57,472 INFO - Marking run <DagRun dw_order_sync @ 2026-02-13 00:18:24.219436+00:00: manual__2026-02-13T00:18:24.219436+00:00, state:running, queued_at: 2026-02-13 00:18:24.242798+00:00. externally triggered: True> successful
2026-02-13 08:18:57,473 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-13 00:18:24.219436+00:00, run_id=manual__2026-02-13T00:18:24.219436+00:00, run_start_date=2026-02-13 00:18:26.017239+00:00, run_end_date=2026-02-13 00:18:57.473471+00:00, run_duration=31.456232, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-12 00:00:00+00:00, data_interval_end=2026-02-13 00:00:00+00:00, dag_hash=340b2787125831c612b39fbc22c30471
2026-02-13 08:19:43,818 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:43,819 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:19:43,820 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:43,822 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:19:43,824 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:19:43,824 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:19:43,827 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:19:47,263 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1)
2026-02-13 08:19:47,272 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:19:42.112301+00:00, map_index=-1, run_start_date=2026-02-13 00:19:46.421204+00:00, run_end_date=2026-02-13 00:19:46.743513+00:00, run_duration=0.322309, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:19:43.821755+00:00, queued_by_job_id=7, pid=1697
2026-02-13 08:19:47,347 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:47,348 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:19:47,349 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:47,351 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:19:47,352 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:19:47,352 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:19:47,354 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:19:56,969 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1)
2026-02-13 08:19:56,978 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-13T00:19:42.112301+00:00, map_index=-1, run_start_date=2026-02-13 00:19:49.742640+00:00, run_end_date=2026-02-13 00:19:56.428057+00:00, run_duration=6.685417, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:19:47.350025+00:00, queued_by_job_id=7, pid=1700
2026-02-13 08:19:57,044 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:57,045 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:19:57,045 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>
2026-02-13 08:19:57,047 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:19:42.112301+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:19:57,048 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:19:57,049 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:19:57,052 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:19:42.112301+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:20:07,717 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:19:42.112301+00:00', try_number=1, map_index=-1)
2026-02-13 08:20:07,727 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=manual__2026-02-13T00:19:42.112301+00:00, map_index=-1, run_start_date=2026-02-13 00:19:59.681703+00:00, run_end_date=2026-02-13 00:20:07.265611+00:00, run_duration=7.583908, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:19:57.046632+00:00, queued_by_job_id=7, pid=1719
2026-02-13 08:20:10,034 INFO - Marking run <DagRun dw_order_sync @ 2026-02-13 00:19:42.112301+00:00: manual__2026-02-13T00:19:42.112301+00:00, state:running, queued_at: 2026-02-13 00:19:42.125379+00:00. externally triggered: True> successful
2026-02-13 08:20:10,036 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-13 00:19:42.112301+00:00, run_id=manual__2026-02-13T00:19:42.112301+00:00, run_start_date=2026-02-13 00:19:43.798104+00:00, run_end_date=2026-02-13 00:20:10.035933+00:00, run_duration=26.237829, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-12 00:00:00+00:00, data_interval_end=2026-02-13 00:00:00+00:00, dag_hash=340b2787125831c612b39fbc22c30471
2026-02-13 08:20:58,292 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:20:58,293 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:20:58,294 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:20:58,297 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:20:58,298 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:20:58,299 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:20:58,302 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:21:02,514 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1)
2026-02-13 08:21:02,524 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:20:57.274669+00:00, map_index=-1, run_start_date=2026-02-13 00:21:01.677546+00:00, run_end_date=2026-02-13 00:21:02.028194+00:00, run_duration=0.350648, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:20:58.296391+00:00, queued_by_job_id=7, pid=1772
2026-02-13 08:21:02,580 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:21:02,581 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:21:02,582 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:21:02,585 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:21:02,586 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:21:02,586 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:21:02,589 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:21:13,444 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1)
2026-02-13 08:21:13,457 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-13T00:20:57.274669+00:00, map_index=-1, run_start_date=2026-02-13 00:21:05.502972+00:00, run_end_date=2026-02-13 00:21:12.895029+00:00, run_duration=7.392057, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:21:02.583646+00:00, queued_by_job_id=7, pid=1775
2026-02-13 08:21:15,924 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:21:15,925 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:21:15,926 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>
2026-02-13 08:21:15,928 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:20:57.274669+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:21:15,929 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:21:15,930 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:21:15,932 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:20:57.274669+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:21:25,166 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:20:57.274669+00:00', try_number=1, map_index=-1)
2026-02-13 08:21:25,176 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=manual__2026-02-13T00:20:57.274669+00:00, map_index=-1, run_start_date=2026-02-13 00:21:18.637785+00:00, run_end_date=2026-02-13 00:21:24.632121+00:00, run_duration=5.994336, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:21:15.927188+00:00, queued_by_job_id=7, pid=1796
2026-02-13 08:21:25,224 INFO - Marking run <DagRun dw_order_sync @ 2026-02-13 00:20:57.274669+00:00: manual__2026-02-13T00:20:57.274669+00:00, state:running, queued_at: 2026-02-13 00:20:57.314532+00:00. externally triggered: True> successful
2026-02-13 08:21:25,225 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-13 00:20:57.274669+00:00, run_id=manual__2026-02-13T00:20:57.274669+00:00, run_start_date=2026-02-13 00:20:58.266283+00:00, run_end_date=2026-02-13 00:21:25.225572+00:00, run_duration=26.959289, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-12 00:00:00+00:00, data_interval_end=2026-02-13 00:00:00+00:00, dag_hash=340b2787125831c612b39fbc22c30471
2026-02-13 08:22:39,004 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:23:29,386 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 08:23:29,939 INFO - Sending Signals.SIGTERM to group 1168. PIDs of all processes in the group: []
2026-02-13 08:23:29,940 INFO - Sending the signal Signals.SIGTERM to group 1168
2026-02-13 08:23:29,940 INFO - Sending the signal Signals.SIGTERM to process 1168 as process group is missing.
2026-02-13 08:23:29,950 INFO - Sending Signals.SIGTERM to group 1168. PIDs of all processes in the group: []
2026-02-13 08:23:29,950 INFO - Sending the signal Signals.SIGTERM to group 1168
2026-02-13 08:23:29,951 INFO - Sending the signal Signals.SIGTERM to process 1168 as process group is missing.
2026-02-13 08:23:29,952 INFO - Exited execute loop
2026-02-13 08:23:42,233 INFO - Loaded executor: SequentialExecutor
2026-02-13 08:23:42,734 INFO - Starting the scheduler
2026-02-13 08:23:42,735 INFO - Processing each file at most -1 times
2026-02-13 08:23:42,741 INFO - Launched DagFileProcessorManager with pid: 2011
2026-02-13 08:23:42,746 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:23:55,543 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:23:55,544 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:23:55,545 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:23:55,547 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:23:55,549 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:23:55,550 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:23:55,552 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:23:59,521 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1)
2026-02-13 08:23:59,535 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:23:54.584326+00:00, map_index=-1, run_start_date=2026-02-13 00:23:58.552150+00:00, run_end_date=2026-02-13 00:23:58.918671+00:00, run_duration=0.366521, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=21, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:23:55.546246+00:00, queued_by_job_id=20, pid=2017
2026-02-13 08:23:59,601 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:23:59,602 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:23:59,603 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:23:59,605 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:23:59,606 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:23:59,606 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:23:59,609 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:24:11,649 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1)
2026-02-13 08:24:11,662 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-13T00:23:54.584326+00:00, map_index=-1, run_start_date=2026-02-13 00:24:03.078048+00:00, run_end_date=2026-02-13 00:24:11.084049+00:00, run_duration=8.006001, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=22, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:23:59.604060+00:00, queued_by_job_id=20, pid=2023
2026-02-13 08:24:11,723 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:24:11,724 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:24:11,725 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>
2026-02-13 08:24:11,727 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:23:54.584326+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:24:11,729 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:24:11,729 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:24:11,732 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:23:54.584326+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:24:21,197 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:23:54.584326+00:00', try_number=1, map_index=-1)
2026-02-13 08:24:21,208 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=manual__2026-02-13T00:23:54.584326+00:00, map_index=-1, run_start_date=2026-02-13 00:24:14.431573+00:00, run_end_date=2026-02-13 00:24:20.682794+00:00, run_duration=6.251221, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=23, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:24:11.726498+00:00, queued_by_job_id=20, pid=2068
2026-02-13 08:24:23,703 INFO - Marking run <DagRun dw_order_sync @ 2026-02-13 00:23:54.584326+00:00: manual__2026-02-13T00:23:54.584326+00:00, state:running, queued_at: 2026-02-13 00:23:54.610819+00:00. externally triggered: True> successful
2026-02-13 08:24:23,705 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-13 00:23:54.584326+00:00, run_id=manual__2026-02-13T00:23:54.584326+00:00, run_start_date=2026-02-13 00:23:55.367677+00:00, run_end_date=2026-02-13 00:24:23.705760+00:00, run_duration=28.338083, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-11 16:00:00+00:00, data_interval_end=2026-02-12 16:00:00+00:00, dag_hash=e441836cdc8ba3fd622c724e955ce635
2026-02-13 08:28:42,805 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:28:48,974 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 08:28:49,456 INFO - Sending Signals.SIGTERM to group 2011. PIDs of all processes in the group: []
2026-02-13 08:28:49,457 INFO - Sending the signal Signals.SIGTERM to group 2011
2026-02-13 08:28:49,458 INFO - Sending the signal Signals.SIGTERM to process 2011 as process group is missing.
2026-02-13 08:28:49,467 INFO - Sending Signals.SIGTERM to group 2011. PIDs of all processes in the group: []
2026-02-13 08:28:49,468 INFO - Sending the signal Signals.SIGTERM to group 2011
2026-02-13 08:28:49,469 INFO - Sending the signal Signals.SIGTERM to process 2011 as process group is missing.
2026-02-13 08:28:49,469 INFO - Exited execute loop
2026-02-13 08:29:02,714 INFO - Loaded executor: SequentialExecutor
2026-02-13 08:29:03,229 INFO - Starting the scheduler
2026-02-13 08:29:03,230 INFO - Processing each file at most -1 times
2026-02-13 08:29:03,236 INFO - Launched DagFileProcessorManager with pid: 2365
2026-02-13 08:29:03,243 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:29:14,715 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:14,717 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:29:14,717 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:14,721 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:29:14,721 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:29:14,722 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:14,724 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:18,672 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1)
2026-02-13 08:29:18,687 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:29:13.278904+00:00, map_index=-1, run_start_date=2026-02-13 00:29:17.569141+00:00, run_end_date=2026-02-13 00:29:17.961250+00:00, run_duration=0.392109, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=25, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:29:14.719303+00:00, queued_by_job_id=24, pid=2374
2026-02-13 08:29:18,761 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:18,762 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:29:18,763 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:18,765 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:29:18,767 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:29:18,767 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:18,770 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:30,124 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1)
2026-02-13 08:29:30,136 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=manual__2026-02-13T00:29:13.278904+00:00, map_index=-1, run_start_date=2026-02-13 00:29:21.731928+00:00, run_end_date=2026-02-13 00:29:29.534630+00:00, run_duration=7.802702, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=26, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 00:29:18.764704+00:00, queued_by_job_id=24, pid=2377
2026-02-13 08:29:30,195 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:30,196 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:29:30,197 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>
2026-02-13 08:29:30,199 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 manual__2026-02-13T00:29:13.278904+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:29:30,200 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:29:30,201 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:30,203 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'manual__2026-02-13T00:29:13.278904+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:29:40,780 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='manual__2026-02-13T00:29:13.278904+00:00', try_number=1, map_index=-1)
2026-02-13 08:29:40,790 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=manual__2026-02-13T00:29:13.278904+00:00, map_index=-1, run_start_date=2026-02-13 00:29:32.988371+00:00, run_end_date=2026-02-13 00:29:40.200077+00:00, run_duration=7.211706, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=27, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:29:30.198138+00:00, queued_by_job_id=24, pid=2397
2026-02-13 08:29:43,372 INFO - Marking run <DagRun dw_order_sync @ 2026-02-13 00:29:13.278904+00:00: manual__2026-02-13T00:29:13.278904+00:00, state:running, queued_at: 2026-02-13 00:29:13.310493+00:00. externally triggered: True> successful
2026-02-13 08:29:43,374 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-13 00:29:13.278904+00:00, run_id=manual__2026-02-13T00:29:13.278904+00:00, run_start_date=2026-02-13 00:29:14.523500+00:00, run_end_date=2026-02-13 00:29:43.374511+00:00, run_duration=28.851011, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-11 16:00:00+00:00, data_interval_end=2026-02-12 16:00:00+00:00, dag_hash=e441836cdc8ba3fd622c724e955ce635
2026-02-13 08:30:35,235 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 08:30:35,782 INFO - Sending Signals.SIGTERM to group 2365. PIDs of all processes in the group: []
2026-02-13 08:30:35,782 INFO - Sending the signal Signals.SIGTERM to group 2365
2026-02-13 08:30:35,783 INFO - Sending the signal Signals.SIGTERM to process 2365 as process group is missing.
2026-02-13 08:30:35,793 INFO - Sending Signals.SIGTERM to group 2365. PIDs of all processes in the group: []
2026-02-13 08:30:35,794 INFO - Sending the signal Signals.SIGTERM to group 2365
2026-02-13 08:30:35,795 INFO - Sending the signal Signals.SIGTERM to process 2365 as process group is missing.
2026-02-13 08:30:35,796 INFO - Exited execute loop
2026-02-13 08:30:42,025 INFO - Loaded executor: SequentialExecutor
2026-02-13 08:30:42,545 INFO - Starting the scheduler
2026-02-13 08:30:42,546 INFO - Processing each file at most -1 times
2026-02-13 08:30:42,554 INFO - Launched DagFileProcessorManager with pid: 2436
2026-02-13 08:30:42,558 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:31:09,872 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>
2026-02-13 08:31:09,872 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:31:09,873 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>
2026-02-13 08:31:09,876 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:31:09,878 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:31:08.416794+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-13 08:31:09,878 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:31:08.416794+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:31:09,881 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'manual__2026-02-13T00:31:08.416794+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:31:13,564 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T00:31:08.416794+00:00', try_number=1, map_index=-1)
2026-02-13 08:31:13,577 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T00:31:08.416794+00:00, map_index=-1, run_start_date=2026-02-13 00:31:12.615491+00:00, run_end_date=2026-02-13 00:31:12.976229+00:00, run_duration=0.360738, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=29, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 00:31:09.874916+00:00, queued_by_job_id=28, pid=2457
2026-02-13 08:31:13,648 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>
2026-02-13 08:31:13,650 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-13 08:31:13,651 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>
2026-02-13 08:31:13,653 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test manual__2026-02-13T00:31:08.416794+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:31:13,654 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:31:08.416794+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 08:31:13,655 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:31:08.416794+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:31:13,657 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'manual__2026-02-13T00:31:08.416794+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-13 08:31:25,594 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='manual__2026-02-13T00:31:08.416794+00:00', try_number=1, map_index=-1)
2026-02-13 08:35:42,613 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:40:42,882 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:40:55,627 INFO - Setting next_dagrun for test_connection_mysql to None, run_after=None
2026-02-13 08:40:55,661 INFO - 1 tasks up for execution:
	<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 08:40:55,662 INFO - DAG test_connection_mysql has 0/16 running and queued tasks
2026-02-13 08:40:55,663 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 08:40:55,665 INFO - Trying to enqueue tasks: [<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:40:55,666 INFO - Sending TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:40:55,666 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:40:55,670 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:41:00,273 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 08:41:00,288 INFO - TaskInstance Finished: dag_id=test_connection_mysql, task_id=query_mysql, run_id=scheduled__2026-02-11T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 00:40:59.167541+00:00, run_end_date=2026-02-13 00:40:59.564241+00:00, run_duration=0.3967, state=up_for_retry, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=31, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:40:55.664053+00:00, queued_by_job_id=28, pid=2814
2026-02-13 08:42:00,559 INFO - 1 tasks up for execution:
	<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 08:42:00,560 INFO - DAG test_connection_mysql has 0/16 running and queued tasks
2026-02-13 08:42:00,560 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 08:42:00,563 INFO - Trying to enqueue tasks: [<TaskInstance: test_connection_mysql.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:42:00,564 INFO - Sending TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=2, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:42:00,565 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:42:00,568 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:42:05,091 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=2, map_index=-1)
2026-02-13 08:42:05,108 INFO - TaskInstance Finished: dag_id=test_connection_mysql, task_id=query_mysql, run_id=scheduled__2026-02-11T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 00:42:03.948778+00:00, run_end_date=2026-02-13 00:42:04.349330+00:00, run_duration=0.400552, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=2, max_tries=1, job_id=32, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:42:00.561931+00:00, queued_by_job_id=28, pid=2859
2026-02-13 08:42:05,175 ERROR - Marking run <DagRun test_connection_mysql @ 2026-02-11 16:00:00+00:00: scheduled__2026-02-11T16:00:00+00:00, state:running, queued_at: 2026-02-13 00:40:55.618665+00:00. externally triggered: False> failed
2026-02-13 08:42:05,178 INFO - DagRun Finished: dag_id=test_connection_mysql, execution_date=2026-02-11 16:00:00+00:00, run_id=scheduled__2026-02-11T16:00:00+00:00, run_start_date=2026-02-13 00:40:55.639877+00:00, run_end_date=2026-02-13 00:42:05.178026+00:00, run_duration=69.538149, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-11 16:00:00+00:00, data_interval_end=2026-02-11 16:00:00+00:00, dag_hash=8182ea0d671bb8e51dd0fa1c60bbda16
2026-02-13 08:42:05,183 INFO - Setting next_dagrun for test_connection_mysql to None, run_after=None
2026-02-13 08:44:46,863 INFO - 1 tasks up for execution:
	<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:44:42.784171+00:00 [scheduled]>
2026-02-13 08:44:46,865 INFO - DAG test_connection_mysql has 0/16 running and queued tasks
2026-02-13 08:44:46,866 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:44:42.784171+00:00 [scheduled]>
2026-02-13 08:44:46,868 INFO - Trying to enqueue tasks: [<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:44:42.784171+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:44:46,869 INFO - Sending TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='manual__2026-02-13T00:44:42.784171+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:44:46,870 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'manual__2026-02-13T00:44:42.784171+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:44:46,872 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'manual__2026-02-13T00:44:42.784171+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:44:51,430 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='manual__2026-02-13T00:44:42.784171+00:00', try_number=1, map_index=-1)
2026-02-13 08:44:51,439 INFO - TaskInstance Finished: dag_id=test_connection_mysql, task_id=query_mysql, run_id=manual__2026-02-13T00:44:42.784171+00:00, map_index=-1, run_start_date=2026-02-13 00:44:50.072758+00:00, run_end_date=2026-02-13 00:44:50.861206+00:00, run_duration=0.788448, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=33, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:44:46.867278+00:00, queued_by_job_id=28, pid=3074
2026-02-13 08:44:51,517 INFO - Marking run <DagRun test_connection_mysql @ 2026-02-13 00:44:42.784171+00:00: manual__2026-02-13T00:44:42.784171+00:00, state:running, queued_at: 2026-02-13 00:44:42.803526+00:00. externally triggered: True> successful
2026-02-13 08:44:51,517 INFO - DagRun Finished: dag_id=test_connection_mysql, execution_date=2026-02-13 00:44:42.784171+00:00, run_id=manual__2026-02-13T00:44:42.784171+00:00, run_start_date=2026-02-13 00:44:46.843289+00:00, run_end_date=2026-02-13 00:44:51.517879+00:00, run_duration=4.67459, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 00:44:42.784171+00:00, data_interval_end=2026-02-13 00:44:42.784171+00:00, dag_hash=8182ea0d671bb8e51dd0fa1c60bbda16
2026-02-13 08:45:42,918 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:50:42,965 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 08:53:58,786 INFO - 1 tasks up for execution:
	<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:53:57.396587+00:00 [scheduled]>
2026-02-13 08:53:58,788 INFO - DAG test_connection_mysql has 0/16 running and queued tasks
2026-02-13 08:53:58,788 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:53:57.396587+00:00 [scheduled]>
2026-02-13 08:53:58,791 INFO - Trying to enqueue tasks: [<TaskInstance: test_connection_mysql.query_mysql manual__2026-02-13T00:53:57.396587+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 08:53:58,792 INFO - Sending TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='manual__2026-02-13T00:53:57.396587+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 08:53:58,792 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'manual__2026-02-13T00:53:57.396587+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:53:58,795 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_connection_mysql', 'query_mysql', 'manual__2026-02-13T00:53:57.396587+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 08:54:03,075 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_connection_mysql', task_id='query_mysql', run_id='manual__2026-02-13T00:53:57.396587+00:00', try_number=1, map_index=-1)
2026-02-13 08:54:03,084 INFO - TaskInstance Finished: dag_id=test_connection_mysql, task_id=query_mysql, run_id=manual__2026-02-13T00:53:57.396587+00:00, map_index=-1, run_start_date=2026-02-13 00:54:02.066217+00:00, run_end_date=2026-02-13 00:54:02.476936+00:00, run_duration=0.410719, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=34, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 00:53:58.790025+00:00, queued_by_job_id=28, pid=3438
2026-02-13 08:54:03,133 INFO - Marking run <DagRun test_connection_mysql @ 2026-02-13 00:53:57.396587+00:00: manual__2026-02-13T00:53:57.396587+00:00, state:running, queued_at: 2026-02-13 00:53:57.404714+00:00. externally triggered: True> successful
2026-02-13 08:54:03,134 INFO - DagRun Finished: dag_id=test_connection_mysql, execution_date=2026-02-13 00:53:57.396587+00:00, run_id=manual__2026-02-13T00:53:57.396587+00:00, run_start_date=2026-02-13 00:53:58.761358+00:00, run_end_date=2026-02-13 00:54:03.134804+00:00, run_duration=4.373446, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 00:53:57.396587+00:00, data_interval_end=2026-02-13 00:53:57.396587+00:00, dag_hash=8182ea0d671bb8e51dd0fa1c60bbda16
2026-02-13 08:55:43,011 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:00:43,041 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:05:43,084 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:10:43,123 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:11:02,748 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 09:11:03,362 INFO - Sending Signals.SIGTERM to group 2436. PIDs of all processes in the group: []
2026-02-13 09:11:03,364 INFO - Sending the signal Signals.SIGTERM to group 2436
2026-02-13 09:11:03,365 INFO - Sending the signal Signals.SIGTERM to process 2436 as process group is missing.
2026-02-13 09:11:03,378 INFO - Sending Signals.SIGTERM to group 2436. PIDs of all processes in the group: []
2026-02-13 09:11:03,379 INFO - Sending the signal Signals.SIGTERM to group 2436
2026-02-13 09:11:03,379 INFO - Sending the signal Signals.SIGTERM to process 2436 as process group is missing.
2026-02-13 09:11:03,380 INFO - Exited execute loop
2026-02-13 09:11:12,391 INFO - Loaded executor: SequentialExecutor
2026-02-13 09:11:12,965 INFO - Starting the scheduler
2026-02-13 09:11:12,966 INFO - Processing each file at most -1 times
2026-02-13 09:11:12,972 INFO - Launched DagFileProcessorManager with pid: 3753
2026-02-13 09:11:12,977 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:12:05,446 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 09:12:06,128 INFO - Sending Signals.SIGTERM to group 3753. PIDs of all processes in the group: []
2026-02-13 09:12:06,129 INFO - Sending the signal Signals.SIGTERM to group 3753
2026-02-13 09:12:06,129 INFO - Sending the signal Signals.SIGTERM to process 3753 as process group is missing.
2026-02-13 09:12:06,140 INFO - Sending Signals.SIGTERM to group 3753. PIDs of all processes in the group: []
2026-02-13 09:12:06,140 INFO - Sending the signal Signals.SIGTERM to group 3753
2026-02-13 09:12:06,141 INFO - Sending the signal Signals.SIGTERM to process 3753 as process group is missing.
2026-02-13 09:12:06,142 INFO - Exited execute loop
2026-02-13 09:12:14,160 INFO - Loaded executor: SequentialExecutor
2026-02-13 09:12:14,587 INFO - Starting the scheduler
2026-02-13 09:12:14,589 INFO - Processing each file at most -1 times
2026-02-13 09:12:14,594 INFO - Launched DagFileProcessorManager with pid: 3821
2026-02-13 09:12:14,601 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:12:39,295 INFO - Setting next_dagrun for test_crm to None, run_after=None
2026-02-13 09:12:39,349 INFO - 1 tasks up for execution:
	<TaskInstance: test_crm.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 09:12:39,350 INFO - DAG test_crm has 0/16 running and queued tasks
2026-02-13 09:12:39,350 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_crm.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 09:12:39,353 INFO - Trying to enqueue tasks: [<TaskInstance: test_crm.query_mysql scheduled__2026-02-11T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 09:12:39,354 INFO - Sending TaskInstanceKey(dag_id='test_crm', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 09:12:39,355 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_crm', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:39,359 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_crm', 'query_mysql', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:44,256 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_crm', task_id='query_mysql', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 09:12:44,273 INFO - TaskInstance Finished: dag_id=test_crm, task_id=query_mysql, run_id=scheduled__2026-02-11T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 01:12:43.115596+00:00, run_end_date=2026-02-13 01:12:43.579810+00:00, run_duration=0.464214, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=37, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 01:12:39.351859+00:00, queued_by_job_id=36, pid=3846
2026-02-13 09:12:44,334 INFO - 1 tasks up for execution:
	<TaskInstance: test_crm.process_result scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 09:12:44,335 INFO - DAG test_crm has 0/16 running and queued tasks
2026-02-13 09:12:44,336 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_crm.process_result scheduled__2026-02-11T16:00:00+00:00 [scheduled]>
2026-02-13 09:12:44,339 INFO - Trying to enqueue tasks: [<TaskInstance: test_crm.process_result scheduled__2026-02-11T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 09:12:44,340 INFO - Sending TaskInstanceKey(dag_id='test_crm', task_id='process_result', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 09:12:44,341 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_crm', 'process_result', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:44,344 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_crm', 'process_result', 'scheduled__2026-02-11T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:48,635 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_crm', task_id='process_result', run_id='scheduled__2026-02-11T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-13 09:12:48,648 INFO - TaskInstance Finished: dag_id=test_crm, task_id=process_result, run_id=scheduled__2026-02-11T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 01:12:47.510069+00:00, run_end_date=2026-02-13 01:12:47.907668+00:00, run_duration=0.397599, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=38, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 01:12:44.337745+00:00, queued_by_job_id=36, pid=3848
2026-02-13 09:12:48,713 INFO - Marking run <DagRun test_crm @ 2026-02-11 16:00:00+00:00: scheduled__2026-02-11T16:00:00+00:00, state:running, queued_at: 2026-02-13 01:12:39.284770+00:00. externally triggered: False> successful
2026-02-13 09:12:48,717 INFO - DagRun Finished: dag_id=test_crm, execution_date=2026-02-11 16:00:00+00:00, run_id=scheduled__2026-02-11T16:00:00+00:00, run_start_date=2026-02-13 01:12:39.306973+00:00, run_end_date=2026-02-13 01:12:48.717091+00:00, run_duration=9.410118, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-11 16:00:00+00:00, data_interval_end=2026-02-11 16:00:00+00:00, dag_hash=ea7b094825b9087a0f7bf9077bdf754f
2026-02-13 09:12:48,722 INFO - Setting next_dagrun for test_crm to None, run_after=None
2026-02-13 09:12:48,732 INFO - 1 tasks up for execution:
	<TaskInstance: test_crm.query_mysql manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>
2026-02-13 09:12:48,732 INFO - DAG test_crm has 0/16 running and queued tasks
2026-02-13 09:12:48,733 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_crm.query_mysql manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>
2026-02-13 09:12:48,736 INFO - Trying to enqueue tasks: [<TaskInstance: test_crm.query_mysql manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 09:12:48,738 INFO - Sending TaskInstanceKey(dag_id='test_crm', task_id='query_mysql', run_id='manual__2026-02-13T01:12:44.736895+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-13 09:12:48,738 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_crm', 'query_mysql', 'manual__2026-02-13T01:12:44.736895+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:48,741 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_crm', 'query_mysql', 'manual__2026-02-13T01:12:44.736895+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:52,576 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_crm', task_id='query_mysql', run_id='manual__2026-02-13T01:12:44.736895+00:00', try_number=1, map_index=-1)
2026-02-13 09:12:52,588 INFO - TaskInstance Finished: dag_id=test_crm, task_id=query_mysql, run_id=manual__2026-02-13T01:12:44.736895+00:00, map_index=-1, run_start_date=2026-02-13 01:12:51.507038+00:00, run_end_date=2026-02-13 01:12:51.930818+00:00, run_duration=0.42378, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=39, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 01:12:48.735440+00:00, queued_by_job_id=36, pid=3850
2026-02-13 09:12:55,277 INFO - 1 tasks up for execution:
	<TaskInstance: test_crm.process_result manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>
2026-02-13 09:12:55,278 INFO - DAG test_crm has 0/16 running and queued tasks
2026-02-13 09:12:55,279 INFO - Setting the following tasks to queued state:
	<TaskInstance: test_crm.process_result manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>
2026-02-13 09:12:55,281 INFO - Trying to enqueue tasks: [<TaskInstance: test_crm.process_result manual__2026-02-13T01:12:44.736895+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 09:12:55,283 INFO - Sending TaskInstanceKey(dag_id='test_crm', task_id='process_result', run_id='manual__2026-02-13T01:12:44.736895+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 09:12:55,284 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'test_crm', 'process_result', 'manual__2026-02-13T01:12:44.736895+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:55,286 INFO - Executing command: ['airflow', 'tasks', 'run', 'test_crm', 'process_result', 'manual__2026-02-13T01:12:44.736895+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_crm/test_crm.py']
2026-02-13 09:12:59,178 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='test_crm', task_id='process_result', run_id='manual__2026-02-13T01:12:44.736895+00:00', try_number=1, map_index=-1)
2026-02-13 09:12:59,189 INFO - TaskInstance Finished: dag_id=test_crm, task_id=process_result, run_id=manual__2026-02-13T01:12:44.736895+00:00, map_index=-1, run_start_date=2026-02-13 01:12:58.054459+00:00, run_end_date=2026-02-13 01:12:58.474659+00:00, run_duration=0.4202, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=40, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 01:12:55.280355+00:00, queued_by_job_id=36, pid=3853
2026-02-13 09:13:02,163 INFO - Marking run <DagRun test_crm @ 2026-02-13 01:12:44.736895+00:00: manual__2026-02-13T01:12:44.736895+00:00, state:running, queued_at: 2026-02-13 01:12:44.758653+00:00. externally triggered: True> successful
2026-02-13 09:13:02,164 INFO - DagRun Finished: dag_id=test_crm, execution_date=2026-02-13 01:12:44.736895+00:00, run_id=manual__2026-02-13T01:12:44.736895+00:00, run_start_date=2026-02-13 01:12:48.694657+00:00, run_end_date=2026-02-13 01:13:02.164612+00:00, run_duration=13.469955, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 01:12:44.736895+00:00, data_interval_end=2026-02-13 01:12:44.736895+00:00, dag_hash=ea7b094825b9087a0f7bf9077bdf754f
2026-02-13 09:17:14,651 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:22:15,420 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:27:15,461 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:32:15,501 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:37:15,709 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:42:16,203 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:47:17,492 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:52:17,526 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 09:57:17,571 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:02:17,615 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:07:17,663 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:12:17,673 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:17:20,012 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:22:20,050 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:27:20,091 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:32:20,132 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:37:20,172 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:42:21,754 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 10:56:19,329 INFO - Heartbeat recovered after 721.45 seconds
2026-02-13 10:58:12,630 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 10:58:13,433 INFO - Sending Signals.SIGTERM to group 3821. PIDs of all processes in the group: []
2026-02-13 10:58:13,435 INFO - Sending the signal Signals.SIGTERM to group 3821
2026-02-13 10:58:13,436 INFO - Sending the signal Signals.SIGTERM to process 3821 as process group is missing.
2026-02-13 10:58:13,453 INFO - Sending Signals.SIGTERM to group 3821. PIDs of all processes in the group: []
2026-02-13 10:58:13,454 INFO - Sending the signal Signals.SIGTERM to group 3821
2026-02-13 10:58:13,454 INFO - Sending the signal Signals.SIGTERM to process 3821 as process group is missing.
2026-02-13 10:58:13,455 INFO - Exited execute loop
2026-02-13 10:58:20,272 INFO - Loaded executor: SequentialExecutor
2026-02-13 10:58:20,874 INFO - Starting the scheduler
2026-02-13 10:58:20,877 INFO - Processing each file at most -1 times
2026-02-13 10:58:20,887 INFO - Launched DagFileProcessorManager with pid: 6580
2026-02-13 10:58:20,897 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:00:57,496 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>
2026-02-13 11:00:57,497 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:00:57,497 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>
2026-02-13 11:00:57,501 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:00:56.955760+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:00:57,502 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:00:56.955760+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:00:57,503 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:00:56.955760+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:00:57,506 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:00:56.955760+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:01:01,698 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:00:56.955760+00:00', try_number=1, map_index=-1)
2026-02-13 11:01:01,712 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:00:56.955760+00:00, map_index=-1, run_start_date=2026-02-13 03:01:00.771346+00:00, run_end_date=2026-02-13 03:01:01.167277+00:00, run_duration=0.395931, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=42, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:00:57.499727+00:00, queued_by_job_id=41, pid=6637
2026-02-13 11:01:01,759 ERROR - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:00:56.955760+00:00: manual__2026-02-13T03:00:56.955760+00:00, state:running, queued_at: 2026-02-13 03:00:56.975748+00:00. externally triggered: True> failed
2026-02-13 11:01:01,761 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:00:56.955760+00:00, run_id=manual__2026-02-13T03:00:56.955760+00:00, run_start_date=2026-02-13 03:00:57.463039+00:00, run_end_date=2026-02-13 03:01:01.761192+00:00, run_duration=4.298153, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:00:56.955760+00:00, data_interval_end=2026-02-13 03:00:56.955760+00:00, dag_hash=292d5ad65135765608fc09000942b435
2026-02-13 11:03:01,854 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>
2026-02-13 11:03:01,855 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:03:01,856 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>
2026-02-13 11:03:01,859 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:03:00.730591+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:03:01,860 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:03:00.730591+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:03:01,861 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:03:00.730591+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:03:01,863 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:03:00.730591+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:03:06,370 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:03:00.730591+00:00', try_number=1, map_index=-1)
2026-02-13 11:03:06,381 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:03:00.730591+00:00, map_index=-1, run_start_date=2026-02-13 03:03:05.280088+00:00, run_end_date=2026-02-13 03:03:05.732314+00:00, run_duration=0.452226, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=43, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:03:01.857929+00:00, queued_by_job_id=41, pid=6697
2026-02-13 11:03:09,188 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:03:00.730591+00:00: manual__2026-02-13T03:03:00.730591+00:00, state:running, queued_at: 2026-02-13 03:03:00.744300+00:00. externally triggered: True> successful
2026-02-13 11:03:09,189 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:03:00.730591+00:00, run_id=manual__2026-02-13T03:03:00.730591+00:00, run_start_date=2026-02-13 03:03:01.834575+00:00, run_end_date=2026-02-13 03:03:09.189026+00:00, run_duration=7.354451, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 03:03:00.730591+00:00, dag_hash=292d5ad65135765608fc09000942b435
2026-02-13 11:03:09,252 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>
2026-02-13 11:03:09,263 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:03:09,264 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>
2026-02-13 11:03:09,267 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:03:05.749067+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:03:09,269 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:03:05.749067+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:03:09,269 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:03:05.749067+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:03:09,288 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:03:05.749067+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:03:13,851 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:03:05.749067+00:00', try_number=1, map_index=-1)
2026-02-13 11:03:13,865 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:03:05.749067+00:00, map_index=-1, run_start_date=2026-02-13 03:03:12.595873+00:00, run_end_date=2026-02-13 03:03:12.993313+00:00, run_duration=0.39744, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=44, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:03:09.266267+00:00, queued_by_job_id=41, pid=6700
2026-02-13 11:03:16,803 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:03:05.749067+00:00: dataset_triggered__2026-02-13T03:03:05.749067+00:00, state:running, queued_at: 2026-02-13 03:03:09.151978+00:00. externally triggered: False> successful
2026-02-13 11:03:16,805 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:03:05.749067+00:00, run_id=dataset_triggered__2026-02-13T03:03:05.749067+00:00, run_start_date=2026-02-13 03:03:09.172019+00:00, run_end_date=2026-02-13 03:03:16.805082+00:00, run_duration=7.633063, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 03:03:00.730591+00:00, dag_hash=336021f9586175064690381be57e6c20
2026-02-13 11:03:20,949 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:08:22,300 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:13:23,601 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:15:30,076 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>
2026-02-13 11:15:30,077 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:15:30,078 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>
2026-02-13 11:15:30,082 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:15:28.766581+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:15:30,086 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:15:28.766581+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:15:30,087 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:15:28.766581+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:15:30,090 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:15:28.766581+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:15:34,771 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:15:28.766581+00:00', try_number=1, map_index=-1)
2026-02-13 11:15:34,780 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:15:28.766581+00:00, map_index=-1, run_start_date=2026-02-13 03:15:33.736246+00:00, run_end_date=2026-02-13 03:15:34.213945+00:00, run_duration=0.477699, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=45, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:15:30.079798+00:00, queued_by_job_id=41, pid=6940
2026-02-13 11:15:37,777 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:15:28.766581+00:00: manual__2026-02-13T03:15:28.766581+00:00, state:running, queued_at: 2026-02-13 03:15:28.788425+00:00. externally triggered: True> successful
2026-02-13 11:15:37,778 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:15:28.766581+00:00, run_id=manual__2026-02-13T03:15:28.766581+00:00, run_start_date=2026-02-13 03:15:30.048480+00:00, run_end_date=2026-02-13 03:15:37.778437+00:00, run_duration=7.729957, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:15:28.766581+00:00, data_interval_end=2026-02-13 03:15:28.766581+00:00, dag_hash=292d5ad65135765608fc09000942b435
2026-02-13 11:15:37,788 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>
2026-02-13 11:15:37,789 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:15:37,789 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>
2026-02-13 11:15:37,792 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:15:34.232141+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:15:37,793 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:15:34.232141+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:15:37,793 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:15:34.232141+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:15:37,796 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:15:34.232141+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:15:41,571 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:15:34.232141+00:00', try_number=1, map_index=-1)
2026-02-13 11:15:41,579 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:15:34.232141+00:00, map_index=-1, run_start_date=2026-02-13 03:15:40.639840+00:00, run_end_date=2026-02-13 03:15:40.968642+00:00, run_duration=0.328802, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=46, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:15:37.790849+00:00, queued_by_job_id=41, pid=6950
2026-02-13 11:15:44,402 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:15:34.232141+00:00: dataset_triggered__2026-02-13T03:15:34.232141+00:00, state:running, queued_at: 2026-02-13 03:15:37.748990+00:00. externally triggered: False> successful
2026-02-13 11:15:44,403 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:15:34.232141+00:00, run_id=dataset_triggered__2026-02-13T03:15:34.232141+00:00, run_start_date=2026-02-13 03:15:37.762831+00:00, run_end_date=2026-02-13 03:15:44.403607+00:00, run_duration=6.640776, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:15:28.766581+00:00, data_interval_end=2026-02-13 03:15:28.766581+00:00, dag_hash=336021f9586175064690381be57e6c20
2026-02-13 11:18:23,607 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:20:51,068 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>
2026-02-13 11:20:51,070 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:20:51,070 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>
2026-02-13 11:20:51,073 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:20:50.500982+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:20:51,076 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:20:50.500982+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:20:51,077 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:20:50.500982+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:20:51,079 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:20:50.500982+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:20:55,321 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:20:50.500982+00:00', try_number=1, map_index=-1)
2026-02-13 11:20:55,333 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:20:50.500982+00:00, map_index=-1, run_start_date=2026-02-13 03:20:54.354533+00:00, run_end_date=2026-02-13 03:20:54.789942+00:00, run_duration=0.435409, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=47, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:20:51.072024+00:00, queued_by_job_id=41, pid=7038
2026-02-13 11:20:55,403 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:20:50.500982+00:00: manual__2026-02-13T03:20:50.500982+00:00, state:running, queued_at: 2026-02-13 03:20:50.514067+00:00. externally triggered: True> successful
2026-02-13 11:20:55,404 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:20:50.500982+00:00, run_id=manual__2026-02-13T03:20:50.500982+00:00, run_start_date=2026-02-13 03:20:51.039154+00:00, run_end_date=2026-02-13 03:20:55.404373+00:00, run_duration=4.365219, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:20:50.500982+00:00, data_interval_end=2026-02-13 03:20:50.500982+00:00, dag_hash=292d5ad65135765608fc09000942b435
2026-02-13 11:20:55,414 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>
2026-02-13 11:20:55,415 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:20:55,416 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>
2026-02-13 11:20:55,418 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:20:54.810387+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:20:55,419 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:20:54.810387+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:20:55,420 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:20:54.810387+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:20:55,422 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:20:54.810387+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:20:58,946 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:20:54.810387+00:00', try_number=1, map_index=-1)
2026-02-13 11:20:58,954 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:20:54.810387+00:00, map_index=-1, run_start_date=2026-02-13 03:20:58.179565+00:00, run_end_date=2026-02-13 03:20:58.478434+00:00, run_duration=0.298869, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:20:55.417432+00:00, queued_by_job_id=41, pid=7041
2026-02-13 11:20:58,998 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:20:54.810387+00:00: dataset_triggered__2026-02-13T03:20:54.810387+00:00, state:running, queued_at: 2026-02-13 03:20:55.377690+00:00. externally triggered: False> successful
2026-02-13 11:20:58,999 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:20:54.810387+00:00, run_id=dataset_triggered__2026-02-13T03:20:54.810387+00:00, run_start_date=2026-02-13 03:20:55.389670+00:00, run_end_date=2026-02-13 03:20:58.999297+00:00, run_duration=3.609627, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:20:50.500982+00:00, data_interval_end=2026-02-13 03:20:50.500982+00:00, dag_hash=336021f9586175064690381be57e6c20
2026-02-13 11:23:26,021 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:28:26,768 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:33:29,567 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:36:01,032 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>
2026-02-13 11:36:01,033 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:36:01,033 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>
2026-02-13 11:36:01,036 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:35:59.119446+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:36:01,038 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:35:59.119446+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:36:01,038 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:35:59.119446+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:36:01,041 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:35:59.119446+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:36:05,394 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:35:59.119446+00:00', try_number=1, map_index=-1)
2026-02-13 11:36:05,403 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:35:59.119446+00:00, map_index=-1, run_start_date=2026-02-13 03:36:04.315076+00:00, run_end_date=2026-02-13 03:36:04.807601+00:00, run_duration=0.492525, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=49, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:36:01.035143+00:00, queued_by_job_id=41, pid=7372
2026-02-13 11:36:05,485 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:35:59.119446+00:00: manual__2026-02-13T03:35:59.119446+00:00, state:running, queued_at: 2026-02-13 03:35:59.143100+00:00. externally triggered: True> successful
2026-02-13 11:36:05,486 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:35:59.119446+00:00, run_id=manual__2026-02-13T03:35:59.119446+00:00, run_start_date=2026-02-13 03:36:01.007029+00:00, run_end_date=2026-02-13 03:36:05.486652+00:00, run_duration=4.479623, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:35:59.119446+00:00, data_interval_end=2026-02-13 03:35:59.119446+00:00, dag_hash=292d5ad65135765608fc09000942b435
2026-02-13 11:36:05,495 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>
2026-02-13 11:36:05,496 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:36:05,497 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>
2026-02-13 11:36:05,500 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:36:04.836355+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:36:05,501 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:36:04.836355+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:36:05,503 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:36:04.836355+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:36:05,505 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:36:04.836355+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:36:09,727 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:36:04.836355+00:00', try_number=1, map_index=-1)
2026-02-13 11:36:09,739 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:36:04.836355+00:00, map_index=-1, run_start_date=2026-02-13 03:36:08.780611+00:00, run_end_date=2026-02-13 03:36:09.134619+00:00, run_duration=0.354008, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=50, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:36:05.498785+00:00, queued_by_job_id=41, pid=7374
2026-02-13 11:36:12,405 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:36:04.836355+00:00: dataset_triggered__2026-02-13T03:36:04.836355+00:00, state:running, queued_at: 2026-02-13 03:36:05.455807+00:00. externally triggered: False> successful
2026-02-13 11:36:12,406 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:36:04.836355+00:00, run_id=dataset_triggered__2026-02-13T03:36:04.836355+00:00, run_start_date=2026-02-13 03:36:05.471261+00:00, run_end_date=2026-02-13 03:36:12.406530+00:00, run_duration=6.935269, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:35:59.119446+00:00, data_interval_end=2026-02-13 03:35:59.119446+00:00, dag_hash=336021f9586175064690381be57e6c20
2026-02-13 11:38:31,668 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:43:34,324 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:46:30,369 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>
2026-02-13 11:46:30,370 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:46:30,371 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>
2026-02-13 11:46:30,373 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:46:26.998824+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:46:30,375 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:46:26.998824+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:46:30,375 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:46:26.998824+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:46:30,378 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:46:26.998824+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:46:34,819 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:46:26.998824+00:00', try_number=1, map_index=-1)
2026-02-13 11:46:34,830 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:46:26.998824+00:00, map_index=-1, run_start_date=2026-02-13 03:46:33.729665+00:00, run_end_date=2026-02-13 03:46:34.216917+00:00, run_duration=0.487252, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:46:30.372500+00:00, queued_by_job_id=41, pid=7653
2026-02-13 11:46:34,916 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:46:26.998824+00:00: manual__2026-02-13T03:46:26.998824+00:00, state:running, queued_at: 2026-02-13 03:46:27.011741+00:00. externally triggered: True> successful
2026-02-13 11:46:34,917 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:46:26.998824+00:00, run_id=manual__2026-02-13T03:46:26.998824+00:00, run_start_date=2026-02-13 03:46:30.350067+00:00, run_end_date=2026-02-13 03:46:34.917490+00:00, run_duration=4.567423, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:46:26.998824+00:00, data_interval_end=2026-02-13 03:46:26.998824+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 11:46:35,986 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>
2026-02-13 11:46:35,987 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:46:35,988 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>
2026-02-13 11:46:35,990 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:46:34.235716+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:46:35,991 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:46:34.235716+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:46:35,992 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:46:34.235716+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:46:35,995 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:46:34.235716+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:46:39,588 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:46:34.235716+00:00', try_number=1, map_index=-1)
2026-02-13 11:46:39,598 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:46:34.235716+00:00, map_index=-1, run_start_date=2026-02-13 03:46:38.748184+00:00, run_end_date=2026-02-13 03:46:39.042286+00:00, run_duration=0.294102, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=52, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:46:35.989011+00:00, queued_by_job_id=41, pid=7658
2026-02-13 11:46:39,637 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:46:34.235716+00:00: dataset_triggered__2026-02-13T03:46:34.235716+00:00, state:running, queued_at: 2026-02-13 03:46:34.891969+00:00. externally triggered: False> successful
2026-02-13 11:46:39,638 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:46:34.235716+00:00, run_id=dataset_triggered__2026-02-13T03:46:34.235716+00:00, run_start_date=2026-02-13 03:46:35.955545+00:00, run_end_date=2026-02-13 03:46:39.638248+00:00, run_duration=3.682703, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:46:26.998824+00:00, data_interval_end=2026-02-13 03:46:26.998824+00:00, dag_hash=4f5cee350ba129ab524aa1ccabba746a
2026-02-13 11:48:34,351 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:53:34,384 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 11:58:14,988 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>
2026-02-13 11:58:14,990 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 11:58:14,990 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>
2026-02-13 11:58:14,993 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T03:58:13.633239+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:58:14,995 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:58:13.633239+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:58:14,996 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:58:13.633239+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:58:14,998 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T03:58:13.633239+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:58:19,405 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T03:58:13.633239+00:00', try_number=1, map_index=-1)
2026-02-13 11:58:19,413 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T03:58:13.633239+00:00, map_index=-1, run_start_date=2026-02-13 03:58:18.240372+00:00, run_end_date=2026-02-13 03:58:18.829312+00:00, run_duration=0.58894, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=53, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:58:14.992155+00:00, queued_by_job_id=41, pid=7917
2026-02-13 11:58:19,491 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 03:58:13.633239+00:00: manual__2026-02-13T03:58:13.633239+00:00, state:running, queued_at: 2026-02-13 03:58:13.650127+00:00. externally triggered: True> successful
2026-02-13 11:58:19,492 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 03:58:13.633239+00:00, run_id=manual__2026-02-13T03:58:13.633239+00:00, run_start_date=2026-02-13 03:58:14.959849+00:00, run_end_date=2026-02-13 03:58:19.492612+00:00, run_duration=4.532763, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 03:58:13.633239+00:00, data_interval_end=2026-02-13 03:58:13.633239+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 11:58:19,502 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>
2026-02-13 11:58:19,503 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 11:58:19,504 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>
2026-02-13 11:58:19,506 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T03:58:18.846380+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 11:58:19,507 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:58:18.846380+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 11:58:19,508 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:58:18.846380+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:58:19,510 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T03:58:18.846380+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 11:58:23,280 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T03:58:18.846380+00:00', try_number=1, map_index=-1)
2026-02-13 11:58:23,290 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T03:58:18.846380+00:00, map_index=-1, run_start_date=2026-02-13 03:58:22.470670+00:00, run_end_date=2026-02-13 03:58:22.760214+00:00, run_duration=0.289544, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=54, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 03:58:19.505294+00:00, queued_by_job_id=41, pid=7928
2026-02-13 11:58:25,935 ERROR - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 03:58:18.846380+00:00: dataset_triggered__2026-02-13T03:58:18.846380+00:00, state:running, queued_at: 2026-02-13 03:58:19.464908+00:00. externally triggered: False> failed
2026-02-13 11:58:25,937 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 03:58:18.846380+00:00, run_id=dataset_triggered__2026-02-13T03:58:18.846380+00:00, run_start_date=2026-02-13 03:58:19.478973+00:00, run_end_date=2026-02-13 03:58:25.937314+00:00, run_duration=6.458341, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:58:13.633239+00:00, data_interval_end=2026-02-13 03:58:13.633239+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
2026-02-13 11:58:36,153 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 12:00:37,867 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>
2026-02-13 12:00:37,869 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 12:00:37,870 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>
2026-02-13 12:00:37,873 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T04:00:36.991126+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 12:00:37,874 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T04:00:36.991126+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 12:00:37,875 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T04:00:36.991126+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 12:00:37,877 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T04:00:36.991126+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 12:00:42,458 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T04:00:36.991126+00:00', try_number=1, map_index=-1)
2026-02-13 12:00:42,467 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T04:00:36.991126+00:00, map_index=-1, run_start_date=2026-02-13 04:00:41.386106+00:00, run_end_date=2026-02-13 04:00:41.894495+00:00, run_duration=0.508389, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=55, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 04:00:37.871705+00:00, queued_by_job_id=41, pid=7993
2026-02-13 12:00:44,981 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 04:00:36.991126+00:00: manual__2026-02-13T04:00:36.991126+00:00, state:running, queued_at: 2026-02-13 04:00:37.010364+00:00. externally triggered: True> successful
2026-02-13 12:00:44,982 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 04:00:36.991126+00:00, run_id=manual__2026-02-13T04:00:36.991126+00:00, run_start_date=2026-02-13 04:00:37.845866+00:00, run_end_date=2026-02-13 04:00:44.982304+00:00, run_duration=7.136438, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 04:00:36.991126+00:00, data_interval_end=2026-02-13 04:00:36.991126+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 12:00:44,992 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>
2026-02-13 12:00:44,993 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 12:00:44,994 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>
2026-02-13 12:00:44,997 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T04:00:41.913811+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 12:00:44,998 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T04:00:41.913811+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 12:00:44,999 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T04:00:41.913811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 12:00:45,002 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T04:00:41.913811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 12:00:48,637 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T04:00:41.913811+00:00', try_number=1, map_index=-1)
2026-02-13 12:00:48,646 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T04:00:41.913811+00:00, map_index=-1, run_start_date=2026-02-13 04:00:47.820152+00:00, run_end_date=2026-02-13 04:00:48.130048+00:00, run_duration=0.309896, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=56, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 04:00:44.995486+00:00, queued_by_job_id=41, pid=7996
2026-02-13 12:00:51,386 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 04:00:41.913811+00:00: dataset_triggered__2026-02-13T04:00:41.913811+00:00, state:running, queued_at: 2026-02-13 04:00:44.951380+00:00. externally triggered: False> successful
2026-02-13 12:00:51,388 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 04:00:41.913811+00:00, run_id=dataset_triggered__2026-02-13T04:00:41.913811+00:00, run_start_date=2026-02-13 04:00:44.966340+00:00, run_end_date=2026-02-13 04:00:51.387965+00:00, run_duration=6.421625, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 04:00:36.991126+00:00, data_interval_end=2026-02-13 04:00:36.991126+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
2026-02-13 12:03:36,505 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 12:04:19,944 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 12:04:20,828 INFO - Sending Signals.SIGTERM to group 6580. PIDs of all processes in the group: []
2026-02-13 12:04:20,829 INFO - Sending the signal Signals.SIGTERM to group 6580
2026-02-13 12:04:20,830 INFO - Sending the signal Signals.SIGTERM to process 6580 as process group is missing.
2026-02-13 12:04:20,841 INFO - Sending Signals.SIGTERM to group 6580. PIDs of all processes in the group: []
2026-02-13 12:04:20,842 INFO - Sending the signal Signals.SIGTERM to group 6580
2026-02-13 12:04:20,842 INFO - Sending the signal Signals.SIGTERM to process 6580 as process group is missing.
2026-02-13 12:04:20,843 INFO - Exited execute loop
2026-02-13 19:42:32,372 INFO - Loaded executor: SequentialExecutor
2026-02-13 19:42:33,149 INFO - Starting the scheduler
2026-02-13 19:42:33,150 INFO - Processing each file at most -1 times
2026-02-13 19:42:33,154 INFO - Launched DagFileProcessorManager with pid: 1145
2026-02-13 19:42:33,163 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 19:47:33,229 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 19:50:54,997 INFO - Exiting gracefully upon receiving signal 15
2026-02-13 19:50:55,522 INFO - Sending Signals.SIGTERM to group 1145. PIDs of all processes in the group: []
2026-02-13 19:50:55,523 INFO - Sending the signal Signals.SIGTERM to group 1145
2026-02-13 19:50:55,524 INFO - Sending the signal Signals.SIGTERM to process 1145 as process group is missing.
2026-02-13 19:50:55,532 INFO - Sending Signals.SIGTERM to group 1145. PIDs of all processes in the group: []
2026-02-13 19:50:55,532 INFO - Sending the signal Signals.SIGTERM to group 1145
2026-02-13 19:50:55,533 INFO - Sending the signal Signals.SIGTERM to process 1145 as process group is missing.
2026-02-13 19:50:55,534 INFO - Exited execute loop
2026-02-13 19:50:57,816 INFO - Loaded executor: SequentialExecutor
2026-02-13 19:50:58,423 INFO - Starting the scheduler
2026-02-13 19:50:58,424 INFO - Processing each file at most -1 times
2026-02-13 19:50:58,430 INFO - Launched DagFileProcessorManager with pid: 1967
2026-02-13 19:50:58,434 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 19:55:58,497 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:00:58,534 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:05:43,697 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>
2026-02-13 20:05:43,698 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 20:05:43,699 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>
2026-02-13 20:05:43,702 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:05:42.321399+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:05:43,703 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:05:42.321399+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:05:43,704 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:05:42.321399+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:05:43,706 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:05:42.321399+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:05:47,540 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:05:42.321399+00:00', try_number=1, map_index=-1)
2026-02-13 20:05:47,554 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:05:42.321399+00:00, map_index=-1, run_start_date=2026-02-13 12:05:46.682805+00:00, run_end_date=2026-02-13 12:05:46.990071+00:00, run_duration=0.307266, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=59, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:05:43.701172+00:00, queued_by_job_id=58, pid=3698
2026-02-13 20:05:50,213 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:05:42.321399+00:00: manual__2026-02-13T12:05:42.321399+00:00, state:running, queued_at: 2026-02-13 12:05:42.344376+00:00. externally triggered: True> successful
2026-02-13 20:05:50,215 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:05:42.321399+00:00, run_id=manual__2026-02-13T12:05:42.321399+00:00, run_start_date=2026-02-13 12:05:43.663188+00:00, run_end_date=2026-02-13 12:05:50.215457+00:00, run_duration=6.552269, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:05:42.321399+00:00, data_interval_end=2026-02-13 12:05:42.321399+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 20:05:50,227 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>
2026-02-13 20:05:50,228 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 20:05:50,228 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>
2026-02-13 20:05:50,231 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:05:47.008811+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:05:50,231 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:05:47.008811+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:05:50,232 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:05:47.008811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:05:50,235 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:05:47.008811+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:05:53,881 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:05:47.008811+00:00', try_number=1, map_index=-1)
2026-02-13 20:05:53,890 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:05:47.008811+00:00, map_index=-1, run_start_date=2026-02-13 12:05:53.046067+00:00, run_end_date=2026-02-13 12:05:53.334332+00:00, run_duration=0.288265, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=60, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:05:50.229712+00:00, queued_by_job_id=58, pid=3703
2026-02-13 20:05:56,508 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:05:47.008811+00:00: dataset_triggered__2026-02-13T12:05:47.008811+00:00, state:running, queued_at: 2026-02-13 12:05:50.177063+00:00. externally triggered: False> successful
2026-02-13 20:05:56,510 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:05:47.008811+00:00, run_id=dataset_triggered__2026-02-13T12:05:47.008811+00:00, run_start_date=2026-02-13 12:05:50.200171+00:00, run_end_date=2026-02-13 12:05:56.510050+00:00, run_duration=6.309879, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:05:42.321399+00:00, data_interval_end=2026-02-13 12:05:42.321399+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
2026-02-13 20:05:58,580 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:09:33,541 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>
2026-02-13 20:09:33,542 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 20:09:33,543 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>
2026-02-13 20:09:33,545 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:09:29.883245+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:09:33,547 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:09:29.883245+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:09:33,547 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:09:29.883245+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:09:33,550 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:09:29.883245+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:09:37,223 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:09:29.883245+00:00', try_number=1, map_index=-1)
2026-02-13 20:09:37,235 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:09:29.883245+00:00, map_index=-1, run_start_date=2026-02-13 12:09:36.390309+00:00, run_end_date=2026-02-13 12:09:36.700380+00:00, run_duration=0.310071, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=61, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:09:33.544335+00:00, queued_by_job_id=58, pid=4043
2026-02-13 20:09:40,006 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:09:29.883245+00:00: manual__2026-02-13T12:09:29.883245+00:00, state:running, queued_at: 2026-02-13 12:09:29.901006+00:00. externally triggered: True> successful
2026-02-13 20:09:40,007 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:09:29.883245+00:00, run_id=manual__2026-02-13T12:09:29.883245+00:00, run_start_date=2026-02-13 12:09:33.517270+00:00, run_end_date=2026-02-13 12:09:40.007568+00:00, run_duration=6.490298, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:09:29.883245+00:00, data_interval_end=2026-02-13 12:09:29.883245+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 20:09:40,017 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>
2026-02-13 20:09:40,018 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 20:09:40,019 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>
2026-02-13 20:09:40,021 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:09:36.719651+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:09:40,022 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:09:36.719651+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:09:40,023 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:09:36.719651+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:09:40,025 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:09:36.719651+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:09:43,612 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:09:36.719651+00:00', try_number=1, map_index=-1)
2026-02-13 20:09:43,623 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:09:36.719651+00:00, map_index=-1, run_start_date=2026-02-13 12:09:42.727518+00:00, run_end_date=2026-02-13 12:09:43.057478+00:00, run_duration=0.32996, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=62, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:09:40.020429+00:00, queued_by_job_id=58, pid=4046
2026-02-13 20:09:43,675 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:09:36.719651+00:00: dataset_triggered__2026-02-13T12:09:36.719651+00:00, state:running, queued_at: 2026-02-13 12:09:39.972173+00:00. externally triggered: False> successful
2026-02-13 20:09:43,676 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:09:36.719651+00:00, run_id=dataset_triggered__2026-02-13T12:09:36.719651+00:00, run_start_date=2026-02-13 12:09:39.990831+00:00, run_end_date=2026-02-13 12:09:43.676394+00:00, run_duration=3.685563, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:09:29.883245+00:00, data_interval_end=2026-02-13 12:09:29.883245+00:00, dag_hash=14c7fa46f6353af733f2f3432dad8780
2026-02-13 20:10:58,656 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:13:51,739 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>
2026-02-13 20:13:51,741 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 20:13:51,742 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>
2026-02-13 20:13:51,745 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:13:50.192029+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:13:51,746 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:13:50.192029+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:13:51,747 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:13:50.192029+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:13:51,751 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:13:50.192029+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:13:55,931 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:13:50.192029+00:00', try_number=1, map_index=-1)
2026-02-13 20:13:55,940 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:13:50.192029+00:00, map_index=-1, run_start_date=2026-02-13 12:13:55.058956+00:00, run_end_date=2026-02-13 12:13:55.377104+00:00, run_duration=0.318148, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=63, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:51.743848+00:00, queued_by_job_id=58, pid=4129
2026-02-13 20:13:58,602 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:13:50.192029+00:00: manual__2026-02-13T12:13:50.192029+00:00, state:running, queued_at: 2026-02-13 12:13:50.210479+00:00. externally triggered: True> successful
2026-02-13 20:13:58,603 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:13:50.192029+00:00, run_id=manual__2026-02-13T12:13:50.192029+00:00, run_start_date=2026-02-13 12:13:51.717860+00:00, run_end_date=2026-02-13 12:13:58.603159+00:00, run_duration=6.885299, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:13:50.192029+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 20:13:58,614 INFO - 2 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>
2026-02-13 20:13:58,615 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 20:13:58,615 INFO - DAG consumer_dw_order_stat has 0/16 running and queued tasks
2026-02-13 20:13:58,616 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>
2026-02-13 20:13:58,620 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:13:55.394014+00:00 [scheduled]>, <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:13:55.395586+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:13:58,621 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.394014+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:13:58,622 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:13:55.394014+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:13:58,623 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.395586+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:13:58,624 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:13:55.395586+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:13:58,626 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:13:55.394014+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:14:02,003 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:13:55.395586+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:14:05,778 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.394014+00:00', try_number=1, map_index=-1)
2026-02-13 20:14:05,781 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:13:55.395586+00:00', try_number=1, map_index=-1)
2026-02-13 20:14:05,792 INFO - TaskInstance Finished: dag_id=consumer_dw_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T12:13:55.395586+00:00, map_index=-1, run_start_date=2026-02-13 12:14:04.714562+00:00, run_end_date=2026-02-13 12:14:05.087446+00:00, run_duration=0.372884, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=65, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:58.618017+00:00, queued_by_job_id=58, pid=4137
2026-02-13 20:14:05,794 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:13:55.394014+00:00, map_index=-1, run_start_date=2026-02-13 12:14:01.165983+00:00, run_end_date=2026-02-13 12:14:01.461394+00:00, run_duration=0.295411, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=64, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:13:58.618017+00:00, queued_by_job_id=58, pid=4135
2026-02-13 20:14:08,181 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:13:55.394014+00:00: dataset_triggered__2026-02-13T12:13:55.394014+00:00, state:running, queued_at: 2026-02-13 12:13:58.566381+00:00. externally triggered: False> successful
2026-02-13 20:14:08,182 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:13:55.394014+00:00, run_id=dataset_triggered__2026-02-13T12:13:55.394014+00:00, run_start_date=2026-02-13 12:13:58.580294+00:00, run_end_date=2026-02-13 12:14:08.182001+00:00, run_duration=9.601707, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:13:50.192029+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=812fdc4eaaef0b4b0ecbeb4a4a10f6ba
2026-02-13 20:14:08,186 INFO - Marking run <DagRun consumer_dw_order_stat @ 2026-02-13 12:13:55.395586+00:00: dataset_triggered__2026-02-13T12:13:55.395586+00:00, state:running, queued_at: 2026-02-13 12:13:58.543810+00:00. externally triggered: False> successful
2026-02-13 20:14:08,187 INFO - DagRun Finished: dag_id=consumer_dw_order_stat, execution_date=2026-02-13 12:13:55.395586+00:00, run_id=dataset_triggered__2026-02-13T12:13:55.395586+00:00, run_start_date=2026-02-13 12:13:58.580476+00:00, run_end_date=2026-02-13 12:14:08.187176+00:00, run_duration=9.6067, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 12:13:50.192029+00:00, dag_hash=c48b04123af906598e7caee12eaa4367
2026-02-13 20:15:58,690 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:20:58,733 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:21:51,040 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>
2026-02-13 20:21:51,041 INFO - DAG producer_dw_order_sync has 0/16 running and queued tasks
2026-02-13 20:21:51,042 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>
2026-02-13 20:21:51,044 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dw_order_sync.sync_order_data manual__2026-02-13T12:21:48.664790+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:21:51,046 INFO - Sending TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:21:48.664790+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:21:51,046 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:21:48.664790+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:21:51,049 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dw_order_sync', 'sync_order_data', 'manual__2026-02-13T12:21:48.664790+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:21:54,887 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-13T12:21:48.664790+00:00', try_number=1, map_index=-1)
2026-02-13 20:21:54,896 INFO - TaskInstance Finished: dag_id=producer_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-13T12:21:48.664790+00:00, map_index=-1, run_start_date=2026-02-13 12:21:53.977335+00:00, run_end_date=2026-02-13 12:21:54.269269+00:00, run_duration=0.291934, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=66, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:51.043245+00:00, queued_by_job_id=58, pid=4284
2026-02-13 20:21:54,978 INFO - Marking run <DagRun producer_dw_order_sync @ 2026-02-13 12:21:48.664790+00:00: manual__2026-02-13T12:21:48.664790+00:00, state:running, queued_at: 2026-02-13 12:21:48.677286+00:00. externally triggered: True> successful
2026-02-13 20:21:54,979 INFO - DagRun Finished: dag_id=producer_dw_order_sync, execution_date=2026-02-13 12:21:48.664790+00:00, run_id=manual__2026-02-13T12:21:48.664790+00:00, run_start_date=2026-02-13 12:21:51.020627+00:00, run_end_date=2026-02-13 12:21:54.979822+00:00, run_duration=3.959195, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=3f1d48a4dcef25843133576f388d134e
2026-02-13 20:21:56,044 INFO - 2 tasks up for execution:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>
2026-02-13 20:21:56,045 INFO - DAG consumer_dw_order_clean has 0/16 running and queued tasks
2026-02-13 20:21:56,046 INFO - DAG consumer_dw_order_stat has 0/16 running and queued tasks
2026-02-13 20:21:56,046 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>
	<TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>
2026-02-13 20:21:56,049 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dw_order_clean.clean_order_data dataset_triggered__2026-02-13T12:21:54.286709+00:00 [scheduled]>, <TaskInstance: consumer_dw_order_stat.stat_order_data dataset_triggered__2026-02-13T12:21:54.288058+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 20:21:56,051 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.286709+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:21:56,051 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:21:54.286709+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:21:56,052 INFO - Sending TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.288058+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 20:21:56,053 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:21:54.288058+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:21:56,056 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T12:21:54.286709+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:21:59,594 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dw_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T12:21:54.288058+00:00', '--local', '--subdir', 'DAGS_FOLDER/dataset_demo.py']
2026-02-13 20:22:03,258 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.286709+00:00', try_number=1, map_index=-1)
2026-02-13 20:22:03,261 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dw_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T12:21:54.288058+00:00', try_number=1, map_index=-1)
2026-02-13 20:22:03,270 INFO - TaskInstance Finished: dag_id=consumer_dw_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T12:21:54.288058+00:00, map_index=-1, run_start_date=2026-02-13 12:22:02.385933+00:00, run_end_date=2026-02-13 12:22:02.704072+00:00, run_duration=0.318139, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=68, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:56.047787+00:00, queued_by_job_id=58, pid=4291
2026-02-13 20:22:03,271 INFO - TaskInstance Finished: dag_id=consumer_dw_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T12:21:54.286709+00:00, map_index=-1, run_start_date=2026-02-13 12:21:58.731908+00:00, run_end_date=2026-02-13 12:21:59.040815+00:00, run_duration=0.308907, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=67, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 12:21:56.047787+00:00, queued_by_job_id=58, pid=4286
2026-02-13 20:22:03,319 INFO - Marking run <DagRun consumer_dw_order_clean @ 2026-02-13 12:21:54.286709+00:00: dataset_triggered__2026-02-13T12:21:54.286709+00:00, state:running, queued_at: 2026-02-13 12:21:54.960377+00:00. externally triggered: False> successful
2026-02-13 20:22:03,320 INFO - DagRun Finished: dag_id=consumer_dw_order_clean, execution_date=2026-02-13 12:21:54.286709+00:00, run_id=dataset_triggered__2026-02-13T12:21:54.286709+00:00, run_start_date=2026-02-13 12:21:56.016376+00:00, run_end_date=2026-02-13 12:22:03.320464+00:00, run_duration=7.304088, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=812fdc4eaaef0b4b0ecbeb4a4a10f6ba
2026-02-13 20:22:03,326 INFO - Marking run <DagRun consumer_dw_order_stat @ 2026-02-13 12:21:54.288058+00:00: dataset_triggered__2026-02-13T12:21:54.288058+00:00, state:running, queued_at: 2026-02-13 12:21:54.951524+00:00. externally triggered: False> successful
2026-02-13 20:22:03,326 INFO - DagRun Finished: dag_id=consumer_dw_order_stat, execution_date=2026-02-13 12:21:54.288058+00:00, run_id=dataset_triggered__2026-02-13T12:21:54.288058+00:00, run_start_date=2026-02-13 12:21:56.016534+00:00, run_end_date=2026-02-13 12:22:03.326873+00:00, run_duration=7.310339, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 12:21:48.664790+00:00, data_interval_end=2026-02-13 12:21:48.664790+00:00, dag_hash=c48b04123af906598e7caee12eaa4367
2026-02-13 20:25:58,762 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:30:59,716 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:35:47,170 INFO - Heartbeat recovered after 52.16 seconds
2026-02-13 20:36:43,312 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 20:46:08,120 INFO - Heartbeat recovered after 566.01 seconds
2026-02-13 20:56:12,320 INFO - Heartbeat recovered after 564.29 seconds
2026-02-13 21:03:46,056 INFO - Heartbeat recovered after 412.95 seconds
2026-02-13 21:07:02,437 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:12:02,479 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:17:02,520 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:19:19,678 INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>
2026-02-13 21:19:19,679 INFO - DAG producer_variable_sync has 0/16 running and queued tasks
2026-02-13 21:19:19,680 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>
2026-02-13 21:19:19,683 INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.sync_order_data manual__2026-02-13T13:19:17.469369+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:19:19,684 INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='sync_order_data', run_id='manual__2026-02-13T13:19:17.469369+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:19:19,685 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'sync_order_data', 'manual__2026-02-13T13:19:17.469369+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:19:19,687 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'sync_order_data', 'manual__2026-02-13T13:19:17.469369+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:19:23,528 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='sync_order_data', run_id='manual__2026-02-13T13:19:17.469369+00:00', try_number=1, map_index=-1)
2026-02-13 21:19:23,537 INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=sync_order_data, run_id=manual__2026-02-13T13:19:17.469369+00:00, map_index=-1, run_start_date=2026-02-13 13:19:22.495248+00:00, run_end_date=2026-02-13 13:19:22.877515+00:00, run_duration=0.382267, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:19:19.681950+00:00, queued_by_job_id=58, pid=5226
2026-02-13 21:19:25,891 INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:19:17.469369+00:00: manual__2026-02-13T13:19:17.469369+00:00, state:running, queued_at: 2026-02-13 13:19:17.493292+00:00. externally triggered: True> successful
2026-02-13 21:19:25,892 INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:19:17.469369+00:00, run_id=manual__2026-02-13T13:19:17.469369+00:00, run_start_date=2026-02-13 13:19:19.656584+00:00, run_end_date=2026-02-13 13:19:25.892123+00:00, run_duration=6.235539, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:19:17.469369+00:00, data_interval_end=2026-02-13 13:19:17.469369+00:00, dag_hash=2a5b3cdcb15949c1bf7f46b81e786a9a
2026-02-13 21:19:25,902 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>
2026-02-13 21:19:25,903 INFO - DAG consumer_variable has 0/16 running and queued tasks
2026-02-13 21:19:25,903 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>
2026-02-13 21:19:25,905 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.clean_order_data dataset_triggered__2026-02-13T13:19:22.894249+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:19:25,906 INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T13:19:22.894249+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:19:25,907 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'clean_order_data', 'dataset_triggered__2026-02-13T13:19:22.894249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:19:25,910 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'clean_order_data', 'dataset_triggered__2026-02-13T13:19:22.894249+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:19:29,339 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T13:19:22.894249+00:00', try_number=1, map_index=-1)
2026-02-13 21:19:29,346 INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T13:19:22.894249+00:00, map_index=-1, run_start_date=2026-02-13 13:19:28.423596+00:00, run_end_date=2026-02-13 13:19:28.783442+00:00, run_duration=0.359846, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=70, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:19:25.904698+00:00, queued_by_job_id=58, pid=5229
2026-02-13 21:19:31,724 INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:19:22.894249+00:00: dataset_triggered__2026-02-13T13:19:22.894249+00:00, state:running, queued_at: 2026-02-13 13:19:25.861534+00:00. externally triggered: False> successful
2026-02-13 21:19:31,725 INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:19:22.894249+00:00, run_id=dataset_triggered__2026-02-13T13:19:22.894249+00:00, run_start_date=2026-02-13 13:19:25.879140+00:00, run_end_date=2026-02-13 13:19:31.725056+00:00, run_duration=5.845916, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 03:03:00.730591+00:00, data_interval_end=2026-02-13 13:19:17.469369+00:00, dag_hash=dc521a83a3ec097ba48717ae4aff0641
2026-02-13 21:22:04,558 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:27:05,895 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:28:31,067 INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>
2026-02-13 21:28:31,068 INFO - DAG producer_variable_sync has 0/16 running and queued tasks
2026-02-13 21:28:31,070 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>
2026-02-13 21:28:31,072 INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:28:30.405028+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:28:31,073 INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:28:30.405028+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:28:31,074 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:28:30.405028+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:28:31,076 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:28:30.405028+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:28:35,156 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:28:30.405028+00:00', try_number=1, map_index=-1)
2026-02-13 21:28:35,166 INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=producer_variable_save_data, run_id=manual__2026-02-13T13:28:30.405028+00:00, map_index=-1, run_start_date=2026-02-13 13:28:34.258092+00:00, run_end_date=2026-02-13 13:28:34.611299+00:00, run_duration=0.353207, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=71, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:28:31.071275+00:00, queued_by_job_id=58, pid=5475
2026-02-13 21:28:37,689 INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:28:30.405028+00:00: manual__2026-02-13T13:28:30.405028+00:00, state:running, queued_at: 2026-02-13 13:28:30.415552+00:00. externally triggered: True> successful
2026-02-13 21:28:37,690 INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:28:30.405028+00:00, run_id=manual__2026-02-13T13:28:30.405028+00:00, run_start_date=2026-02-13 13:28:31.048918+00:00, run_end_date=2026-02-13 13:28:37.690420+00:00, run_duration=6.641502, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:28:30.405028+00:00, data_interval_end=2026-02-13 13:28:30.405028+00:00, dag_hash=47cb4504eff6f0d33ccb9eb1ea3c16ae
2026-02-13 21:28:37,700 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>
2026-02-13 21:28:37,701 INFO - DAG consumer_variable has 0/16 running and queued tasks
2026-02-13 21:28:37,702 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>
2026-02-13 21:28:37,704 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:28:34.631014+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:28:37,705 INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:28:34.631014+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:28:37,706 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:28:34.631014+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:28:37,708 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:28:34.631014+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:28:41,345 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:28:34.631014+00:00', try_number=1, map_index=-1)
2026-02-13 21:28:41,354 INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=consumer_variable_read_data, run_id=dataset_triggered__2026-02-13T13:28:34.631014+00:00, map_index=-1, run_start_date=2026-02-13 13:28:40.356836+00:00, run_end_date=2026-02-13 13:28:40.752507+00:00, run_duration=0.395671, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=72, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:28:37.702957+00:00, queued_by_job_id=58, pid=5478
2026-02-13 21:28:43,942 INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:28:34.631014+00:00: dataset_triggered__2026-02-13T13:28:34.631014+00:00, state:running, queued_at: 2026-02-13 13:28:37.659479+00:00. externally triggered: False> successful
2026-02-13 21:28:43,943 INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:28:34.631014+00:00, run_id=dataset_triggered__2026-02-13T13:28:34.631014+00:00, run_start_date=2026-02-13 13:28:37.675820+00:00, run_end_date=2026-02-13 13:28:43.943383+00:00, run_duration=6.267563, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:28:30.405028+00:00, data_interval_end=2026-02-13 13:28:30.405028+00:00, dag_hash=ea6f50333c860200cf7319bc15b25ef1
2026-02-13 21:31:43,552 INFO - 1 tasks up for execution:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>
2026-02-13 21:31:43,553 INFO - DAG producer_variable_sync has 0/16 running and queued tasks
2026-02-13 21:31:43,553 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>
2026-02-13 21:31:43,556 INFO - Trying to enqueue tasks: [<TaskInstance: producer_variable_sync.producer_variable_save_data manual__2026-02-13T13:31:41.629277+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:31:43,557 INFO - Sending TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:31:41.629277+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:31:43,558 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:31:41.629277+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:31:43,560 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_variable_sync', 'producer_variable_save_data', 'manual__2026-02-13T13:31:41.629277+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:31:47,575 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_variable_sync', task_id='producer_variable_save_data', run_id='manual__2026-02-13T13:31:41.629277+00:00', try_number=1, map_index=-1)
2026-02-13 21:31:47,585 INFO - TaskInstance Finished: dag_id=producer_variable_sync, task_id=producer_variable_save_data, run_id=manual__2026-02-13T13:31:41.629277+00:00, map_index=-1, run_start_date=2026-02-13 13:31:46.632761+00:00, run_end_date=2026-02-13 13:31:47.023060+00:00, run_duration=0.390299, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:31:43.554757+00:00, queued_by_job_id=58, pid=5562
2026-02-13 21:31:47,689 INFO - Marking run <DagRun producer_variable_sync @ 2026-02-13 13:31:41.629277+00:00: manual__2026-02-13T13:31:41.629277+00:00, state:running, queued_at: 2026-02-13 13:31:41.647968+00:00. externally triggered: True> successful
2026-02-13 21:31:47,690 INFO - DagRun Finished: dag_id=producer_variable_sync, execution_date=2026-02-13 13:31:41.629277+00:00, run_id=manual__2026-02-13T13:31:41.629277+00:00, run_start_date=2026-02-13 13:31:43.532261+00:00, run_end_date=2026-02-13 13:31:47.690795+00:00, run_duration=4.158534, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:31:41.629277+00:00, data_interval_end=2026-02-13 13:31:41.629277+00:00, dag_hash=47cb4504eff6f0d33ccb9eb1ea3c16ae
2026-02-13 21:31:48,750 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>
2026-02-13 21:31:48,751 INFO - DAG consumer_variable has 0/16 running and queued tasks
2026-02-13 21:31:48,752 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>
2026-02-13 21:31:48,755 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_variable.consumer_variable_read_data dataset_triggered__2026-02-13T13:31:47.043088+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:31:48,756 INFO - Sending TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:31:47.043088+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:31:48,757 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:31:47.043088+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:31:48,759 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_variable', 'consumer_variable_read_data', 'dataset_triggered__2026-02-13T13:31:47.043088+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_variable.py']
2026-02-13 21:31:52,346 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_variable', task_id='consumer_variable_read_data', run_id='dataset_triggered__2026-02-13T13:31:47.043088+00:00', try_number=1, map_index=-1)
2026-02-13 21:31:52,356 INFO - TaskInstance Finished: dag_id=consumer_variable, task_id=consumer_variable_read_data, run_id=dataset_triggered__2026-02-13T13:31:47.043088+00:00, map_index=-1, run_start_date=2026-02-13 13:31:51.450539+00:00, run_end_date=2026-02-13 13:31:51.793322+00:00, run_duration=0.342783, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=74, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:31:48.753858+00:00, queued_by_job_id=58, pid=5565
2026-02-13 21:31:52,397 INFO - Marking run <DagRun consumer_variable @ 2026-02-13 13:31:47.043088+00:00: dataset_triggered__2026-02-13T13:31:47.043088+00:00, state:running, queued_at: 2026-02-13 13:31:47.668267+00:00. externally triggered: False> successful
2026-02-13 21:31:52,398 INFO - DagRun Finished: dag_id=consumer_variable, execution_date=2026-02-13 13:31:47.043088+00:00, run_id=dataset_triggered__2026-02-13T13:31:47.043088+00:00, run_start_date=2026-02-13 13:31:48.728389+00:00, run_end_date=2026-02-13 13:31:52.398312+00:00, run_duration=3.669923, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:31:41.629277+00:00, data_interval_end=2026-02-13 13:31:41.629277+00:00, dag_hash=ea6f50333c860200cf7319bc15b25ef1
2026-02-13 21:32:05,939 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:37:07,679 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:39:02,998 INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>
2026-02-13 21:39:02,999 INFO - DAG producer_object_sync has 0/16 running and queued tasks
2026-02-13 21:39:03,000 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>
2026-02-13 21:39:03,002 INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:38:59.836012+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:39:03,003 INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:38:59.836012+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:39:03,005 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:38:59.836012+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:39:03,007 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:38:59.836012+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:39:06,811 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:38:59.836012+00:00', try_number=1, map_index=-1)
2026-02-13 21:39:06,821 INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:38:59.836012+00:00, map_index=-1, run_start_date=2026-02-13 13:39:05.940543+00:00, run_end_date=2026-02-13 13:39:06.286740+00:00, run_duration=0.346197, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:39:03.001168+00:00, queued_by_job_id=58, pid=5812
2026-02-13 21:39:09,593 ERROR - Marking run <DagRun producer_object_sync @ 2026-02-13 13:38:59.836012+00:00: manual__2026-02-13T13:38:59.836012+00:00, state:running, queued_at: 2026-02-13 13:38:59.851509+00:00. externally triggered: True> failed
2026-02-13 21:39:09,594 INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:38:59.836012+00:00, run_id=manual__2026-02-13T13:38:59.836012+00:00, run_start_date=2026-02-13 13:39:02.978128+00:00, run_end_date=2026-02-13 13:39:09.594334+00:00, run_duration=6.616206, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:38:59.836012+00:00, data_interval_end=2026-02-13 13:38:59.836012+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
2026-02-13 21:42:07,722 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:42:54,108 INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>
2026-02-13 21:42:54,110 INFO - DAG producer_object_sync has 0/16 running and queued tasks
2026-02-13 21:42:54,111 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>
2026-02-13 21:42:54,115 INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:42:53.973560+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:42:54,116 INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:42:53.973560+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:42:54,117 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:42:53.973560+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:42:54,120 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:42:53.973560+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:42:58,061 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:42:53.973560+00:00', try_number=1, map_index=-1)
2026-02-13 21:42:58,072 INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:42:53.973560+00:00, map_index=-1, run_start_date=2026-02-13 13:42:57.153377+00:00, run_end_date=2026-02-13 13:42:57.481345+00:00, run_duration=0.327968, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=76, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:42:54.113360+00:00, queued_by_job_id=58, pid=5890
2026-02-13 21:42:58,121 ERROR - Marking run <DagRun producer_object_sync @ 2026-02-13 13:42:53.973560+00:00: manual__2026-02-13T13:42:53.973560+00:00, state:running, queued_at: 2026-02-13 13:42:53.990856+00:00. externally triggered: True> failed
2026-02-13 21:42:58,122 INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:42:53.973560+00:00, run_id=manual__2026-02-13T13:42:53.973560+00:00, run_start_date=2026-02-13 13:42:54.085628+00:00, run_end_date=2026-02-13 13:42:58.122290+00:00, run_duration=4.036662, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:42:53.973560+00:00, data_interval_end=2026-02-13 13:42:53.973560+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
2026-02-13 21:43:38,119 INFO - 1 tasks up for execution:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>
2026-02-13 21:43:38,119 INFO - DAG producer_object_sync has 0/16 running and queued tasks
2026-02-13 21:43:38,120 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>
2026-02-13 21:43:38,122 INFO - Trying to enqueue tasks: [<TaskInstance: producer_object_sync.producer_object_save_data manual__2026-02-13T13:43:37.197651+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:43:38,123 INFO - Sending TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:43:37.197651+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:43:38,124 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:43:37.197651+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:43:38,127 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_object_sync', 'producer_object_save_data', 'manual__2026-02-13T13:43:37.197651+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:43:41,738 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_object_sync', task_id='producer_object_save_data', run_id='manual__2026-02-13T13:43:37.197651+00:00', try_number=1, map_index=-1)
2026-02-13 21:43:41,753 INFO - TaskInstance Finished: dag_id=producer_object_sync, task_id=producer_object_save_data, run_id=manual__2026-02-13T13:43:37.197651+00:00, map_index=-1, run_start_date=2026-02-13 13:43:40.845454+00:00, run_end_date=2026-02-13 13:43:41.203293+00:00, run_duration=0.357839, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:43:38.121725+00:00, queued_by_job_id=58, pid=5930
2026-02-13 21:43:41,819 INFO - Marking run <DagRun producer_object_sync @ 2026-02-13 13:43:37.197651+00:00: manual__2026-02-13T13:43:37.197651+00:00, state:running, queued_at: 2026-02-13 13:43:37.209001+00:00. externally triggered: True> successful
2026-02-13 21:43:41,820 INFO - DagRun Finished: dag_id=producer_object_sync, execution_date=2026-02-13 13:43:37.197651+00:00, run_id=manual__2026-02-13T13:43:37.197651+00:00, run_start_date=2026-02-13 13:43:38.088254+00:00, run_end_date=2026-02-13 13:43:41.820298+00:00, run_duration=3.732044, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:43:37.197651+00:00, data_interval_end=2026-02-13 13:43:37.197651+00:00, dag_hash=7d555625b44488c0b76434bc238566ca
2026-02-13 21:43:45,429 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>
2026-02-13 21:43:45,430 INFO - DAG consumer_object has 0/16 running and queued tasks
2026-02-13 21:43:45,431 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>
2026-02-13 21:43:45,433 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_object.consumer_object_read_data dataset_triggered__2026-02-13T13:43:41.221425+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:43:45,434 INFO - Sending TaskInstanceKey(dag_id='consumer_object', task_id='consumer_object_read_data', run_id='dataset_triggered__2026-02-13T13:43:41.221425+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:43:45,434 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_object', 'consumer_object_read_data', 'dataset_triggered__2026-02-13T13:43:41.221425+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:43:45,437 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_object', 'consumer_object_read_data', 'dataset_triggered__2026-02-13T13:43:41.221425+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_object.py']
2026-02-13 21:43:48,831 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_object', task_id='consumer_object_read_data', run_id='dataset_triggered__2026-02-13T13:43:41.221425+00:00', try_number=1, map_index=-1)
2026-02-13 21:43:48,841 INFO - TaskInstance Finished: dag_id=consumer_object, task_id=consumer_object_read_data, run_id=dataset_triggered__2026-02-13T13:43:41.221425+00:00, map_index=-1, run_start_date=2026-02-13 13:43:47.997174+00:00, run_end_date=2026-02-13 13:43:48.343217+00:00, run_duration=0.346043, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=78, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:43:45.431998+00:00, queued_by_job_id=58, pid=5934
2026-02-13 21:43:51,539 INFO - Marking run <DagRun consumer_object @ 2026-02-13 13:43:41.221425+00:00: dataset_triggered__2026-02-13T13:43:41.221425+00:00, state:running, queued_at: 2026-02-13 13:43:41.800292+00:00. externally triggered: False> successful
2026-02-13 21:43:51,540 INFO - DagRun Finished: dag_id=consumer_object, execution_date=2026-02-13 13:43:41.221425+00:00, run_id=dataset_triggered__2026-02-13T13:43:41.221425+00:00, run_start_date=2026-02-13 13:43:45.407571+00:00, run_end_date=2026-02-13 13:43:51.540699+00:00, run_duration=6.133128, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:43:37.197651+00:00, data_interval_end=2026-02-13 13:43:37.197651+00:00, dag_hash=9b4f52719dbf1e041ad0cea210d36db5
2026-02-13 21:47:07,760 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:51:55,988 INFO - Heartbeat recovered after 31.24 seconds
2026-02-13 21:52:07,535 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>
2026-02-13 21:52:07,535 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 21:52:07,536 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>
2026-02-13 21:52:07,538 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:52:06.718290+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:52:07,539 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:52:06.718290+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:52:07,540 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:52:06.718290+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:52:07,542 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:52:06.718290+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:52:11,323 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:52:06.718290+00:00', try_number=1, map_index=-1)
2026-02-13 21:52:11,333 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:52:06.718290+00:00, map_index=-1, run_start_date=2026-02-13 13:52:10.400351+00:00, run_end_date=2026-02-13 13:52:10.729860+00:00, run_duration=0.329509, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=79, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:52:07.537647+00:00, queued_by_job_id=58, pid=6332
2026-02-13 21:52:11,362 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:52:14,130 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:14,142 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:52:06.718290+00:00: manual__2026-02-13T13:52:06.718290+00:00, state:running, queued_at: 2026-02-13 13:52:06.732084+00:00. externally triggered: True> successful
2026-02-13 21:52:14,143 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:52:06.718290+00:00, run_id=manual__2026-02-13T13:52:06.718290+00:00, run_start_date=2026-02-13 13:52:07.514774+00:00, run_end_date=2026-02-13 13:52:14.143258+00:00, run_duration=6.628484, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:52:06.718290+00:00, data_interval_end=2026-02-13 13:52:06.718290+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 21:52:17,893 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:21,713 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:22,763 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:22,946 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:23,993 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:25,034 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:28,558 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:29,614 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:33,248 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:36,508 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:40,325 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:41,368 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:41,564 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:42,627 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:43,293 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:43,610 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:47,287 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:50,962 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:54,576 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:55,650 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:52:56,691 ERROR - DAG 'consumer_dagRun' not found in serialized_dag table
2026-02-13 21:54:49,428 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>
2026-02-13 21:54:49,429 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 21:54:49,429 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>
2026-02-13 21:54:49,432 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:52:10.746398+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:54:49,433 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:52:10.746398+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:54:49,434 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:52:10.746398+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:54:49,436 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:52:10.746398+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:54:53,066 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:52:10.746398+00:00', try_number=1, map_index=-1)
2026-02-13 21:54:53,076 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:52:10.746398+00:00, map_index=-1, run_start_date=2026-02-13 13:54:52.208560+00:00, run_end_date=2026-02-13 13:54:52.544442+00:00, run_duration=0.335882, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:54:49.430891+00:00, queued_by_job_id=58, pid=6449
2026-02-13 21:54:56,219 ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:52:10.746398+00:00: dataset_triggered__2026-02-13T13:52:10.746398+00:00, state:running, queued_at: 2026-02-13 13:54:49.394621+00:00. externally triggered: False> failed
2026-02-13 21:54:56,220 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:52:10.746398+00:00, run_id=dataset_triggered__2026-02-13T13:52:10.746398+00:00, run_start_date=2026-02-13 13:54:49.408553+00:00, run_end_date=2026-02-13 13:54:56.220773+00:00, run_duration=6.81222, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:52:06.718290+00:00, data_interval_end=2026-02-13 13:52:06.718290+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 21:55:14,007 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>
2026-02-13 21:55:14,009 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 21:55:14,009 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>
2026-02-13 21:55:14,013 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:55:12.814268+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:55:14,014 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:55:12.814268+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:55:14,015 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:55:12.814268+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:55:14,017 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:55:12.814268+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:55:17,669 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:55:12.814268+00:00', try_number=1, map_index=-1)
2026-02-13 21:55:17,678 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:55:12.814268+00:00, map_index=-1, run_start_date=2026-02-13 13:55:16.717090+00:00, run_end_date=2026-02-13 13:55:17.073853+00:00, run_duration=0.356763, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=81, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:55:14.011004+00:00, queued_by_job_id=58, pid=6502
2026-02-13 21:55:20,368 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:55:12.814268+00:00: manual__2026-02-13T13:55:12.814268+00:00, state:running, queued_at: 2026-02-13 13:55:12.825993+00:00. externally triggered: True> successful
2026-02-13 21:55:20,369 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:55:12.814268+00:00, run_id=manual__2026-02-13T13:55:12.814268+00:00, run_start_date=2026-02-13 13:55:13.982701+00:00, run_end_date=2026-02-13 13:55:20.368974+00:00, run_duration=6.386273, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:55:12.814268+00:00, data_interval_end=2026-02-13 13:55:12.814268+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 21:55:20,379 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>
2026-02-13 21:55:20,380 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 21:55:20,381 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>
2026-02-13 21:55:20,384 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:55:17.093128+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:55:20,385 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:55:17.093128+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:55:20,385 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:55:17.093128+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:55:20,388 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:55:17.093128+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:55:24,307 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:55:17.093128+00:00', try_number=1, map_index=-1)
2026-02-13 21:55:24,325 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:55:17.093128+00:00, map_index=-1, run_start_date=2026-02-13 13:55:23.391484+00:00, run_end_date=2026-02-13 13:55:23.734496+00:00, run_duration=0.343012, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:55:20.382361+00:00, queued_by_job_id=58, pid=6513
2026-02-13 21:55:27,141 ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:55:17.093128+00:00: dataset_triggered__2026-02-13T13:55:17.093128+00:00, state:running, queued_at: 2026-02-13 13:55:20.336102+00:00. externally triggered: False> failed
2026-02-13 21:55:27,142 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:55:17.093128+00:00, run_id=dataset_triggered__2026-02-13T13:55:17.093128+00:00, run_start_date=2026-02-13 13:55:20.353740+00:00, run_end_date=2026-02-13 13:55:27.142224+00:00, run_duration=6.788484, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:55:12.814268+00:00, data_interval_end=2026-02-13 13:55:12.814268+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 21:57:11,554 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 21:57:49,410 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>
2026-02-13 21:57:49,411 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 21:57:49,411 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>
2026-02-13 21:57:49,414 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T13:57:46.936479+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:57:49,415 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:57:46.936479+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:57:49,415 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:57:46.936479+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:57:49,418 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T13:57:46.936479+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:57:53,060 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T13:57:46.936479+00:00', try_number=1, map_index=-1)
2026-02-13 21:57:53,069 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T13:57:46.936479+00:00, map_index=-1, run_start_date=2026-02-13 13:57:52.121692+00:00, run_end_date=2026-02-13 13:57:52.477999+00:00, run_duration=0.356307, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:57:49.412761+00:00, queued_by_job_id=58, pid=6600
2026-02-13 21:57:55,764 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 13:57:46.936479+00:00: manual__2026-02-13T13:57:46.936479+00:00, state:running, queued_at: 2026-02-13 13:57:46.947402+00:00. externally triggered: True> successful
2026-02-13 21:57:55,765 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 13:57:46.936479+00:00, run_id=manual__2026-02-13T13:57:46.936479+00:00, run_start_date=2026-02-13 13:57:49.390250+00:00, run_end_date=2026-02-13 13:57:55.765280+00:00, run_duration=6.37503, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 13:57:46.936479+00:00, data_interval_end=2026-02-13 13:57:46.936479+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 21:57:55,775 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>
2026-02-13 21:57:55,776 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 21:57:55,776 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>
2026-02-13 21:57:55,779 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T13:57:52.497306+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 21:57:55,780 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:57:52.497306+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 21:57:55,780 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:57:52.497306+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:57:55,783 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T13:57:52.497306+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 21:57:59,135 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T13:57:52.497306+00:00', try_number=1, map_index=-1)
2026-02-13 21:57:59,145 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T13:57:52.497306+00:00, map_index=-1, run_start_date=2026-02-13 13:57:58.299452+00:00, run_end_date=2026-02-13 13:57:58.648892+00:00, run_duration=0.34944, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=84, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 13:57:55.777895+00:00, queued_by_job_id=58, pid=6605
2026-02-13 21:58:01,737 INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 13:57:52.497306+00:00: dataset_triggered__2026-02-13T13:57:52.497306+00:00, state:running, queued_at: 2026-02-13 13:57:55.735901+00:00. externally triggered: False> successful
2026-02-13 21:58:01,739 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 13:57:52.497306+00:00, run_id=dataset_triggered__2026-02-13T13:57:52.497306+00:00, run_start_date=2026-02-13 13:57:55.751523+00:00, run_end_date=2026-02-13 13:58:01.738990+00:00, run_duration=5.987467, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 13:57:46.936479+00:00, data_interval_end=2026-02-13 13:57:46.936479+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:02:12,674 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:06:36,977 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>
2026-02-13 22:06:36,978 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 22:06:36,979 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>
2026-02-13 22:06:36,982 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:06:35.989891+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:06:36,983 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:06:35.989891+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:06:36,984 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:06:35.989891+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:06:36,987 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:06:35.989891+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:06:40,628 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:06:35.989891+00:00', try_number=1, map_index=-1)
2026-02-13 22:06:40,637 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:06:35.989891+00:00, map_index=-1, run_start_date=2026-02-13 14:06:39.723961+00:00, run_end_date=2026-02-13 14:06:40.072376+00:00, run_duration=0.348415, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=85, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:06:36.980892+00:00, queued_by_job_id=58, pid=6815
2026-02-13 22:06:40,707 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:06:35.989891+00:00: manual__2026-02-13T14:06:35.989891+00:00, state:running, queued_at: 2026-02-13 14:06:36.008553+00:00. externally triggered: True> successful
2026-02-13 22:06:40,708 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:06:35.989891+00:00, run_id=manual__2026-02-13T14:06:35.989891+00:00, run_start_date=2026-02-13 14:06:36.954082+00:00, run_end_date=2026-02-13 14:06:40.708210+00:00, run_duration=3.754128, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:06:35.989891+00:00, data_interval_end=2026-02-13 14:06:35.989891+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 22:06:41,764 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>
2026-02-13 22:06:41,765 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 22:06:41,766 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>
2026-02-13 22:06:41,768 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:06:40.090635+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:06:41,769 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:06:40.090635+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:06:41,770 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:06:40.090635+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:06:41,772 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:06:40.090635+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:06:45,440 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:06:40.090635+00:00', try_number=1, map_index=-1)
2026-02-13 22:06:45,449 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:06:40.090635+00:00, map_index=-1, run_start_date=2026-02-13 14:06:44.502527+00:00, run_end_date=2026-02-13 14:06:44.869335+00:00, run_duration=0.366808, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:06:41.767241+00:00, queued_by_job_id=58, pid=6818
2026-02-13 22:06:47,915 INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:06:40.090635+00:00: dataset_triggered__2026-02-13T14:06:40.090635+00:00, state:running, queued_at: 2026-02-13 14:06:40.689164+00:00. externally triggered: False> successful
2026-02-13 22:06:47,916 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:06:40.090635+00:00, run_id=dataset_triggered__2026-02-13T14:06:40.090635+00:00, run_start_date=2026-02-13 14:06:41.744462+00:00, run_end_date=2026-02-13 14:06:47.916201+00:00, run_duration=6.171739, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:06:35.989891+00:00, data_interval_end=2026-02-13 14:06:35.989891+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:07:12,550 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:10:21,604 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>
2026-02-13 22:10:21,605 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 22:10:21,605 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>
2026-02-13 22:10:21,608 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:10:18.122537+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:10:21,609 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:10:18.122537+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:10:21,609 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:10:18.122537+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:10:21,612 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:10:18.122537+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:10:25,302 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:10:18.122537+00:00', try_number=1, map_index=-1)
2026-02-13 22:10:25,313 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:10:18.122537+00:00, map_index=-1, run_start_date=2026-02-13 14:10:24.346755+00:00, run_end_date=2026-02-13 14:10:24.735967+00:00, run_duration=0.389212, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:10:21.607014+00:00, queued_by_job_id=58, pid=6947
2026-02-13 22:10:27,802 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:10:18.122537+00:00: manual__2026-02-13T14:10:18.122537+00:00, state:running, queued_at: 2026-02-13 14:10:18.133454+00:00. externally triggered: True> successful
2026-02-13 22:10:27,803 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:10:18.122537+00:00, run_id=manual__2026-02-13T14:10:18.122537+00:00, run_start_date=2026-02-13 14:10:21.584849+00:00, run_end_date=2026-02-13 14:10:27.803156+00:00, run_duration=6.218307, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:10:18.122537+00:00, data_interval_end=2026-02-13 14:10:18.122537+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 22:10:27,812 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>
2026-02-13 22:10:27,812 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 22:10:27,813 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>
2026-02-13 22:10:27,815 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:10:24.755287+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:10:27,816 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:10:24.755287+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:10:27,817 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:10:24.755287+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:10:27,819 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:10:24.755287+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:10:31,354 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:10:24.755287+00:00', try_number=1, map_index=-1)
2026-02-13 22:10:31,365 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:10:24.755287+00:00, map_index=-1, run_start_date=2026-02-13 14:10:30.335444+00:00, run_end_date=2026-02-13 14:10:30.752163+00:00, run_duration=0.416719, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=88, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:10:27.814468+00:00, queued_by_job_id=58, pid=6950
2026-02-13 22:10:33,872 INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:10:24.755287+00:00: dataset_triggered__2026-02-13T14:10:24.755287+00:00, state:running, queued_at: 2026-02-13 14:10:27.773909+00:00. externally triggered: False> successful
2026-02-13 22:10:33,873 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:10:24.755287+00:00, run_id=dataset_triggered__2026-02-13T14:10:24.755287+00:00, run_start_date=2026-02-13 14:10:27.789025+00:00, run_end_date=2026-02-13 14:10:33.873466+00:00, run_duration=6.084441, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:10:18.122537+00:00, data_interval_end=2026-02-13 14:10:18.122537+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:12:13,179 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:13:59,439 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>
2026-02-13 22:13:59,440 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 22:13:59,441 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>
2026-02-13 22:13:59,443 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:13:57.660372+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:13:59,444 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:13:57.660372+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:13:59,445 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:13:57.660372+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:13:59,447 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:13:57.660372+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:14:02,992 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:13:57.660372+00:00', try_number=1, map_index=-1)
2026-02-13 22:14:03,002 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:13:57.660372+00:00, map_index=-1, run_start_date=2026-02-13 14:14:02.134441+00:00, run_end_date=2026-02-13 14:14:02.467478+00:00, run_duration=0.333037, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=89, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:13:59.442521+00:00, queued_by_job_id=58, pid=7044
2026-02-13 22:14:05,542 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:13:57.660372+00:00: manual__2026-02-13T14:13:57.660372+00:00, state:running, queued_at: 2026-02-13 14:13:57.671428+00:00. externally triggered: True> successful
2026-02-13 22:14:05,543 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:13:57.660372+00:00, run_id=manual__2026-02-13T14:13:57.660372+00:00, run_start_date=2026-02-13 14:13:59.418194+00:00, run_end_date=2026-02-13 14:14:05.543694+00:00, run_duration=6.1255, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:13:57.660372+00:00, data_interval_end=2026-02-13 14:13:57.660372+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 22:14:05,554 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>
2026-02-13 22:14:05,555 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 22:14:05,555 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>
2026-02-13 22:14:05,558 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:14:02.484868+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:14:05,559 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:14:02.484868+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:14:05,560 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:14:02.484868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:14:05,562 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:14:02.484868+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:14:09,256 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:14:02.484868+00:00', try_number=1, map_index=-1)
2026-02-13 22:14:09,266 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:14:02.484868+00:00, map_index=-1, run_start_date=2026-02-13 14:14:08.328089+00:00, run_end_date=2026-02-13 14:14:08.672120+00:00, run_duration=0.344031, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=90, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:14:05.557023+00:00, queued_by_job_id=58, pid=7047
2026-02-13 22:14:11,726 ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:14:02.484868+00:00: dataset_triggered__2026-02-13T14:14:02.484868+00:00, state:running, queued_at: 2026-02-13 14:14:05.513229+00:00. externally triggered: False> failed
2026-02-13 22:14:11,727 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:14:02.484868+00:00, run_id=dataset_triggered__2026-02-13T14:14:02.484868+00:00, run_start_date=2026-02-13 14:14:05.528608+00:00, run_end_date=2026-02-13 14:14:11.727269+00:00, run_duration=6.198661, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:13:57.660372+00:00, data_interval_end=2026-02-13 14:13:57.660372+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:16:13,521 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>
2026-02-13 22:16:13,523 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 22:16:13,524 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>
2026-02-13 22:16:13,530 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:16:12.947371+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:16:13,532 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:16:12.947371+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:16:13,535 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:16:12.947371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:16:13,538 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:16:12.947371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:16:17,388 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:16:12.947371+00:00', try_number=1, map_index=-1)
2026-02-13 22:16:17,397 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:16:12.947371+00:00, map_index=-1, run_start_date=2026-02-13 14:16:16.438898+00:00, run_end_date=2026-02-13 14:16:16.785992+00:00, run_duration=0.347094, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=91, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:16:13.528314+00:00, queued_by_job_id=58, pid=7109
2026-02-13 22:16:19,827 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:16:12.947371+00:00: manual__2026-02-13T14:16:12.947371+00:00, state:running, queued_at: 2026-02-13 14:16:12.958807+00:00. externally triggered: True> successful
2026-02-13 22:16:19,828 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:16:12.947371+00:00, run_id=manual__2026-02-13T14:16:12.947371+00:00, run_start_date=2026-02-13 14:16:13.494028+00:00, run_end_date=2026-02-13 14:16:19.828416+00:00, run_duration=6.334388, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:16:12.947371+00:00, data_interval_end=2026-02-13 14:16:12.947371+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 22:16:19,839 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>
2026-02-13 22:16:19,840 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 22:16:19,841 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>
2026-02-13 22:16:19,843 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:16:16.802861+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:16:19,845 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:16:16.802861+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:16:19,845 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:16:16.802861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:16:19,859 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:16:16.802861+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:16:23,430 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:16:16.802861+00:00', try_number=1, map_index=-1)
2026-02-13 22:16:23,440 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:16:16.802861+00:00, map_index=-1, run_start_date=2026-02-13 14:16:22.517118+00:00, run_end_date=2026-02-13 14:16:22.859699+00:00, run_duration=0.342581, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=92, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:16:19.842621+00:00, queued_by_job_id=58, pid=7112
2026-02-13 22:16:25,793 ERROR - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:16:16.802861+00:00: dataset_triggered__2026-02-13T14:16:16.802861+00:00, state:running, queued_at: 2026-02-13 14:16:19.797692+00:00. externally triggered: False> failed
2026-02-13 22:16:25,794 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:16:16.802861+00:00, run_id=dataset_triggered__2026-02-13T14:16:16.802861+00:00, run_start_date=2026-02-13 14:16:19.813628+00:00, run_end_date=2026-02-13 14:16:25.794565+00:00, run_duration=5.980937, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:16:12.947371+00:00, data_interval_end=2026-02-13 14:16:12.947371+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:17:15,957 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:18:36,852 INFO - 1 tasks up for execution:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>
2026-02-13 22:18:36,853 INFO - DAG producer_dagRun_sync has 0/16 running and queued tasks
2026-02-13 22:18:36,854 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>
2026-02-13 22:18:36,856 INFO - Trying to enqueue tasks: [<TaskInstance: producer_dagRun_sync.producer_dagRun_read_data manual__2026-02-13T14:18:33.086772+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:18:36,857 INFO - Sending TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:18:33.086772+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:18:36,857 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:18:33.086772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:18:36,860 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_dagRun_sync', 'producer_dagRun_read_data', 'manual__2026-02-13T14:18:33.086772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:18:40,386 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_dagRun_sync', task_id='producer_dagRun_read_data', run_id='manual__2026-02-13T14:18:33.086772+00:00', try_number=1, map_index=-1)
2026-02-13 22:18:40,396 INFO - TaskInstance Finished: dag_id=producer_dagRun_sync, task_id=producer_dagRun_read_data, run_id=manual__2026-02-13T14:18:33.086772+00:00, map_index=-1, run_start_date=2026-02-13 14:18:39.380049+00:00, run_end_date=2026-02-13 14:18:39.772192+00:00, run_duration=0.392143, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=93, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:18:36.855081+00:00, queued_by_job_id=58, pid=7167
2026-02-13 22:18:42,913 INFO - Marking run <DagRun producer_dagRun_sync @ 2026-02-13 14:18:33.086772+00:00: manual__2026-02-13T14:18:33.086772+00:00, state:running, queued_at: 2026-02-13 14:18:33.097380+00:00. externally triggered: True> successful
2026-02-13 22:18:42,915 INFO - DagRun Finished: dag_id=producer_dagRun_sync, execution_date=2026-02-13 14:18:33.086772+00:00, run_id=manual__2026-02-13T14:18:33.086772+00:00, run_start_date=2026-02-13 14:18:36.832831+00:00, run_end_date=2026-02-13 14:18:42.914955+00:00, run_duration=6.082124, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 14:18:33.086772+00:00, data_interval_end=2026-02-13 14:18:33.086772+00:00, dag_hash=25f10c75cc4c07b582101befabedd510
2026-02-13 22:18:42,923 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>
2026-02-13 22:18:42,924 INFO - DAG consumer_dagRun has 0/16 running and queued tasks
2026-02-13 22:18:42,925 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>
2026-02-13 22:18:42,927 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_dagRun.consumer_dagRun_read_data dataset_triggered__2026-02-13T14:18:39.791063+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-13 22:18:42,928 INFO - Sending TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:18:39.791063+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-13 22:18:42,929 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:18:39.791063+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:18:42,931 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_dagRun', 'consumer_dagRun_read_data', 'dataset_triggered__2026-02-13T14:18:39.791063+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_dagRun.py']
2026-02-13 22:18:46,451 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_dagRun', task_id='consumer_dagRun_read_data', run_id='dataset_triggered__2026-02-13T14:18:39.791063+00:00', try_number=1, map_index=-1)
2026-02-13 22:18:46,462 INFO - TaskInstance Finished: dag_id=consumer_dagRun, task_id=consumer_dagRun_read_data, run_id=dataset_triggered__2026-02-13T14:18:39.791063+00:00, map_index=-1, run_start_date=2026-02-13 14:18:45.469212+00:00, run_end_date=2026-02-13 14:18:45.856053+00:00, run_duration=0.386841, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=94, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 14:18:42.926427+00:00, queued_by_job_id=58, pid=7170
2026-02-13 22:18:48,993 INFO - Marking run <DagRun consumer_dagRun @ 2026-02-13 14:18:39.791063+00:00: dataset_triggered__2026-02-13T14:18:39.791063+00:00, state:running, queued_at: 2026-02-13 14:18:42.889311+00:00. externally triggered: False> successful
2026-02-13 22:18:48,994 INFO - DagRun Finished: dag_id=consumer_dagRun, execution_date=2026-02-13 14:18:39.791063+00:00, run_id=dataset_triggered__2026-02-13T14:18:39.791063+00:00, run_start_date=2026-02-13 14:18:42.901593+00:00, run_end_date=2026-02-13 14:18:48.994842+00:00, run_duration=6.093249, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 14:18:33.086772+00:00, data_interval_end=2026-02-13 14:18:33.086772+00:00, dag_hash=02234ad8a70e7998ae33f6010c2f1915
2026-02-13 22:22:17,944 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:27:19,824 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 22:40:51,979 INFO - Heartbeat recovered after 571.90 seconds
2026-02-13 22:50:59,301 INFO - Heartbeat recovered after 563.76 seconds
2026-02-13 22:50:59,308 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 23:01:06,531 INFO - Heartbeat recovered after 563.61 seconds
2026-02-13 23:11:12,154 INFO - Heartbeat recovered after 563.49 seconds
2026-02-13 23:21:20,678 INFO - Heartbeat recovered after 562.76 seconds
2026-02-13 23:31:30,594 INFO - Heartbeat recovered after 561.78 seconds
2026-02-13 23:41:33,667 INFO - Heartbeat recovered after 563.32 seconds
2026-02-13 23:51:41,698 INFO - Heartbeat recovered after 563.13 seconds
2026-02-13 23:51:44,900 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-13 23:56:53,714 INFO - Heartbeat recovered after 254.35 seconds
2026-02-14 00:00:01,286 INFO - Setting next_dagrun for dw_order_sync to 2026-02-13 16:00:00+00:00, run_after=2026-02-14 16:00:00+00:00
2026-02-14 00:00:01,317 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:01,318 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-14 00:00:01,319 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:01,321 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.sync_order_data scheduled__2026-02-12T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:00:01,322 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
2026-02-14 00:00:01,323 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:01,325 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'sync_order_data', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:04,849 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='sync_order_data', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 00:00:04,858 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=sync_order_data, run_id=scheduled__2026-02-12T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 16:00:03.979413+00:00, run_end_date=2026-02-13 16:00:04.328890+00:00, run_duration=0.349477, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=95, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2026-02-13 16:00:01.320394+00:00, queued_by_job_id=58, pid=7943
2026-02-14 00:00:04,930 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:04,931 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-14 00:00:04,932 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:04,934 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test scheduled__2026-02-12T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:00:04,935 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-14 00:00:04,935 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:04,938 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:18,552 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 00:00:18,562 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test, run_id=scheduled__2026-02-12T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 16:00:07.576342+00:00, run_end_date=2026-02-13 16:00:17.993253+00:00, run_duration=10.416911, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=96, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-13 16:00:04.933235+00:00, queued_by_job_id=58, pid=7947
2026-02-14 00:00:20,968 INFO - 1 tasks up for execution:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:20,969 INFO - DAG dw_order_sync has 0/16 running and queued tasks
2026-02-14 00:00:20,969 INFO - Setting the following tasks to queued state:
	<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T16:00:00+00:00 [scheduled]>
2026-02-14 00:00:20,972 INFO - Trying to enqueue tasks: [<TaskInstance: dw_order_sync.run_automation_test2 scheduled__2026-02-12T16:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:00:20,973 INFO - Sending TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:00:20,974 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:20,976 INFO - Executing command: ['airflow', 'tasks', 'run', 'dw_order_sync', 'run_automation_test2', 'scheduled__2026-02-12T16:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/tag_data_warehouse/dw_order_sync.py']
2026-02-14 00:00:30,194 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='dw_order_sync', task_id='run_automation_test2', run_id='scheduled__2026-02-12T16:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 00:00:30,204 INFO - TaskInstance Finished: dag_id=dw_order_sync, task_id=run_automation_test2, run_id=scheduled__2026-02-12T16:00:00+00:00, map_index=-1, run_start_date=2026-02-13 16:00:23.620092+00:00, run_end_date=2026-02-13 16:00:29.702270+00:00, run_duration=6.082178, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=97, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:00:20.970755+00:00, queued_by_job_id=58, pid=7978
2026-02-14 00:00:33,560 INFO - Marking run <DagRun dw_order_sync @ 2026-02-12 16:00:00+00:00: scheduled__2026-02-12T16:00:00+00:00, state:running, queued_at: 2026-02-13 16:00:01.279749+00:00. externally triggered: False> successful
2026-02-14 00:00:33,563 INFO - DagRun Finished: dag_id=dw_order_sync, execution_date=2026-02-12 16:00:00+00:00, run_id=scheduled__2026-02-12T16:00:00+00:00, run_start_date=2026-02-13 16:00:01.295060+00:00, run_end_date=2026-02-13 16:00:33.563140+00:00, run_duration=32.26808, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-12 16:00:00+00:00, data_interval_end=2026-02-13 16:00:00+00:00, dag_hash=e441836cdc8ba3fd622c724e955ce635
2026-02-14 00:00:33,572 INFO - Setting next_dagrun for dw_order_sync to 2026-02-13 16:00:00+00:00, run_after=2026-02-14 16:00:00+00:00
2026-02-14 00:00:58,043 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:02:55,975 INFO - Orphaning unreferenced dataset 'dataset://xcom/'
2026-02-14 00:05:08,446 INFO - Exiting gracefully upon receiving signal 15
2026-02-14 00:05:09,313 INFO - Sending Signals.SIGTERM to group 1967. PIDs of all processes in the group: []
2026-02-14 00:05:09,315 INFO - Sending the signal Signals.SIGTERM to group 1967
2026-02-14 00:05:09,316 INFO - Sending the signal Signals.SIGTERM to process 1967 as process group is missing.
2026-02-14 00:05:09,328 INFO - Sending Signals.SIGTERM to group 1967. PIDs of all processes in the group: []
2026-02-14 00:05:09,328 INFO - Sending the signal Signals.SIGTERM to group 1967
2026-02-14 00:05:09,329 INFO - Sending the signal Signals.SIGTERM to process 1967 as process group is missing.
2026-02-14 00:05:09,330 INFO - Exited execute loop
2026-02-14 00:05:11,444 INFO - Loaded executor: SequentialExecutor
2026-02-14 00:05:11,974 INFO - Starting the scheduler
2026-02-14 00:05:11,975 INFO - Processing each file at most -1 times
2026-02-14 00:05:11,981 INFO - Launched DagFileProcessorManager with pid: 8376
2026-02-14 00:05:11,986 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:05:58,194 INFO - 1 tasks up for execution:
	<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>
2026-02-14 00:05:58,195 INFO - DAG producer_xcom_sync has 0/16 running and queued tasks
2026-02-14 00:05:58,196 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>
2026-02-14 00:05:58,200 INFO - Trying to enqueue tasks: [<TaskInstance: producer_xcom_sync.producer_xcom_save_data manual__2026-02-13T16:05:55.895216+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:05:58,200 INFO - Sending TaskInstanceKey(dag_id='producer_xcom_sync', task_id='producer_xcom_save_data', run_id='manual__2026-02-13T16:05:55.895216+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:05:58,201 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_xcom_sync', 'producer_xcom_save_data', 'manual__2026-02-13T16:05:55.895216+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:05:58,204 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_xcom_sync', 'producer_xcom_save_data', 'manual__2026-02-13T16:05:55.895216+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:06:01,825 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_xcom_sync', task_id='producer_xcom_save_data', run_id='manual__2026-02-13T16:05:55.895216+00:00', try_number=1, map_index=-1)
2026-02-14 00:06:01,839 INFO - TaskInstance Finished: dag_id=producer_xcom_sync, task_id=producer_xcom_save_data, run_id=manual__2026-02-13T16:05:55.895216+00:00, map_index=-1, run_start_date=2026-02-13 16:06:00.749982+00:00, run_end_date=2026-02-13 16:06:01.099449+00:00, run_duration=0.349467, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=99, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:05:58.197995+00:00, queued_by_job_id=98, pid=8426
2026-02-14 00:06:04,561 INFO - Marking run <DagRun producer_xcom_sync @ 2026-02-13 16:05:55.895216+00:00: manual__2026-02-13T16:05:55.895216+00:00, state:running, queued_at: 2026-02-13 16:05:55.916304+00:00. externally triggered: True> successful
2026-02-14 00:06:04,562 INFO - DagRun Finished: dag_id=producer_xcom_sync, execution_date=2026-02-13 16:05:55.895216+00:00, run_id=manual__2026-02-13T16:05:55.895216+00:00, run_start_date=2026-02-13 16:05:58.158330+00:00, run_end_date=2026-02-13 16:06:04.562528+00:00, run_duration=6.404198, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=122444dd0c644311f38ec0d9e938cb3b
2026-02-14 00:06:04,572 INFO - 2 tasks up for execution:
	<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>
	<TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>
2026-02-14 00:06:04,573 INFO - DAG consumer_xcom2 has 0/16 running and queued tasks
2026-02-14 00:06:04,574 INFO - DAG consumer_xcom1 has 0/16 running and queued tasks
2026-02-14 00:06:04,575 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>
	<TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>
2026-02-14 00:06:04,578 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_xcom2.consumer_xcom_read_data2 dataset_triggered__2026-02-13T16:06:01.114736+00:00 [scheduled]>, <TaskInstance: consumer_xcom1.consumer_xcom_read_data1 dataset_triggered__2026-02-13T16:06:01.116213+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:06:04,579 INFO - Sending TaskInstanceKey(dag_id='consumer_xcom2', task_id='consumer_xcom_read_data2', run_id='dataset_triggered__2026-02-13T16:06:01.114736+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:06:04,579 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_xcom2', 'consumer_xcom_read_data2', 'dataset_triggered__2026-02-13T16:06:01.114736+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:06:04,580 INFO - Sending TaskInstanceKey(dag_id='consumer_xcom1', task_id='consumer_xcom_read_data1', run_id='dataset_triggered__2026-02-13T16:06:01.116213+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:06:04,581 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_xcom1', 'consumer_xcom_read_data1', 'dataset_triggered__2026-02-13T16:06:01.116213+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:06:04,583 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_xcom2', 'consumer_xcom_read_data2', 'dataset_triggered__2026-02-13T16:06:01.114736+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:06:08,150 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_xcom1', 'consumer_xcom_read_data1', 'dataset_triggered__2026-02-13T16:06:01.116213+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_xcom.py']
2026-02-14 00:06:11,952 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_xcom2', task_id='consumer_xcom_read_data2', run_id='dataset_triggered__2026-02-13T16:06:01.114736+00:00', try_number=1, map_index=-1)
2026-02-14 00:06:11,955 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_xcom1', task_id='consumer_xcom_read_data1', run_id='dataset_triggered__2026-02-13T16:06:01.116213+00:00', try_number=1, map_index=-1)
2026-02-14 00:06:11,966 INFO - TaskInstance Finished: dag_id=consumer_xcom2, task_id=consumer_xcom_read_data2, run_id=dataset_triggered__2026-02-13T16:06:01.114736+00:00, map_index=-1, run_start_date=2026-02-13 16:06:07.095447+00:00, run_end_date=2026-02-13 16:06:07.457738+00:00, run_duration=0.362291, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=100, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:06:04.576236+00:00, queued_by_job_id=98, pid=8429
2026-02-14 00:06:11,968 INFO - TaskInstance Finished: dag_id=consumer_xcom1, task_id=consumer_xcom_read_data1, run_id=dataset_triggered__2026-02-13T16:06:01.116213+00:00, map_index=-1, run_start_date=2026-02-13 16:06:10.881821+00:00, run_end_date=2026-02-13 16:06:11.256517+00:00, run_duration=0.374696, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=101, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:06:04.576236+00:00, queued_by_job_id=98, pid=8431
2026-02-14 00:06:14,399 INFO - Marking run <DagRun consumer_xcom2 @ 2026-02-13 16:06:01.114736+00:00: dataset_triggered__2026-02-13T16:06:01.114736+00:00, state:running, queued_at: 2026-02-13 16:06:04.530766+00:00. externally triggered: False> successful
2026-02-14 00:06:14,400 INFO - DagRun Finished: dag_id=consumer_xcom2, execution_date=2026-02-13 16:06:01.114736+00:00, run_id=dataset_triggered__2026-02-13T16:06:01.114736+00:00, run_start_date=2026-02-13 16:06:04.545102+00:00, run_end_date=2026-02-13 16:06:14.400651+00:00, run_duration=9.855549, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=18e83b8aaf201b23343a6d45144041ed
2026-02-14 00:06:14,405 INFO - Marking run <DagRun consumer_xcom1 @ 2026-02-13 16:06:01.116213+00:00: dataset_triggered__2026-02-13T16:06:01.116213+00:00, state:running, queued_at: 2026-02-13 16:06:04.518235+00:00. externally triggered: False> successful
2026-02-14 00:06:14,406 INFO - DagRun Finished: dag_id=consumer_xcom1, execution_date=2026-02-13 16:06:01.116213+00:00, run_id=dataset_triggered__2026-02-13T16:06:01.116213+00:00, run_start_date=2026-02-13 16:06:04.545210+00:00, run_end_date=2026-02-13 16:06:14.406239+00:00, run_duration=9.861029, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:05:55.895216+00:00, data_interval_end=2026-02-13 16:05:55.895216+00:00, dag_hash=5e2a1ff8e090009575648e79d38671d9
2026-02-14 00:10:13,490 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:15:13,529 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:20:13,567 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:25:15,945 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:25:19,569 INFO - 1 tasks up for execution:
	<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>
2026-02-14 00:25:19,570 INFO - DAG producer_DIPOA_dw_order_raw_sync has 0/16 running and queued tasks
2026-02-14 00:25:19,570 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>
2026-02-14 00:25:19,573 INFO - Trying to enqueue tasks: [<TaskInstance: producer_DIPOA_dw_order_raw_sync.sync_raw_order_data manual__2026-02-13T16:25:15.518091+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:25:19,574 INFO - Sending TaskInstanceKey(dag_id='producer_DIPOA_dw_order_raw_sync', task_id='sync_raw_order_data', run_id='manual__2026-02-13T16:25:15.518091+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:25:19,574 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_DIPOA_dw_order_raw_sync', 'sync_raw_order_data', 'manual__2026-02-13T16:25:15.518091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:19,577 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_DIPOA_dw_order_raw_sync', 'sync_raw_order_data', 'manual__2026-02-13T16:25:15.518091+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:23,379 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_DIPOA_dw_order_raw_sync', task_id='sync_raw_order_data', run_id='manual__2026-02-13T16:25:15.518091+00:00', try_number=1, map_index=-1)
2026-02-14 00:25:23,390 INFO - TaskInstance Finished: dag_id=producer_DIPOA_dw_order_raw_sync, task_id=sync_raw_order_data, run_id=manual__2026-02-13T16:25:15.518091+00:00, map_index=-1, run_start_date=2026-02-13 16:25:22.386615+00:00, run_end_date=2026-02-13 16:25:22.769273+00:00, run_duration=0.382658, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=102, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:19.571822+00:00, queued_by_job_id=98, pid=8989
2026-02-14 00:25:25,975 INFO - Marking run <DagRun producer_DIPOA_dw_order_raw_sync @ 2026-02-13 16:25:15.518091+00:00: manual__2026-02-13T16:25:15.518091+00:00, state:running, queued_at: 2026-02-13 16:25:15.533854+00:00. externally triggered: True> successful
2026-02-14 00:25:25,977 INFO - DagRun Finished: dag_id=producer_DIPOA_dw_order_raw_sync, execution_date=2026-02-13 16:25:15.518091+00:00, run_id=manual__2026-02-13T16:25:15.518091+00:00, run_start_date=2026-02-13 16:25:19.549371+00:00, run_end_date=2026-02-13 16:25:25.976934+00:00, run_duration=6.427563, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=24391711b5e16fd626025bfde1dadd75
2026-02-14 00:25:25,986 INFO - 1 tasks up for execution:
	<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>
2026-02-14 00:25:25,987 INFO - DAG consumer1_order_clean has 0/16 running and queued tasks
2026-02-14 00:25:25,987 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>
2026-02-14 00:25:25,990 INFO - Trying to enqueue tasks: [<TaskInstance: consumer1_order_clean.clean_order_data dataset_triggered__2026-02-13T16:25:22.789030+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:25:25,991 INFO - Sending TaskInstanceKey(dag_id='consumer1_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T16:25:22.789030+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:25:25,992 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer1_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T16:25:22.789030+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:25,994 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer1_order_clean', 'clean_order_data', 'dataset_triggered__2026-02-13T16:25:22.789030+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:29,627 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer1_order_clean', task_id='clean_order_data', run_id='dataset_triggered__2026-02-13T16:25:22.789030+00:00', try_number=1, map_index=-1)
2026-02-14 00:25:29,638 INFO - TaskInstance Finished: dag_id=consumer1_order_clean, task_id=clean_order_data, run_id=dataset_triggered__2026-02-13T16:25:22.789030+00:00, map_index=-1, run_start_date=2026-02-13 16:25:28.594463+00:00, run_end_date=2026-02-13 16:25:28.990966+00:00, run_duration=0.396503, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=103, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:25.988916+00:00, queued_by_job_id=98, pid=8992
2026-02-14 00:25:32,110 INFO - Marking run <DagRun consumer1_order_clean @ 2026-02-13 16:25:22.789030+00:00: dataset_triggered__2026-02-13T16:25:22.789030+00:00, state:running, queued_at: 2026-02-13 16:25:25.946870+00:00. externally triggered: False> successful
2026-02-14 00:25:32,111 INFO - DagRun Finished: dag_id=consumer1_order_clean, execution_date=2026-02-13 16:25:22.789030+00:00, run_id=dataset_triggered__2026-02-13T16:25:22.789030+00:00, run_start_date=2026-02-13 16:25:25.963573+00:00, run_end_date=2026-02-13 16:25:32.111196+00:00, run_duration=6.147623, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=3829db3dbf828c470c9a060bc9bfe7f1
2026-02-14 00:25:32,120 INFO - 1 tasks up for execution:
	<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>
2026-02-14 00:25:32,121 INFO - DAG consumer2_order_stat has 0/16 running and queued tasks
2026-02-14 00:25:32,122 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>
2026-02-14 00:25:32,124 INFO - Trying to enqueue tasks: [<TaskInstance: consumer2_order_stat.stat_order_data dataset_triggered__2026-02-13T16:25:29.009309+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 00:25:32,126 INFO - Sending TaskInstanceKey(dag_id='consumer2_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T16:25:29.009309+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 00:25:32,126 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer2_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T16:25:29.009309+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:32,129 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer2_order_stat', 'stat_order_data', 'dataset_triggered__2026-02-13T16:25:29.009309+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_DIPOA.py']
2026-02-14 00:25:35,745 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer2_order_stat', task_id='stat_order_data', run_id='dataset_triggered__2026-02-13T16:25:29.009309+00:00', try_number=1, map_index=-1)
2026-02-14 00:25:35,757 INFO - TaskInstance Finished: dag_id=consumer2_order_stat, task_id=stat_order_data, run_id=dataset_triggered__2026-02-13T16:25:29.009309+00:00, map_index=-1, run_start_date=2026-02-13 16:25:34.752886+00:00, run_end_date=2026-02-13 16:25:35.142861+00:00, run_duration=0.389975, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=104, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-13 16:25:32.123201+00:00, queued_by_job_id=98, pid=8995
2026-02-14 00:25:38,337 INFO - Marking run <DagRun consumer2_order_stat @ 2026-02-13 16:25:29.009309+00:00: dataset_triggered__2026-02-13T16:25:29.009309+00:00, state:running, queued_at: 2026-02-13 16:25:32.084110+00:00. externally triggered: False> successful
2026-02-14 00:25:38,338 INFO - DagRun Finished: dag_id=consumer2_order_stat, execution_date=2026-02-13 16:25:29.009309+00:00, run_id=dataset_triggered__2026-02-13T16:25:29.009309+00:00, run_start_date=2026-02-13 16:25:32.096788+00:00, run_end_date=2026-02-13 16:25:38.338114+00:00, run_duration=6.241326, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-13 16:25:15.518091+00:00, data_interval_end=2026-02-13 16:25:15.518091+00:00, dag_hash=366c5b167887ebcb721d4da129564e00
2026-02-14 00:30:18,234 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 00:34:04,027 INFO - Exiting gracefully upon receiving signal 15
2026-02-14 00:34:05,035 INFO - Sending Signals.SIGTERM to group 8376. PIDs of all processes in the group: [8376]
2026-02-14 00:34:05,038 INFO - Sending the signal Signals.SIGTERM to group 8376
2026-02-14 00:34:05,099 INFO - Process psutil.Process(pid=8376, status='terminated', exitcode=<Negsignal.SIGTERM: -15>, started='00:05:11') (8376) terminated with exit code Negsignal.SIGTERM
2026-02-14 00:34:05,105 INFO - Sending Signals.SIGTERM to group 8376. PIDs of all processes in the group: []
2026-02-14 00:34:05,107 INFO - Sending the signal Signals.SIGTERM to group 8376
2026-02-14 00:34:05,108 INFO - Sending the signal Signals.SIGTERM to process 8376 as process group is missing.
2026-02-14 00:34:05,109 INFO - Exited execute loop
2026-02-14 11:07:57,862 INFO - Loaded executor: SequentialExecutor
2026-02-14 11:07:58,408 INFO - Starting the scheduler
2026-02-14 11:07:58,409 INFO - Processing each file at most -1 times
2026-02-14 11:07:58,416 INFO - Launched DagFileProcessorManager with pid: 1047
2026-02-14 11:07:58,420 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:13:00,155 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:18:00,182 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:23:00,301 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:28:00,470 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:32:47,572 INFO - 1 tasks up for execution:
	<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>
2026-02-14 11:32:47,573 INFO - DAG producer_1_N_sync has 0/16 running and queued tasks
2026-02-14 11:32:47,574 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>
2026-02-14 11:32:47,577 INFO - Trying to enqueue tasks: [<TaskInstance: producer_1_N_sync.producer_1_N_save_data manual__2026-02-14T03:32:46.003371+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:32:47,577 INFO - Sending TaskInstanceKey(dag_id='producer_1_N_sync', task_id='producer_1_N_save_data', run_id='manual__2026-02-14T03:32:46.003371+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:32:47,578 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_1_N_sync', 'producer_1_N_save_data', 'manual__2026-02-14T03:32:46.003371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:32:47,581 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_1_N_sync', 'producer_1_N_save_data', 'manual__2026-02-14T03:32:46.003371+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:32:51,778 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_1_N_sync', task_id='producer_1_N_save_data', run_id='manual__2026-02-14T03:32:46.003371+00:00', try_number=1, map_index=-1)
2026-02-14 11:32:51,791 INFO - TaskInstance Finished: dag_id=producer_1_N_sync, task_id=producer_1_N_save_data, run_id=manual__2026-02-14T03:32:46.003371+00:00, map_index=-1, run_start_date=2026-02-14 03:32:50.698525+00:00, run_end_date=2026-02-14 03:32:51.091467+00:00, run_duration=0.392942, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=106, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:47.575365+00:00, queued_by_job_id=105, pid=2428
2026-02-14 11:32:54,743 INFO - Marking run <DagRun producer_1_N_sync @ 2026-02-14 03:32:46.003371+00:00: manual__2026-02-14T03:32:46.003371+00:00, state:running, queued_at: 2026-02-14 03:32:46.030701+00:00. externally triggered: True> successful
2026-02-14 11:32:54,745 INFO - DagRun Finished: dag_id=producer_1_N_sync, execution_date=2026-02-14 03:32:46.003371+00:00, run_id=manual__2026-02-14T03:32:46.003371+00:00, run_start_date=2026-02-14 03:32:47.531979+00:00, run_end_date=2026-02-14 03:32:54.745296+00:00, run_duration=7.213317, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=9c94ea8be6439fe2a4c875752cf2dfbb
2026-02-14 11:32:54,756 INFO - 2 tasks up for execution:
	<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>
	<TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>
2026-02-14 11:32:54,757 INFO - DAG consumer_1_N_stat_order_data has 0/16 running and queued tasks
2026-02-14 11:32:54,757 INFO - DAG consumer_1_N_clean_order_data has 0/16 running and queued tasks
2026-02-14 11:32:54,758 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>
	<TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>
2026-02-14 11:32:54,762 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_1_N_stat_order_data.consumer_1_N_read_stat_order_data dataset_triggered__2026-02-14T03:32:51.110002+00:00 [scheduled]>, <TaskInstance: consumer_1_N_clean_order_data.consumer_1_N_read_clean_order_data dataset_triggered__2026-02-14T03:32:51.112812+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:32:54,763 INFO - Sending TaskInstanceKey(dag_id='consumer_1_N_stat_order_data', task_id='consumer_1_N_read_stat_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.110002+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:32:54,763 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1_N_stat_order_data', 'consumer_1_N_read_stat_order_data', 'dataset_triggered__2026-02-14T03:32:51.110002+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:32:54,764 INFO - Sending TaskInstanceKey(dag_id='consumer_1_N_clean_order_data', task_id='consumer_1_N_read_clean_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.112812+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:32:54,765 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_data', 'consumer_1_N_read_clean_order_data', 'dataset_triggered__2026-02-14T03:32:51.112812+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:32:54,767 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1_N_stat_order_data', 'consumer_1_N_read_stat_order_data', 'dataset_triggered__2026-02-14T03:32:51.110002+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:32:58,392 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_1_N_clean_order_data', 'consumer_1_N_read_clean_order_data', 'dataset_triggered__2026-02-14T03:32:51.112812+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /1_producer_N_consumer.py']
2026-02-14 11:33:02,475 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1_N_stat_order_data', task_id='consumer_1_N_read_stat_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.110002+00:00', try_number=1, map_index=-1)
2026-02-14 11:33:02,477 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_1_N_clean_order_data', task_id='consumer_1_N_read_clean_order_data', run_id='dataset_triggered__2026-02-14T03:32:51.112812+00:00', try_number=1, map_index=-1)
2026-02-14 11:33:02,491 INFO - TaskInstance Finished: dag_id=consumer_1_N_stat_order_data, task_id=consumer_1_N_read_stat_order_data, run_id=dataset_triggered__2026-02-14T03:32:51.110002+00:00, map_index=-1, run_start_date=2026-02-14 03:32:57.388387+00:00, run_end_date=2026-02-14 03:32:57.754514+00:00, run_duration=0.366127, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=107, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:54.760069+00:00, queued_by_job_id=105, pid=2432
2026-02-14 11:33:02,492 INFO - TaskInstance Finished: dag_id=consumer_1_N_clean_order_data, task_id=consumer_1_N_read_clean_order_data, run_id=dataset_triggered__2026-02-14T03:32:51.112812+00:00, map_index=-1, run_start_date=2026-02-14 03:33:01.472039+00:00, run_end_date=2026-02-14 03:33:01.863441+00:00, run_duration=0.391402, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=108, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:32:54.760069+00:00, queued_by_job_id=105, pid=2437
2026-02-14 11:33:02,523 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:33:05,158 INFO - Marking run <DagRun consumer_1_N_stat_order_data @ 2026-02-14 03:32:51.110002+00:00: dataset_triggered__2026-02-14T03:32:51.110002+00:00, state:running, queued_at: 2026-02-14 03:32:54.715732+00:00. externally triggered: False> successful
2026-02-14 11:33:05,159 INFO - DagRun Finished: dag_id=consumer_1_N_stat_order_data, execution_date=2026-02-14 03:32:51.110002+00:00, run_id=dataset_triggered__2026-02-14T03:32:51.110002+00:00, run_start_date=2026-02-14 03:32:54.727529+00:00, run_end_date=2026-02-14 03:33:05.158995+00:00, run_duration=10.431466, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=94f55b14533c43cd663b6f903564f319
2026-02-14 11:33:05,164 INFO - Marking run <DagRun consumer_1_N_clean_order_data @ 2026-02-14 03:32:51.112812+00:00: dataset_triggered__2026-02-14T03:32:51.112812+00:00, state:running, queued_at: 2026-02-14 03:32:54.699044+00:00. externally triggered: False> successful
2026-02-14 11:33:05,165 INFO - DagRun Finished: dag_id=consumer_1_N_clean_order_data, execution_date=2026-02-14 03:32:51.112812+00:00, run_id=dataset_triggered__2026-02-14T03:32:51.112812+00:00, run_start_date=2026-02-14 03:32:54.727628+00:00, run_end_date=2026-02-14 03:33:05.165330+00:00, run_duration=10.437702, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:32:46.003371+00:00, data_interval_end=2026-02-14 03:32:46.003371+00:00, dag_hash=c61c84e3308591cb31098ffc7795ade0
2026-02-14 11:36:03,831 INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>
2026-02-14 11:36:03,832 INFO - DAG producer_N_1_dw_order_sync has 0/16 running and queued tasks
2026-02-14 11:36:03,832 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>
2026-02-14 11:36:03,835 INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:36:00.776536+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:36:03,836 INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:36:00.776536+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:36:03,836 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:36:00.776536+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:03,839 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:36:00.776536+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:07,738 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:36:00.776536+00:00', try_number=1, map_index=-1)
2026-02-14 11:36:07,752 INFO - TaskInstance Finished: dag_id=producer_N_1_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-14T03:36:00.776536+00:00, map_index=-1, run_start_date=2026-02-14 03:36:06.815869+00:00, run_end_date=2026-02-14 03:36:07.204195+00:00, run_duration=0.388326, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=109, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:03.833916+00:00, queued_by_job_id=105, pid=2607
2026-02-14 11:36:10,397 INFO - Marking run <DagRun producer_N_1_dw_order_sync @ 2026-02-14 03:36:00.776536+00:00: manual__2026-02-14T03:36:00.776536+00:00, state:running, queued_at: 2026-02-14 03:36:00.799507+00:00. externally triggered: True> successful
2026-02-14 11:36:10,398 INFO - DagRun Finished: dag_id=producer_N_1_dw_order_sync, execution_date=2026-02-14 03:36:00.776536+00:00, run_id=manual__2026-02-14T03:36:00.776536+00:00, run_start_date=2026-02-14 03:36:03.811605+00:00, run_end_date=2026-02-14 03:36:10.398453+00:00, run_duration=6.586848, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:36:00.776536+00:00, data_interval_end=2026-02-14 03:36:00.776536+00:00, dag_hash=c52fa1a388ba1be81c6c42bf2d418436
2026-02-14 11:36:24,211 INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>
2026-02-14 11:36:24,213 INFO - DAG producer_N_1_dw_user_sync has 0/16 running and queued tasks
2026-02-14 11:36:24,213 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>
2026-02-14 11:36:24,216 INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:36:20.021965+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:36:24,217 INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:36:20.021965+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:36:24,218 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:36:20.021965+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:24,220 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:36:20.021965+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:28,110 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:36:20.021965+00:00', try_number=1, map_index=-1)
2026-02-14 11:36:28,123 INFO - TaskInstance Finished: dag_id=producer_N_1_dw_user_sync, task_id=sync_user_data, run_id=manual__2026-02-14T03:36:20.021965+00:00, map_index=-1, run_start_date=2026-02-14 03:36:27.111353+00:00, run_end_date=2026-02-14 03:36:27.495680+00:00, run_duration=0.384327, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=110, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:24.214954+00:00, queued_by_job_id=105, pid=2616
2026-02-14 11:36:30,604 INFO - Marking run <DagRun producer_N_1_dw_user_sync @ 2026-02-14 03:36:20.021965+00:00: manual__2026-02-14T03:36:20.021965+00:00, state:running, queued_at: 2026-02-14 03:36:20.035784+00:00. externally triggered: True> successful
2026-02-14 11:36:30,605 INFO - DagRun Finished: dag_id=producer_N_1_dw_user_sync, execution_date=2026-02-14 03:36:20.021965+00:00, run_id=manual__2026-02-14T03:36:20.021965+00:00, run_start_date=2026-02-14 03:36:24.193164+00:00, run_end_date=2026-02-14 03:36:30.605369+00:00, run_duration=6.412205, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:36:20.021965+00:00, data_interval_end=2026-02-14 03:36:20.021965+00:00, dag_hash=f21aec43673b2c968a2fc26e9e7f5cea
2026-02-14 11:36:30,614 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>
2026-02-14 11:36:30,615 INFO - DAG consumer_N_1_order_user_analysis has 0/16 running and queued tasks
2026-02-14 11:36:30,615 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>
2026-02-14 11:36:30,618 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:36:27.516973+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:36:30,619 INFO - Sending TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:36:27.516973+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:36:30,619 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:36:27.516973+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:30,622 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:36:27.516973+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:36:34,161 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:36:27.516973+00:00', try_number=1, map_index=-1)
2026-02-14 11:36:34,171 INFO - TaskInstance Finished: dag_id=consumer_N_1_order_user_analysis, task_id=analyze_order_user, run_id=dataset_triggered__2026-02-14T03:36:27.516973+00:00, map_index=-1, run_start_date=2026-02-14 03:36:33.250171+00:00, run_end_date=2026-02-14 03:36:33.631813+00:00, run_duration=0.381642, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=111, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:36:30.616775+00:00, queued_by_job_id=105, pid=2620
2026-02-14 11:36:36,612 ERROR - Marking run <DagRun consumer_N_1_order_user_analysis @ 2026-02-14 03:36:27.516973+00:00: dataset_triggered__2026-02-14T03:36:27.516973+00:00, state:running, queued_at: 2026-02-14 03:36:30.577081+00:00. externally triggered: False> failed
2026-02-14 11:36:36,613 INFO - DagRun Finished: dag_id=consumer_N_1_order_user_analysis, execution_date=2026-02-14 03:36:27.516973+00:00, run_id=dataset_triggered__2026-02-14T03:36:27.516973+00:00, run_start_date=2026-02-14 03:36:30.591444+00:00, run_end_date=2026-02-14 03:36:36.613428+00:00, run_duration=6.021984, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:36:00.776536+00:00, data_interval_end=2026-02-14 03:36:20.021965+00:00, dag_hash=6baf9b26ce8f026143cfbb65a8bc4568
2026-02-14 11:38:03,284 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:38:57,771 INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>
2026-02-14 11:38:57,772 INFO - DAG producer_N_1_dw_order_sync has 0/16 running and queued tasks
2026-02-14 11:38:57,773 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>
2026-02-14 11:38:57,777 INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_order_sync.sync_order_data manual__2026-02-14T03:38:53.631710+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:38:57,778 INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:38:53.631710+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:38:57,779 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:38:53.631710+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:38:57,782 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_order_sync', 'sync_order_data', 'manual__2026-02-14T03:38:53.631710+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:39:02,244 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_order_sync', task_id='sync_order_data', run_id='manual__2026-02-14T03:38:53.631710+00:00', try_number=1, map_index=-1)
2026-02-14 11:39:02,255 INFO - TaskInstance Finished: dag_id=producer_N_1_dw_order_sync, task_id=sync_order_data, run_id=manual__2026-02-14T03:38:53.631710+00:00, map_index=-1, run_start_date=2026-02-14 03:39:01.278348+00:00, run_end_date=2026-02-14 03:39:01.646406+00:00, run_duration=0.368058, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=112, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:38:57.775611+00:00, queued_by_job_id=105, pid=2724
2026-02-14 11:39:04,761 INFO - Marking run <DagRun producer_N_1_dw_order_sync @ 2026-02-14 03:38:53.631710+00:00: manual__2026-02-14T03:38:53.631710+00:00, state:running, queued_at: 2026-02-14 03:38:53.643631+00:00. externally triggered: True> successful
2026-02-14 11:39:04,762 INFO - DagRun Finished: dag_id=producer_N_1_dw_order_sync, execution_date=2026-02-14 03:38:53.631710+00:00, run_id=manual__2026-02-14T03:38:53.631710+00:00, run_start_date=2026-02-14 03:38:57.749214+00:00, run_end_date=2026-02-14 03:39:04.762095+00:00, run_duration=7.012881, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:38:53.631710+00:00, data_interval_end=2026-02-14 03:38:53.631710+00:00, dag_hash=c52fa1a388ba1be81c6c42bf2d418436
2026-02-14 11:39:04,771 INFO - 1 tasks up for execution:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>
2026-02-14 11:39:04,772 INFO - DAG producer_N_1_dw_user_sync has 0/16 running and queued tasks
2026-02-14 11:39:04,773 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>
2026-02-14 11:39:04,775 INFO - Trying to enqueue tasks: [<TaskInstance: producer_N_1_dw_user_sync.sync_user_data manual__2026-02-14T03:38:57.131057+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:39:04,777 INFO - Sending TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:38:57.131057+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:39:04,778 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:38:57.131057+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:39:04,780 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_N_1_dw_user_sync', 'sync_user_data', 'manual__2026-02-14T03:38:57.131057+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:39:08,354 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_N_1_dw_user_sync', task_id='sync_user_data', run_id='manual__2026-02-14T03:38:57.131057+00:00', try_number=1, map_index=-1)
2026-02-14 11:39:08,364 INFO - TaskInstance Finished: dag_id=producer_N_1_dw_user_sync, task_id=sync_user_data, run_id=manual__2026-02-14T03:38:57.131057+00:00, map_index=-1, run_start_date=2026-02-14 03:39:07.398305+00:00, run_end_date=2026-02-14 03:39:07.752752+00:00, run_duration=0.354447, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=113, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:39:04.774486+00:00, queued_by_job_id=105, pid=2732
2026-02-14 11:39:10,872 INFO - Marking run <DagRun producer_N_1_dw_user_sync @ 2026-02-14 03:38:57.131057+00:00: manual__2026-02-14T03:38:57.131057+00:00, state:running, queued_at: 2026-02-14 03:38:57.144678+00:00. externally triggered: True> successful
2026-02-14 11:39:10,873 INFO - DagRun Finished: dag_id=producer_N_1_dw_user_sync, execution_date=2026-02-14 03:38:57.131057+00:00, run_id=manual__2026-02-14T03:38:57.131057+00:00, run_start_date=2026-02-14 03:39:04.743265+00:00, run_end_date=2026-02-14 03:39:10.873656+00:00, run_duration=6.130391, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 03:38:57.131057+00:00, data_interval_end=2026-02-14 03:38:57.131057+00:00, dag_hash=f21aec43673b2c968a2fc26e9e7f5cea
2026-02-14 11:39:10,884 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>
2026-02-14 11:39:10,885 INFO - DAG consumer_N_1_order_user_analysis has 0/16 running and queued tasks
2026-02-14 11:39:10,886 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>
2026-02-14 11:39:10,888 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_N_1_order_user_analysis.analyze_order_user dataset_triggered__2026-02-14T03:39:07.770729+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 11:39:10,890 INFO - Sending TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:39:07.770729+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-14 11:39:10,890 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:39:07.770729+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:39:10,894 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_N_1_order_user_analysis', 'analyze_order_user', 'dataset_triggered__2026-02-14T03:39:07.770729+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /N_producer_1_consumer.py']
2026-02-14 11:39:14,462 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_N_1_order_user_analysis', task_id='analyze_order_user', run_id='dataset_triggered__2026-02-14T03:39:07.770729+00:00', try_number=1, map_index=-1)
2026-02-14 11:39:14,473 INFO - TaskInstance Finished: dag_id=consumer_N_1_order_user_analysis, task_id=analyze_order_user, run_id=dataset_triggered__2026-02-14T03:39:07.770729+00:00, map_index=-1, run_start_date=2026-02-14 03:39:13.505551+00:00, run_end_date=2026-02-14 03:39:13.874952+00:00, run_duration=0.369401, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=114, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-14 03:39:10.887510+00:00, queued_by_job_id=105, pid=2736
2026-02-14 11:39:17,035 INFO - Marking run <DagRun consumer_N_1_order_user_analysis @ 2026-02-14 03:39:07.770729+00:00: dataset_triggered__2026-02-14T03:39:07.770729+00:00, state:running, queued_at: 2026-02-14 03:39:10.836944+00:00. externally triggered: False> successful
2026-02-14 11:39:17,036 INFO - DagRun Finished: dag_id=consumer_N_1_order_user_analysis, execution_date=2026-02-14 03:39:07.770729+00:00, run_id=dataset_triggered__2026-02-14T03:39:07.770729+00:00, run_start_date=2026-02-14 03:39:10.858349+00:00, run_end_date=2026-02-14 03:39:17.036580+00:00, run_duration=6.178231, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-14 03:38:53.631710+00:00, data_interval_end=2026-02-14 03:38:57.131057+00:00, dag_hash=6baf9b26ce8f026143cfbb65a8bc4568
2026-02-14 11:43:05,661 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:49:34,163 INFO - Heartbeat recovered after 255.48 seconds
2026-02-14 11:52:16,522 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 11:57:19,028 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:02:19,963 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:07:21,610 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:12:21,762 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:17:21,998 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:22:22,287 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:27:24,316 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:32:26,676 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:44:02,216 INFO - Heartbeat recovered after 568.58 seconds
2026-02-14 12:48:32,153 INFO - Heartbeat recovered after 234.68 seconds
2026-02-14 12:50:41,958 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 12:55:43,868 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 13:01:11,138 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 13:06:15,550 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 13:09:12,928 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 05:00:00+00:00, run_after=2026-02-14 06:00:00+00:00
2026-02-14 13:09:16,251 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
2026-02-14 13:09:16,253 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 13:09:16,254 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
2026-02-14 13:09:16,257 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T04:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 13:09:16,258 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2026-02-14 13:09:16,258 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:16,261 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:20,291 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 13:09:20,301 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=scheduled__2026-02-14T04:00:00+00:00, map_index=-1, run_start_date=2026-02-14 05:09:19.313693+00:00, run_end_date=2026-02-14 05:09:19.708239+00:00, run_duration=0.394546, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=115, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 05:09:16.255557+00:00, queued_by_job_id=105, pid=5146
2026-02-14 13:09:23,142 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
2026-02-14 13:09:23,143 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 13:09:23,143 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>
2026-02-14 13:09:23,146 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T04:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 13:09:23,147 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-14 13:09:23,148 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:23,150 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T04:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:26,861 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T04:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 13:09:26,871 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=scheduled__2026-02-14T04:00:00+00:00, map_index=-1, run_start_date=2026-02-14 05:09:25.932846+00:00, run_end_date=2026-02-14 05:09:26.284209+00:00, run_duration=0.351363, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=116, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 05:09:23.144899+00:00, queued_by_job_id=105, pid=5149
2026-02-14 13:09:29,730 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
2026-02-14 13:09:29,731 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 13:09:29,731 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
2026-02-14 13:09:29,734 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 13:09:29,735 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2026-02-14 13:09:29,736 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:29,738 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:33,343 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1)
2026-02-14 13:09:33,353 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=manual__2026-02-14T05:09:16.552321+00:00, map_index=-1, run_start_date=2026-02-14 05:09:32.398471+00:00, run_end_date=2026-02-14 05:09:32.757500+00:00, run_duration=0.359029, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=117, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 05:09:29.732802+00:00, queued_by_job_id=105, pid=5152
2026-02-14 13:09:36,117 INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 04:00:00+00:00: scheduled__2026-02-14T04:00:00+00:00, state:running, queued_at: 2026-02-14 05:09:12.917691+00:00. externally triggered: False> successful
2026-02-14 13:09:36,119 INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 04:00:00+00:00, run_id=scheduled__2026-02-14T04:00:00+00:00, run_start_date=2026-02-14 05:09:12.942178+00:00, run_end_date=2026-02-14 05:09:36.119027+00:00, run_duration=23.176849, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-14 04:00:00+00:00, data_interval_end=2026-02-14 05:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
2026-02-14 13:09:36,126 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 05:00:00+00:00, run_after=2026-02-14 06:00:00+00:00
2026-02-14 13:09:36,139 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
2026-02-14 13:09:36,140 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 13:09:36,140 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>
2026-02-14 13:09:36,143 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.task_process_normal manual__2026-02-14T05:09:16.552321+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 13:09:36,145 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-14 13:09:36,145 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:36,148 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'manual__2026-02-14T05:09:16.552321+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 13:09:39,699 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='manual__2026-02-14T05:09:16.552321+00:00', try_number=1, map_index=-1)
2026-02-14 13:09:39,710 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=manual__2026-02-14T05:09:16.552321+00:00, map_index=-1, run_start_date=2026-02-14 05:09:38.799845+00:00, run_end_date=2026-02-14 05:09:39.154774+00:00, run_duration=0.354929, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=118, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 05:09:36.141855+00:00, queued_by_job_id=105, pid=5155
2026-02-14 13:09:46,088 INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 05:09:16.552321+00:00: manual__2026-02-14T05:09:16.552321+00:00, state:running, queued_at: 2026-02-14 05:09:16.580469+00:00. externally triggered: True> successful
2026-02-14 13:09:46,089 INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 05:09:16.552321+00:00, run_id=manual__2026-02-14T05:09:16.552321+00:00, run_start_date=2026-02-14 05:09:23.107910+00:00, run_end_date=2026-02-14 05:09:46.089112+00:00, run_duration=22.981202, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 04:00:00+00:00, data_interval_end=2026-02-14 05:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
2026-02-14 13:11:18,109 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 13:16:19,692 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 13:18:39,036 INFO - Exiting gracefully upon receiving signal 15
2026-02-14 13:18:39,767 INFO - Sending Signals.SIGTERM to group 1047. PIDs of all processes in the group: []
2026-02-14 13:18:39,768 INFO - Sending the signal Signals.SIGTERM to group 1047
2026-02-14 13:18:39,769 INFO - Sending the signal Signals.SIGTERM to process 1047 as process group is missing.
2026-02-14 13:18:39,777 INFO - Sending Signals.SIGTERM to group 1047. PIDs of all processes in the group: []
2026-02-14 13:18:39,778 INFO - Sending the signal Signals.SIGTERM to group 1047
2026-02-14 13:18:39,778 INFO - Sending the signal Signals.SIGTERM to process 1047 as process group is missing.
2026-02-14 13:18:39,779 INFO - Exited execute loop
2026-02-14 19:33:33,041 INFO - Loaded executor: SequentialExecutor
2026-02-14 19:33:33,953 INFO - Starting the scheduler
2026-02-14 19:33:33,954 INFO - Processing each file at most -1 times
2026-02-14 19:33:33,964 INFO - Launched DagFileProcessorManager with pid: 2807
2026-02-14 19:33:33,973 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 19:33:44,002 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 10:00:00+00:00, run_after=2026-02-14 11:00:00+00:00
2026-02-14 19:33:49,857 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 11:00:00+00:00, run_after=2026-02-14 12:00:00+00:00
2026-02-14 19:33:49,915 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T05:00:00+00:00 [scheduled]>
2026-02-14 19:33:49,916 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 19:33:49,917 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T05:00:00+00:00 [scheduled]>
2026-02-14 19:33:49,921 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T05:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 19:33:49,923 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T05:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2026-02-14 19:33:49,924 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T05:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:33:49,928 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T05:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:33:55,043 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T05:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 19:33:55,062 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=scheduled__2026-02-14T05:00:00+00:00, map_index=-1, run_start_date=2026-02-14 11:33:53.945767+00:00, run_end_date=2026-02-14 11:33:54.401521+00:00, run_duration=0.455754, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=120, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 11:33:49.919014+00:00, queued_by_job_id=119, pid=2840
2026-02-14 19:33:58,760 INFO - 2 tasks up for execution:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T10:00:00+00:00 [scheduled]>
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T05:00:00+00:00 [scheduled]>
2026-02-14 19:33:58,761 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 19:33:58,762 INFO - DAG branch_task_data_quality has 1/16 running and queued tasks
2026-02-14 19:33:58,763 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T10:00:00+00:00 [scheduled]>
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T05:00:00+00:00 [scheduled]>
2026-02-14 19:33:58,768 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.branch_check_quality scheduled__2026-02-14T10:00:00+00:00 [scheduled]>, <TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T05:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 19:33:58,770 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T10:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 5 and queue default
2026-02-14 19:33:58,771 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T10:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:33:58,772 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T05:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-14 19:33:58,773 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T05:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:33:58,776 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'branch_check_quality', 'scheduled__2026-02-14T10:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:34:06,061 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T05:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:34:12,078 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='branch_check_quality', run_id='scheduled__2026-02-14T10:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 19:34:12,088 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T05:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 19:34:12,110 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=branch_check_quality, run_id=scheduled__2026-02-14T10:00:00+00:00, map_index=-1, run_start_date=2026-02-14 11:34:04.555732+00:00, run_end_date=2026-02-14 11:34:05.050239+00:00, run_duration=0.494507, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=121, pool=default_pool, queue=default, priority_weight=5, operator=BranchPythonOperator, queued_dttm=2026-02-14 11:33:58.765801+00:00, queued_by_job_id=119, pid=2857
2026-02-14 19:34:12,142 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=scheduled__2026-02-14T05:00:00+00:00, map_index=-1, run_start_date=2026-02-14 11:34:10.670552+00:00, run_end_date=2026-02-14 11:34:11.270539+00:00, run_duration=0.599987, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=122, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 11:33:58.765801+00:00, queued_by_job_id=119, pid=2863
2026-02-14 19:34:16,955 INFO - 1 tasks up for execution:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T10:00:00+00:00 [scheduled]>
2026-02-14 19:34:16,957 INFO - DAG branch_task_data_quality has 0/16 running and queued tasks
2026-02-14 19:34:16,958 INFO - Setting the following tasks to queued state:
	<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T10:00:00+00:00 [scheduled]>
2026-02-14 19:34:16,962 INFO - Trying to enqueue tasks: [<TaskInstance: branch_task_data_quality.task_process_normal scheduled__2026-02-14T10:00:00+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-14 19:34:16,964 INFO - Sending TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T10:00:00+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
2026-02-14 19:34:16,965 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T10:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:34:16,969 INFO - Executing command: ['airflow', 'tasks', 'run', 'branch_task_data_quality', 'task_process_normal', 'scheduled__2026-02-14T10:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER//branchDAG.py']
2026-02-14 19:34:22,100 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='branch_task_data_quality', task_id='task_process_normal', run_id='scheduled__2026-02-14T10:00:00+00:00', try_number=1, map_index=-1)
2026-02-14 19:34:22,115 INFO - TaskInstance Finished: dag_id=branch_task_data_quality, task_id=task_process_normal, run_id=scheduled__2026-02-14T10:00:00+00:00, map_index=-1, run_start_date=2026-02-14 11:34:20.997576+00:00, run_end_date=2026-02-14 11:34:21.457591+00:00, run_duration=0.460015, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=1, job_id=123, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2026-02-14 11:34:16.960513+00:00, queued_by_job_id=119, pid=2881
2026-02-14 19:34:26,607 INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 05:00:00+00:00: scheduled__2026-02-14T05:00:00+00:00, state:running, queued_at: 2026-02-14 11:33:43.966748+00:00. externally triggered: False> successful
2026-02-14 19:34:26,610 INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 05:00:00+00:00, run_id=scheduled__2026-02-14T05:00:00+00:00, run_start_date=2026-02-14 11:33:44.023914+00:00, run_end_date=2026-02-14 11:34:26.610385+00:00, run_duration=42.586471, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-14 05:00:00+00:00, data_interval_end=2026-02-14 06:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
2026-02-14 19:34:26,618 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 10:00:00+00:00, run_after=2026-02-14 11:00:00+00:00
2026-02-14 19:34:30,098 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 11:00:00+00:00, run_after=2026-02-14 12:00:00+00:00
2026-02-14 19:34:30,117 INFO - Marking run <DagRun branch_task_data_quality @ 2026-02-14 10:00:00+00:00: scheduled__2026-02-14T10:00:00+00:00, state:running, queued_at: 2026-02-14 11:33:49.846385+00:00. externally triggered: False> successful
2026-02-14 19:34:30,119 INFO - DagRun Finished: dag_id=branch_task_data_quality, execution_date=2026-02-14 10:00:00+00:00, run_id=scheduled__2026-02-14T10:00:00+00:00, run_start_date=2026-02-14 11:33:49.867999+00:00, run_end_date=2026-02-14 11:34:30.119234+00:00, run_duration=40.251235, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2026-02-14 10:00:00+00:00, data_interval_end=2026-02-14 11:00:00+00:00, dag_hash=7306175be177de820215d4c99a20fe56
2026-02-14 19:34:30,128 INFO - Setting next_dagrun for branch_task_data_quality to 2026-02-14 11:00:00+00:00, run_after=2026-02-14 12:00:00+00:00
2026-02-14 19:38:35,448 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 19:43:36,543 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 19:48:39,176 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 19:53:39,282 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 19:56:49,114 INFO - Marking run <DagRun producer_mysql_order_data @ 2026-02-14 11:56:46.825462+00:00: manual__2026-02-14T11:56:46.825462+00:00, state:running, queued_at: 2026-02-14 11:56:46.844695+00:00. externally triggered: True> successful
2026-02-14 19:56:49,116 INFO - DagRun Finished: dag_id=producer_mysql_order_data, execution_date=2026-02-14 11:56:46.825462+00:00, run_id=manual__2026-02-14T11:56:46.825462+00:00, run_start_date=2026-02-14 11:56:49.104174+00:00, run_end_date=2026-02-14 11:56:49.116144+00:00, run_duration=0.01197, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 11:56:46.825462+00:00, data_interval_end=2026-02-14 11:56:46.825462+00:00, dag_hash=2a678ab063ca21354030065030dcffdc
2026-02-14 19:58:39,309 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:03:41,016 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:08:14,152 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-14 12:08:12.052459+00:00: manual__2026-02-14T12:08:12.052459+00:00, state:running, queued_at: 2026-02-14 12:08:12.068867+00:00. externally triggered: True> successful
2026-02-14 20:08:14,154 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-14 12:08:12.052459+00:00, run_id=manual__2026-02-14T12:08:12.052459+00:00, run_start_date=2026-02-14 12:08:14.145698+00:00, run_end_date=2026-02-14 12:08:14.154553+00:00, run_duration=0.008855, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 12:08:12.052459+00:00, data_interval_end=2026-02-14 12:08:12.052459+00:00, dag_hash=964d20dddc21507f57e8633fe2e1e46a
2026-02-14 20:08:41,976 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:12:18,130 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-14 12:12:16.345549+00:00: manual__2026-02-14T12:12:16.345549+00:00, state:running, queued_at: 2026-02-14 12:12:16.358900+00:00. externally triggered: True> successful
2026-02-14 20:12:18,131 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-14 12:12:16.345549+00:00, run_id=manual__2026-02-14T12:12:16.345549+00:00, run_start_date=2026-02-14 12:12:18.123075+00:00, run_end_date=2026-02-14 12:12:18.131554+00:00, run_duration=0.008479, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 12:12:16.345549+00:00, data_interval_end=2026-02-14 12:12:16.345549+00:00, dag_hash=964d20dddc21507f57e8633fe2e1e46a
2026-02-14 20:13:44,525 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:16:53,110 INFO - Exiting gracefully upon receiving signal 15
2026-02-14 20:16:54,127 INFO - Sending Signals.SIGTERM to group 2807. PIDs of all processes in the group: [2807]
2026-02-14 20:16:54,128 INFO - Sending the signal Signals.SIGTERM to group 2807
2026-02-14 20:16:54,158 INFO - Process psutil.Process(pid=2807, status='terminated', exitcode=<Negsignal.SIGTERM: -15>, started='19:33:33') (2807) terminated with exit code Negsignal.SIGTERM
2026-02-14 20:16:54,179 INFO - Sending Signals.SIGTERM to group 2807. PIDs of all processes in the group: []
2026-02-14 20:16:54,181 INFO - Sending the signal Signals.SIGTERM to group 2807
2026-02-14 20:16:54,182 INFO - Sending the signal Signals.SIGTERM to process 2807 as process group is missing.
2026-02-14 20:16:54,183 INFO - Exited execute loop
2026-02-14 20:21:05,982 INFO - Loaded executor: SequentialExecutor
2026-02-14 20:21:06,566 INFO - Starting the scheduler
2026-02-14 20:21:06,567 INFO - Processing each file at most -1 times
2026-02-14 20:21:06,572 INFO - Launched DagFileProcessorManager with pid: 4958
2026-02-14 20:21:06,579 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:21:27,573 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-14 12:21:26.210043+00:00: manual__2026-02-14T12:21:26.210043+00:00, state:running, queued_at: 2026-02-14 12:21:26.235594+00:00. externally triggered: True> successful
2026-02-14 20:21:27,575 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-14 12:21:26.210043+00:00, run_id=manual__2026-02-14T12:21:26.210043+00:00, run_start_date=2026-02-14 12:21:27.403715+00:00, run_end_date=2026-02-14 12:21:27.575589+00:00, run_duration=0.171874, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 12:21:26.210043+00:00, data_interval_end=2026-02-14 12:21:26.210043+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-14 20:26:09,004 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:30:49,645 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-14 12:30:47.770071+00:00: manual__2026-02-14T12:30:47.770071+00:00, state:running, queued_at: 2026-02-14 12:30:47.792143+00:00. externally triggered: True> successful
2026-02-14 20:30:49,647 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-14 12:30:47.770071+00:00, run_id=manual__2026-02-14T12:30:47.770071+00:00, run_start_date=2026-02-14 12:30:49.636523+00:00, run_end_date=2026-02-14 12:30:49.647114+00:00, run_duration=0.010591, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 12:30:47.770071+00:00, data_interval_end=2026-02-14 12:30:47.770071+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-14 20:31:11,321 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:33:12,293 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-14 12:33:09.114451+00:00: manual__2026-02-14T12:33:09.114451+00:00, state:running, queued_at: 2026-02-14 12:33:09.133033+00:00. externally triggered: True> successful
2026-02-14 20:33:12,294 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-14 12:33:09.114451+00:00, run_id=manual__2026-02-14T12:33:09.114451+00:00, run_start_date=2026-02-14 12:33:12.284247+00:00, run_end_date=2026-02-14 12:33:12.294469+00:00, run_duration=0.010222, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-14 12:33:09.114451+00:00, data_interval_end=2026-02-14 12:33:09.114451+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-14 20:36:13,886 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:41:15,065 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:46:17,383 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:51:17,416 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 20:56:19,702 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:01:22,108 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:06:24,844 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:11:25,419 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:16:27,907 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:21:30,267 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:26:31,688 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:31:34,107 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:36:35,892 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:41:38,300 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:46:41,081 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:51:44,142 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 21:56:46,974 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:01:49,871 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:06:51,365 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:11:52,802 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:16:54,667 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:21:57,036 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:25:24,841 INFO - Heartbeat recovered after 124.13 seconds
2026-02-14 22:28:54,772 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:33:57,543 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:39:00,381 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:44:01,496 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:49:04,247 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:54:05,654 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 22:59:08,430 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:04:10,309 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:09:12,801 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:14:13,708 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:19:15,223 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:24:15,888 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:29:18,115 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-14 23:29:25,388 INFO - Exiting gracefully upon receiving signal 15
2026-02-14 23:29:26,282 INFO - Sending Signals.SIGTERM to group 4958. PIDs of all processes in the group: []
2026-02-14 23:29:26,284 INFO - Sending the signal Signals.SIGTERM to group 4958
2026-02-14 23:29:26,285 INFO - Sending the signal Signals.SIGTERM to process 4958 as process group is missing.
2026-02-14 23:29:26,294 INFO - Sending Signals.SIGTERM to group 4958. PIDs of all processes in the group: []
2026-02-14 23:29:26,294 INFO - Sending the signal Signals.SIGTERM to group 4958
2026-02-14 23:29:26,295 INFO - Sending the signal Signals.SIGTERM to process 4958 as process group is missing.
2026-02-14 23:29:26,296 INFO - Exited execute loop
2026-02-15 15:57:53,910 INFO - Loaded executor: SequentialExecutor
2026-02-15 15:57:54,960 INFO - Starting the scheduler
2026-02-15 15:57:54,962 INFO - Processing each file at most -1 times
2026-02-15 15:57:54,973 INFO - Launched DagFileProcessorManager with pid: 1011
2026-02-15 15:57:54,983 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:02:56,734 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:11:25,198 INFO - Heartbeat recovered after 390.08 seconds
2026-02-15 16:14:20,523 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:19:23,164 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:20:34,310 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>
2026-02-15 16:20:34,310 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:20:34,311 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>
2026-02-15 16:20:34,314 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:20:31.221949+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:20:34,315 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:20:31.221949+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:20:34,316 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:20:31.221949+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:20:34,318 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:20:31.221949+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:20:37,990 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:20:31.221949+00:00', try_number=1, map_index=-1)
2026-02-15 16:20:38,005 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:20:31.221949+00:00, map_index=-1, run_start_date=2026-02-15 08:20:36.957461+00:00, run_end_date=2026-02-15 08:20:37.318201+00:00, run_duration=0.36074, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=126, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:20:34.313061+00:00, queued_by_job_id=125, pid=2348
2026-02-15 16:20:40,656 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:20:31.221949+00:00: manual__2026-02-15T08:20:31.221949+00:00, state:running, queued_at: 2026-02-15 08:20:31.249112+00:00. externally triggered: True> successful
2026-02-15 16:20:40,658 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:20:31.221949+00:00, run_id=manual__2026-02-15T08:20:31.221949+00:00, run_start_date=2026-02-15 08:20:34.270820+00:00, run_end_date=2026-02-15 08:20:40.658445+00:00, run_duration=6.387625, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:20:31.221949+00:00, data_interval_end=2026-02-15 08:20:31.221949+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:20:40,667 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>
2026-02-15 16:20:40,668 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:20:40,669 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>
2026-02-15 16:20:40,671 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:20:37.334108+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:20:40,672 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:20:37.334108+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:20:40,673 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:20:37.334108+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:20:40,675 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:20:37.334108+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:20:44,081 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:20:37.334108+00:00', try_number=1, map_index=-1)
2026-02-15 16:20:44,091 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:20:37.334108+00:00, map_index=-1, run_start_date=2026-02-15 08:20:43.189450+00:00, run_end_date=2026-02-15 08:20:43.529989+00:00, run_duration=0.340539, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=127, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:20:40.670651+00:00, queued_by_job_id=125, pid=2351
2026-02-15 16:20:46,710 ERROR - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:20:37.334108+00:00: dataset_triggered__2026-02-15T08:20:37.334108+00:00, state:running, queued_at: 2026-02-15 08:20:40.623731+00:00. externally triggered: False> failed
2026-02-15 16:20:46,711 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:20:37.334108+00:00, run_id=dataset_triggered__2026-02-15T08:20:37.334108+00:00, run_start_date=2026-02-15 08:20:40.644735+00:00, run_end_date=2026-02-15 08:20:46.711057+00:00, run_duration=6.066322, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:20:31.221949+00:00, data_interval_end=2026-02-15 08:20:31.221949+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:23:06,551 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>
2026-02-15 16:23:06,552 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:23:06,553 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>
2026-02-15 16:23:06,555 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:23:03.595532+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:23:06,556 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:23:03.595532+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:23:06,557 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:23:03.595532+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:23:06,559 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:23:03.595532+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:23:10,251 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:23:03.595532+00:00', try_number=1, map_index=-1)
2026-02-15 16:23:10,263 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:23:03.595532+00:00, map_index=-1, run_start_date=2026-02-15 08:23:09.275814+00:00, run_end_date=2026-02-15 08:23:09.660724+00:00, run_duration=0.38491, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=128, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:23:06.554078+00:00, queued_by_job_id=125, pid=2490
2026-02-15 16:23:12,887 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:23:03.595532+00:00: manual__2026-02-15T08:23:03.595532+00:00, state:running, queued_at: 2026-02-15 08:23:03.615355+00:00. externally triggered: True> successful
2026-02-15 16:23:12,888 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:23:03.595532+00:00, run_id=manual__2026-02-15T08:23:03.595532+00:00, run_start_date=2026-02-15 08:23:06.531922+00:00, run_end_date=2026-02-15 08:23:12.888437+00:00, run_duration=6.356515, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:23:03.595532+00:00, data_interval_end=2026-02-15 08:23:03.595532+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:23:12,897 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>
2026-02-15 16:23:12,898 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:23:12,899 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>
2026-02-15 16:23:12,901 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:23:09.677763+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:23:12,902 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:23:09.677763+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:23:12,903 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:23:09.677763+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:23:12,905 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:23:09.677763+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:23:16,562 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:23:09.677763+00:00', try_number=1, map_index=-1)
2026-02-15 16:23:16,572 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:23:09.677763+00:00, map_index=-1, run_start_date=2026-02-15 08:23:15.449617+00:00, run_end_date=2026-02-15 08:23:15.887352+00:00, run_duration=0.437735, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=129, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:23:12.900177+00:00, queued_by_job_id=125, pid=2493
2026-02-15 16:23:19,131 INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:23:09.677763+00:00: dataset_triggered__2026-02-15T08:23:09.677763+00:00, state:running, queued_at: 2026-02-15 08:23:12.861334+00:00. externally triggered: False> successful
2026-02-15 16:23:19,132 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:23:09.677763+00:00, run_id=dataset_triggered__2026-02-15T08:23:09.677763+00:00, run_start_date=2026-02-15 08:23:12.875117+00:00, run_end_date=2026-02-15 08:23:19.131958+00:00, run_duration=6.256841, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:23:03.595532+00:00, data_interval_end=2026-02-15 08:23:03.595532+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:24:24,206 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:29:25,763 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:34:26,931 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:39:28,907 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:43:39,969 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>
2026-02-15 16:43:39,970 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:43:39,971 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>
2026-02-15 16:43:39,973 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:43:37.783738+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:43:39,974 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:43:37.783738+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:43:39,975 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:43:37.783738+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:43:39,977 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:43:37.783738+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:43:43,700 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:43:37.783738+00:00', try_number=1, map_index=-1)
2026-02-15 16:43:43,709 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:43:37.783738+00:00, map_index=-1, run_start_date=2026-02-15 08:43:42.748972+00:00, run_end_date=2026-02-15 08:43:43.123360+00:00, run_duration=0.374388, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=130, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:43:39.972153+00:00, queued_by_job_id=125, pid=3390
2026-02-15 16:43:46,296 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:43:37.783738+00:00: manual__2026-02-15T08:43:37.783738+00:00, state:running, queued_at: 2026-02-15 08:43:37.802419+00:00. externally triggered: True> successful
2026-02-15 16:43:46,297 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:43:37.783738+00:00, run_id=manual__2026-02-15T08:43:37.783738+00:00, run_start_date=2026-02-15 08:43:39.948365+00:00, run_end_date=2026-02-15 08:43:46.297152+00:00, run_duration=6.348787, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:43:37.783738+00:00, data_interval_end=2026-02-15 08:43:37.783738+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:43:46,307 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>
2026-02-15 16:43:46,308 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:43:46,308 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>
2026-02-15 16:43:46,311 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:43:43.140152+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:43:46,312 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:43:43.140152+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:43:46,312 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:43:43.140152+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:43:46,315 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:43:43.140152+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:43:49,814 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:43:43.140152+00:00', try_number=1, map_index=-1)
2026-02-15 16:43:49,822 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:43:43.140152+00:00, map_index=-1, run_start_date=2026-02-15 08:43:48.872686+00:00, run_end_date=2026-02-15 08:43:49.311331+00:00, run_duration=0.438645, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=131, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:43:46.309880+00:00, queued_by_job_id=125, pid=3393
2026-02-15 16:43:52,492 INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:43:43.140152+00:00: dataset_triggered__2026-02-15T08:43:43.140152+00:00, state:running, queued_at: 2026-02-15 08:43:46.269290+00:00. externally triggered: False> successful
2026-02-15 16:43:52,493 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:43:43.140152+00:00, run_id=dataset_triggered__2026-02-15T08:43:43.140152+00:00, run_start_date=2026-02-15 08:43:46.282966+00:00, run_end_date=2026-02-15 08:43:52.493241+00:00, run_duration=6.210275, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:43:37.783738+00:00, data_interval_end=2026-02-15 08:43:37.783738+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:44:31,221 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:47:10,700 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>
2026-02-15 16:47:10,702 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:47:10,703 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>
2026-02-15 16:47:10,705 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:47:09.878272+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:47:10,706 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:47:09.878272+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:47:10,707 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:47:09.878272+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:47:10,709 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:47:09.878272+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:47:14,866 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:47:09.878272+00:00', try_number=1, map_index=-1)
2026-02-15 16:47:14,875 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:47:09.878272+00:00, map_index=-1, run_start_date=2026-02-15 08:47:13.887974+00:00, run_end_date=2026-02-15 08:47:14.261265+00:00, run_duration=0.373291, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=132, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:47:10.704327+00:00, queued_by_job_id=125, pid=3535
2026-02-15 16:47:17,361 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:47:09.878272+00:00: manual__2026-02-15T08:47:09.878272+00:00, state:running, queued_at: 2026-02-15 08:47:09.890049+00:00. externally triggered: True> successful
2026-02-15 16:47:17,363 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:47:09.878272+00:00, run_id=manual__2026-02-15T08:47:09.878272+00:00, run_start_date=2026-02-15 08:47:10.671755+00:00, run_end_date=2026-02-15 08:47:17.363024+00:00, run_duration=6.691269, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:47:09.878272+00:00, data_interval_end=2026-02-15 08:47:09.878272+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:47:17,374 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>
2026-02-15 16:47:17,375 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:47:17,375 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>
2026-02-15 16:47:17,378 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:47:14.278092+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:47:17,379 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:47:14.278092+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:47:17,380 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:47:14.278092+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:47:17,383 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:47:14.278092+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:47:21,203 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:47:14.278092+00:00', try_number=1, map_index=-1)
2026-02-15 16:47:21,212 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:47:14.278092+00:00, map_index=-1, run_start_date=2026-02-15 08:47:20.124929+00:00, run_end_date=2026-02-15 08:47:20.608615+00:00, run_duration=0.483686, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=133, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:47:17.377017+00:00, queued_by_job_id=125, pid=3538
2026-02-15 16:47:23,839 INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:47:14.278092+00:00: dataset_triggered__2026-02-15T08:47:14.278092+00:00, state:running, queued_at: 2026-02-15 08:47:17.332903+00:00. externally triggered: False> successful
2026-02-15 16:47:23,840 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:47:14.278092+00:00, run_id=dataset_triggered__2026-02-15T08:47:14.278092+00:00, run_start_date=2026-02-15 08:47:17.348345+00:00, run_end_date=2026-02-15 08:47:23.840515+00:00, run_duration=6.49217, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:47:09.878272+00:00, data_interval_end=2026-02-15 08:47:09.878272+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:49:33,736 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:52:01,430 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>
2026-02-15 16:52:01,431 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:52:01,432 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>
2026-02-15 16:52:01,434 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:51:59.598541+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:52:01,436 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:51:59.598541+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:52:01,436 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:51:59.598541+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:52:01,439 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:51:59.598541+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:52:05,410 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:51:59.598541+00:00', try_number=1, map_index=-1)
2026-02-15 16:52:05,419 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:51:59.598541+00:00, map_index=-1, run_start_date=2026-02-15 08:52:04.391133+00:00, run_end_date=2026-02-15 08:52:04.822273+00:00, run_duration=0.43114, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=134, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:52:01.432882+00:00, queued_by_job_id=125, pid=3749
2026-02-15 16:52:08,020 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:51:59.598541+00:00: manual__2026-02-15T08:51:59.598541+00:00, state:running, queued_at: 2026-02-15 08:51:59.609747+00:00. externally triggered: True> successful
2026-02-15 16:52:08,021 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:51:59.598541+00:00, run_id=manual__2026-02-15T08:51:59.598541+00:00, run_start_date=2026-02-15 08:52:01.410145+00:00, run_end_date=2026-02-15 08:52:08.021039+00:00, run_duration=6.610894, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:51:59.598541+00:00, data_interval_end=2026-02-15 08:51:59.598541+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:52:08,030 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>
2026-02-15 16:52:08,031 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:52:08,031 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>
2026-02-15 16:52:08,033 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:52:04.840837+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:52:08,035 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:52:04.840837+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:52:08,035 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:52:04.840837+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:52:08,038 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:52:04.840837+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:52:11,726 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:52:04.840837+00:00', try_number=1, map_index=-1)
2026-02-15 16:52:11,735 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:52:04.840837+00:00, map_index=-1, run_start_date=2026-02-15 08:52:10.658116+00:00, run_end_date=2026-02-15 08:52:11.145601+00:00, run_duration=0.487485, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=135, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:52:08.032712+00:00, queued_by_job_id=125, pid=3752
2026-02-15 16:52:14,362 INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:52:04.840837+00:00: dataset_triggered__2026-02-15T08:52:04.840837+00:00, state:running, queued_at: 2026-02-15 08:52:07.989842+00:00. externally triggered: False> successful
2026-02-15 16:52:14,363 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:52:04.840837+00:00, run_id=dataset_triggered__2026-02-15T08:52:04.840837+00:00, run_start_date=2026-02-15 08:52:08.006323+00:00, run_end_date=2026-02-15 08:52:14.363724+00:00, run_duration=6.357401, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:51:59.598541+00:00, data_interval_end=2026-02-15 08:51:59.598541+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:53:05,118 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>
2026-02-15 16:53:05,119 INFO - DAG producer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:53:05,120 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>
2026-02-15 16:53:05,122 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_mysql.sync_order_data manual__2026-02-15T08:53:01.949106+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:53:05,124 INFO - Sending TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:53:01.949106+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:53:05,124 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:53:01.949106+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:53:05,127 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_mysql', 'sync_order_data', 'manual__2026-02-15T08:53:01.949106+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:53:08,819 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_mysql', task_id='sync_order_data', run_id='manual__2026-02-15T08:53:01.949106+00:00', try_number=1, map_index=-1)
2026-02-15 16:53:08,829 INFO - TaskInstance Finished: dag_id=producer_order_mysql, task_id=sync_order_data, run_id=manual__2026-02-15T08:53:01.949106+00:00, map_index=-1, run_start_date=2026-02-15 08:53:07.797065+00:00, run_end_date=2026-02-15 08:53:08.206592+00:00, run_duration=0.409527, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=136, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:53:05.121622+00:00, queued_by_job_id=125, pid=3781
2026-02-15 16:53:11,710 INFO - Marking run <DagRun producer_order_mysql @ 2026-02-15 08:53:01.949106+00:00: manual__2026-02-15T08:53:01.949106+00:00, state:running, queued_at: 2026-02-15 08:53:01.968186+00:00. externally triggered: True> successful
2026-02-15 16:53:11,711 INFO - DagRun Finished: dag_id=producer_order_mysql, execution_date=2026-02-15 08:53:01.949106+00:00, run_id=manual__2026-02-15T08:53:01.949106+00:00, run_start_date=2026-02-15 08:53:05.094052+00:00, run_end_date=2026-02-15 08:53:11.711074+00:00, run_duration=6.617022, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 08:53:01.949106+00:00, data_interval_end=2026-02-15 08:53:01.949106+00:00, dag_hash=8adaa434cd10062f2620192444602f8e
2026-02-15 16:53:11,720 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>
2026-02-15 16:53:11,721 INFO - DAG consumer_order_mysql has 0/16 running and queued tasks
2026-02-15 16:53:11,722 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>
2026-02-15 16:53:11,724 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_mysql.clean_order_data dataset_triggered__2026-02-15T08:53:08.224546+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 16:53:11,725 INFO - Sending TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:53:08.224546+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 16:53:11,726 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:53:08.224546+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:53:11,728 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_mysql', 'clean_order_data', 'dataset_triggered__2026-02-15T08:53:08.224546+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db2.py']
2026-02-15 16:53:15,456 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_mysql', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T08:53:08.224546+00:00', try_number=1, map_index=-1)
2026-02-15 16:53:15,465 INFO - TaskInstance Finished: dag_id=consumer_order_mysql, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T08:53:08.224546+00:00, map_index=-1, run_start_date=2026-02-15 08:53:14.379673+00:00, run_end_date=2026-02-15 08:53:14.885960+00:00, run_duration=0.506287, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=137, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 08:53:11.723081+00:00, queued_by_job_id=125, pid=3785
2026-02-15 16:53:18,304 INFO - Marking run <DagRun consumer_order_mysql @ 2026-02-15 08:53:08.224546+00:00: dataset_triggered__2026-02-15T08:53:08.224546+00:00, state:running, queued_at: 2026-02-15 08:53:11.684187+00:00. externally triggered: False> successful
2026-02-15 16:53:18,305 INFO - DagRun Finished: dag_id=consumer_order_mysql, execution_date=2026-02-15 08:53:08.224546+00:00, run_id=dataset_triggered__2026-02-15T08:53:08.224546+00:00, run_start_date=2026-02-15 08:53:11.697224+00:00, run_end_date=2026-02-15 08:53:18.305273+00:00, run_duration=6.608049, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 08:53:01.949106+00:00, data_interval_end=2026-02-15 08:53:01.949106+00:00, dag_hash=12e15934c89c6e697010b52e04f32b30
2026-02-15 16:54:36,744 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 16:59:39,240 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:04:40,588 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:08:08,599 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>
2026-02-15 17:08:08,600 INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:08:08,601 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>
2026-02-15 17:08:08,604 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:08:07.117190+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:08:08,605 INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:08:07.117190+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:08:08,606 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:08:07.117190+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:08:08,608 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:08:07.117190+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:08:13,562 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:08:07.117190+00:00', try_number=1, map_index=-1)
2026-02-15 17:08:13,571 INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:08:07.117190+00:00, map_index=-1, run_start_date=2026-02-15 09:08:12.481857+00:00, run_end_date=2026-02-15 09:08:12.852376+00:00, run_duration=0.370519, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=138, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:08:08.602841+00:00, queued_by_job_id=125, pid=4423
2026-02-15 17:08:16,219 INFO - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:08:07.117190+00:00: manual__2026-02-15T09:08:07.117190+00:00, state:running, queued_at: 2026-02-15 09:08:07.129819+00:00. externally triggered: True> successful
2026-02-15 17:08:16,220 INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:08:07.117190+00:00, run_id=manual__2026-02-15T09:08:07.117190+00:00, run_start_date=2026-02-15 09:08:08.576755+00:00, run_end_date=2026-02-15 09:08:16.220235+00:00, run_duration=7.64348, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:08:07.117190+00:00, data_interval_end=2026-02-15 09:08:07.117190+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
2026-02-15 17:08:16,229 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>
2026-02-15 17:08:16,230 INFO - DAG consumer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:08:16,231 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>
2026-02-15 17:08:16,233 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:08:12.868772+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:08:16,234 INFO - Sending TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:08:12.868772+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:08:16,235 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:08:12.868772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:08:16,238 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:08:12.868772+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:08:19,838 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:08:12.868772+00:00', try_number=1, map_index=-1)
2026-02-15 17:08:19,846 INFO - TaskInstance Finished: dag_id=consumer_order_sqlserver, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T09:08:12.868772+00:00, map_index=-1, run_start_date=2026-02-15 09:08:18.847774+00:00, run_end_date=2026-02-15 09:08:19.210150+00:00, run_duration=0.362376, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=139, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:08:16.232545+00:00, queued_by_job_id=125, pid=4426
2026-02-15 17:08:22,549 ERROR - Marking run <DagRun consumer_order_sqlserver @ 2026-02-15 09:08:12.868772+00:00: dataset_triggered__2026-02-15T09:08:12.868772+00:00, state:running, queued_at: 2026-02-15 09:08:16.192012+00:00. externally triggered: False> failed
2026-02-15 17:08:22,550 INFO - DagRun Finished: dag_id=consumer_order_sqlserver, execution_date=2026-02-15 09:08:12.868772+00:00, run_id=dataset_triggered__2026-02-15T09:08:12.868772+00:00, run_start_date=2026-02-15 09:08:16.206206+00:00, run_end_date=2026-02-15 09:08:22.550239+00:00, run_duration=6.344033, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 09:08:07.117190+00:00, data_interval_end=2026-02-15 09:08:07.117190+00:00, dag_hash=4bfcc7af7a5b312e79e6f8e335db1b32
2026-02-15 17:09:43,425 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:11:08,964 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>
2026-02-15 17:11:08,965 INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:11:08,966 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>
2026-02-15 17:11:08,968 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:11:05.621117+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:11:08,970 INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:11:05.621117+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:11:08,970 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:11:05.621117+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:11:08,973 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:11:05.621117+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:11:13,029 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:11:05.621117+00:00', try_number=1, map_index=-1)
2026-02-15 17:11:13,038 INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:11:05.621117+00:00, map_index=-1, run_start_date=2026-02-15 09:11:12.018323+00:00, run_end_date=2026-02-15 09:11:12.396732+00:00, run_duration=0.378409, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=140, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:11:08.967435+00:00, queued_by_job_id=125, pid=4535
2026-02-15 17:11:15,669 ERROR - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:11:05.621117+00:00: manual__2026-02-15T09:11:05.621117+00:00, state:running, queued_at: 2026-02-15 09:11:05.633027+00:00. externally triggered: True> failed
2026-02-15 17:11:15,670 INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:11:05.621117+00:00, run_id=manual__2026-02-15T09:11:05.621117+00:00, run_start_date=2026-02-15 09:11:08.942850+00:00, run_end_date=2026-02-15 09:11:15.670612+00:00, run_duration=6.727762, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:11:05.621117+00:00, data_interval_end=2026-02-15 09:11:05.621117+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
2026-02-15 17:14:45,184 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:15:46,526 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>
2026-02-15 17:15:46,527 INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:15:46,528 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>
2026-02-15 17:15:46,530 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:15:43.478244+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:15:46,531 INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:15:43.478244+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:15:46,532 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:15:43.478244+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:15:46,534 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:15:43.478244+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:15:50,677 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:15:43.478244+00:00', try_number=1, map_index=-1)
2026-02-15 17:15:50,686 INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:15:43.478244+00:00, map_index=-1, run_start_date=2026-02-15 09:15:49.614062+00:00, run_end_date=2026-02-15 09:15:49.993795+00:00, run_duration=0.379733, state=failed, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=141, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:15:46.529585+00:00, queued_by_job_id=125, pid=4704
2026-02-15 17:15:53,248 ERROR - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:15:43.478244+00:00: manual__2026-02-15T09:15:43.478244+00:00, state:running, queued_at: 2026-02-15 09:15:43.489512+00:00. externally triggered: True> failed
2026-02-15 17:15:53,250 INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:15:43.478244+00:00, run_id=manual__2026-02-15T09:15:43.478244+00:00, run_start_date=2026-02-15 09:15:46.507541+00:00, run_end_date=2026-02-15 09:15:53.249903+00:00, run_duration=6.742362, state=failed, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:15:43.478244+00:00, data_interval_end=2026-02-15 09:15:43.478244+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
2026-02-15 17:18:04,013 INFO - 1 tasks up for execution:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>
2026-02-15 17:18:04,027 INFO - DAG producer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:18:04,028 INFO - Setting the following tasks to queued state:
	<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>
2026-02-15 17:18:04,034 INFO - Trying to enqueue tasks: [<TaskInstance: producer_order_sqlserver.sync_order_data manual__2026-02-15T09:18:02.962311+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:18:04,035 INFO - Sending TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:18:02.962311+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:18:04,036 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:18:02.962311+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:18:04,052 INFO - Executing command: ['airflow', 'tasks', 'run', 'producer_order_sqlserver', 'sync_order_data', 'manual__2026-02-15T09:18:02.962311+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:18:08,384 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='producer_order_sqlserver', task_id='sync_order_data', run_id='manual__2026-02-15T09:18:02.962311+00:00', try_number=1, map_index=-1)
2026-02-15 17:18:08,392 INFO - TaskInstance Finished: dag_id=producer_order_sqlserver, task_id=sync_order_data, run_id=manual__2026-02-15T09:18:02.962311+00:00, map_index=-1, run_start_date=2026-02-15 09:18:07.187874+00:00, run_end_date=2026-02-15 09:18:07.829922+00:00, run_duration=0.642048, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=142, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:18:04.031523+00:00, queued_by_job_id=125, pid=4788
2026-02-15 17:18:10,980 INFO - Marking run <DagRun producer_order_sqlserver @ 2026-02-15 09:18:02.962311+00:00: manual__2026-02-15T09:18:02.962311+00:00, state:running, queued_at: 2026-02-15 09:18:02.970703+00:00. externally triggered: True> successful
2026-02-15 17:18:10,981 INFO - DagRun Finished: dag_id=producer_order_sqlserver, execution_date=2026-02-15 09:18:02.962311+00:00, run_id=manual__2026-02-15T09:18:02.962311+00:00, run_start_date=2026-02-15 09:18:03.961549+00:00, run_end_date=2026-02-15 09:18:10.981245+00:00, run_duration=7.019696, state=success, external_trigger=True, run_type=manual, data_interval_start=2026-02-15 09:18:02.962311+00:00, data_interval_end=2026-02-15 09:18:02.962311+00:00, dag_hash=c534735c11f1f608816c92d09c67d709
2026-02-15 17:18:10,991 INFO - 1 tasks up for execution:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>
2026-02-15 17:18:10,992 INFO - DAG consumer_order_sqlserver has 0/16 running and queued tasks
2026-02-15 17:18:10,992 INFO - Setting the following tasks to queued state:
	<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>
2026-02-15 17:18:10,995 INFO - Trying to enqueue tasks: [<TaskInstance: consumer_order_sqlserver.clean_order_data dataset_triggered__2026-02-15T09:18:07.845834+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
2026-02-15 17:18:10,996 INFO - Sending TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:18:07.845834+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
2026-02-15 17:18:10,997 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:18:07.845834+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:18:10,999 INFO - Executing command: ['airflow', 'tasks', 'run', 'consumer_order_sqlserver', 'clean_order_data', 'dataset_triggered__2026-02-15T09:18:07.845834+00:00', '--local', '--subdir', 'DAGS_FOLDER/producer_consumer /dataset_db_sqlserver.py']
2026-02-15 17:18:14,934 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='consumer_order_sqlserver', task_id='clean_order_data', run_id='dataset_triggered__2026-02-15T09:18:07.845834+00:00', try_number=1, map_index=-1)
2026-02-15 17:18:14,942 INFO - TaskInstance Finished: dag_id=consumer_order_sqlserver, task_id=clean_order_data, run_id=dataset_triggered__2026-02-15T09:18:07.845834+00:00, map_index=-1, run_start_date=2026-02-15 09:18:13.644053+00:00, run_end_date=2026-02-15 09:18:14.445610+00:00, run_duration=0.801557, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=143, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2026-02-15 09:18:10.993879+00:00, queued_by_job_id=125, pid=4791
2026-02-15 17:18:17,733 INFO - Marking run <DagRun consumer_order_sqlserver @ 2026-02-15 09:18:07.845834+00:00: dataset_triggered__2026-02-15T09:18:07.845834+00:00, state:running, queued_at: 2026-02-15 09:18:10.951674+00:00. externally triggered: False> successful
2026-02-15 17:18:17,734 INFO - DagRun Finished: dag_id=consumer_order_sqlserver, execution_date=2026-02-15 09:18:07.845834+00:00, run_id=dataset_triggered__2026-02-15T09:18:07.845834+00:00, run_start_date=2026-02-15 09:18:10.966958+00:00, run_end_date=2026-02-15 09:18:17.734565+00:00, run_duration=6.767607, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2026-02-15 09:18:02.962311+00:00, data_interval_end=2026-02-15 09:18:02.962311+00:00, dag_hash=4bfcc7af7a5b312e79e6f8e335db1b32
2026-02-15 17:19:46,387 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:24:48,987 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:29:50,029 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:34:51,907 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:40:42,141 INFO - Heartbeat recovered after 51.66 seconds
2026-02-15 17:40:42,145 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:47:25,644 INFO - Heartbeat recovered after 214.23 seconds
2026-02-15 17:49:08,019 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 17:54:08,175 INFO - Adopting or resetting orphaned tasks for active dag runs
2026-02-15 18:02:28,100 INFO - Heartbeat recovered after 217.74 seconds
2026-02-15 18:02:41,040 INFO - Adopting or resetting orphaned tasks for active dag runs
